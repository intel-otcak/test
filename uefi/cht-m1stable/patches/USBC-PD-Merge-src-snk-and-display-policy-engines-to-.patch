From c4549470e34caae4952cafe0e3241b1451af7e54 Mon Sep 17 00:00:00 2001
From: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
Date: Tue, 15 Dec 2015 18:05:14 +0530
Subject: [PATCH 8/8] USBC/PD: Merge src,snk and display policy engines to
 single PE.

To support USBC PD spec v1.2 policy engine state changes new
policy engine is implemented as detailed below.
1. All three policy engines are merged to single policy engine (PE).
2. For every state on the PE, there will be a state handler function
   which will perform the tasks defined( in PD spec) for that
   particular state.
3. Mainly PE will run in 3 threads
	State handler thread: This thread will execute state handlers
	when PE changes it.s state.
	Packet receive thread: This is the thread in which pe receives
	PD packets from protocol and process.
	Timer expiration thread: If a timer is expired, then this
	timer expire thread will handle based on the type of timer
	and current pe state.

Change-Id: I6a4783347cc18d289d6a4e7fb5cb4c1c104e5d92
Tracked-On: https://jira01.devtools.intel.com/browse/OAM-12097
Signed-off-by: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
Reviewed-on: https://android.intel.com:443/447412
---
 drivers/usb/typec/pd/Makefile             |    2 +
 drivers/usb/typec/pd/pd_policy.h          |   28 +
 drivers/usb/typec/pd/policy_engine.c      | 2682 +++++++++++++++++++++++++++++
 drivers/usb/typec/pd/policy_engine.h      |  573 ++++++
 drivers/usb/typec/pd/protocol.h           |    1 +
 drivers/usb/typec/pd/vdm_process.c        |  647 +++++++
 drivers/usb/typec/pd/vdm_process_helper.h |   35 +
 7 files changed, 3968 insertions(+)
 create mode 100644 drivers/usb/typec/pd/policy_engine.c
 create mode 100644 drivers/usb/typec/pd/policy_engine.h
 create mode 100644 drivers/usb/typec/pd/vdm_process.c
 create mode 100644 drivers/usb/typec/pd/vdm_process_helper.h

diff --git a/drivers/usb/typec/pd/Makefile b/drivers/usb/typec/pd/Makefile
index acca5d6..6c0139e 100644
--- a/drivers/usb/typec/pd/Makefile
+++ b/drivers/usb/typec/pd/Makefile
@@ -4,6 +4,8 @@
 
 pd_policy-y	:= devpolicy_mgr.o
 pd_policy-y	+= protocol.o
+pd_policy-y	+= policy_engine.o
+pd_policy-y	+= vdm_process.o
 
 obj-$(CONFIG_USBC_PD_POLICY)		+= pd_policy.o
 obj-$(CONFIG_USBC_SYSTEM_POLICY)	+= system_policy.o
diff --git a/drivers/usb/typec/pd/pd_policy.h b/drivers/usb/typec/pd/pd_policy.h
index fc54968..4927e91 100644
--- a/drivers/usb/typec/pd/pd_policy.h
+++ b/drivers/usb/typec/pd/pd_policy.h
@@ -1,3 +1,26 @@
+/*
+ * pd_policy.h: Intel USB Power Delivery Policy Header
+ *
+ * Copyright (C) 2015 Intel Corporation
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Seee the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ * Author: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
+ */
+
 #ifndef __PD_POLICY_H__
 #define __PD_POLICY_H__
 
@@ -123,10 +146,15 @@ static inline int pe_notify_dpm_evt(struct policy *p,
 	return -ENOTSUPP;
 }
 
+#if defined(CONFIG_USBC_PD) && defined(CONFIG_USBC_PD_POLICY)
+extern int policy_engine_bind_dpm(struct devpolicy_mgr *dpm);
+extern void policy_engine_unbind_dpm(struct devpolicy_mgr *dpm);
+#else /* CONFIG_USBC_PD && CONFIG_USBC_PD_POLICY */
 static inline int policy_engine_bind_dpm(struct devpolicy_mgr *dpm)
 {
 	return 0;
 }
 static inline void policy_engine_unbind_dpm(struct devpolicy_mgr *dpm)
 { }
+#endif /* CONFIG_USBC_PD && CONFIG_USBC_PD_POLICY */
 #endif /* __PD_POLICY_H__ */
diff --git a/drivers/usb/typec/pd/policy_engine.c b/drivers/usb/typec/pd/policy_engine.c
new file mode 100644
index 0000000..43aafc0
--- /dev/null
+++ b/drivers/usb/typec/pd/policy_engine.c
@@ -0,0 +1,2682 @@
+/*
+ * policy_engine.c: Intel USB Power Delivery Policy Engine Driver
+ *
+ * Copyright (C) 2015 Intel Corporation
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Seee the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ * Author: Kotakonda, Venkataramana <venkataramana.kotakonda@intel.com>
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/timer.h>
+#include <linux/usb_typec_phy.h>
+#include "policy_engine.h"
+#include "vdm_process_helper.h"
+
+
+/* Local function prototypes */
+static void pe_dump_header(struct pd_pkt_header *header);
+static void pe_dump_data_msg(struct pd_packet *pkt);
+static void pe_send_self_sink_caps(struct policy_engine *pe);
+static void pe_start_timer(struct policy_engine *pe,
+				enum pe_timers timer_type, unsigned time);
+static void pe_cancel_timer(struct policy_engine *pe,
+				enum pe_timers timer_type);
+static bool pe_is_timer_pending(struct policy_engine *pe,
+				enum pe_timers timer_type);
+static void
+pe_process_state_pe_src_transition_to_default_exit(struct policy_engine *pe);
+static void pe_set_data_role(struct policy_engine *pe, enum data_role role);
+static void pe_set_power_role(struct policy_engine *pe, enum pwr_role role);
+static void pe_change_state_to_snk_or_src_reset(struct policy_engine *pe);
+static void pe_store_port_partner_caps(struct policy_engine *pe,
+					struct pe_port_pdos *pdos);
+
+static void pe_change_state(struct policy_engine *pe, enum pe_states state)
+{
+	pe->prev_state = pe->cur_state;
+	pe->cur_state = state;
+	schedule_work(&pe->policy_state_work);
+}
+
+static void pe_deactivate_all_timers(struct policy_engine *pe)
+{
+	int i;
+
+	for (i = 0; i < PE_TIMER_CNT; i++)
+		del_timer(&pe->timers[i].timer);
+
+}
+
+static void pe_do_self_reset(struct policy_engine *pe)
+{
+	pe_deactivate_all_timers(pe);
+	/*
+	 * Cancel pending state works. Donot use sync as this
+	 * reset itself might be runnig in the worker.
+	 */
+	pe->alt_state = PE_ALT_STATE_NONE;
+	pe->is_modal_operation = false;
+	if (pe->pp_alt_caps.hpd_state)
+		devpolicy_set_dp_state(pe->p.dpm, CABLE_DETACHED,
+						TYPEC_DP_TYPE_NONE);
+
+	cancel_delayed_work(&pe->vbus_poll_work);
+	/* Reset all counter exept reset counter */
+	pe->retry_counter = 0;
+	pe->src_caps_couner = 0;
+	pe->discover_identity_couner = 0;
+	pe->vdm_busy_couner = 0;
+
+	/* reset required local variable */
+	pe->is_gcrc_received = false;
+	pe->last_rcv_evt = PE_EVT_RCVD_NONE;
+	pe->pd_explicit_contract = false;
+	pe->is_no_response_timer_expired = false;
+	pe->is_pr_swap_rejected = false;
+
+	/* reset saved port partner's data */
+	memset(&pe->pp_snk_pdos, 0, sizeof(struct pe_port_pdos));
+	memset(&pe->pp_src_pdos, 0, sizeof(struct pe_port_pdos));
+	memset(&pe->pp_caps, 0, sizeof(struct pe_port_partner_caps));
+
+	/* Disable PD, auto crc */
+	devpolicy_enable_pd(pe->p.dpm, false);
+	pe->is_pd_enabled = false;
+}
+
+static void pe_do_dpm_reset_entry(struct policy_engine *pe)
+{
+	/* VBUS Off*/
+	if (pe->cur_prole == POWER_ROLE_SOURCE) {
+		devpolicy_set_vbus_state(pe->p.dpm, false);
+		if (pe->cur_drole != DATA_ROLE_DFP)
+			/* Reset data role to NONE */
+			pe_set_data_role(pe, DATA_ROLE_NONE);
+	} else if (pe->cur_prole == POWER_ROLE_SINK) {
+		if (pe->cur_drole != DATA_ROLE_UFP)
+			/* Reset data role to NONE */
+			pe_set_data_role(pe, DATA_ROLE_NONE);
+	} else
+		log_err("Unexpected pwr role =%d", pe->cur_prole);
+	/*VCONN Off*/
+	devpolicy_set_vconn_state(pe->p.dpm, false);
+}
+
+static void pe_do_dpm_reset_complete(struct policy_engine *pe)
+{
+	if (pe->cur_prole == POWER_ROLE_SOURCE) {
+		/* VBUS On if source*/
+		devpolicy_set_vbus_state(pe->p.dpm, true);
+		/*VCONN on if source*/
+		devpolicy_set_vconn_state(pe->p.dpm, true);
+		/* Reset data role to DFP*/
+		pe_set_data_role(pe, DATA_ROLE_DFP);
+	} else if (pe->cur_prole == POWER_ROLE_SINK) {
+		/* Reset data role to UFP*/
+		pe_set_data_role(pe, DATA_ROLE_UFP);
+	} else
+		log_err("Unexpected pwr role =%d", pe->cur_prole);
+}
+
+static void pe_do_complete_reset(struct policy_engine *pe)
+{
+	pe_do_self_reset(pe);
+
+	/* Reset things that should not be cleared on normal reset*/
+	pe->hard_reset_counter = 0;
+	pe->is_pp_pd_capable = 0;
+	pe->vbus_status = 0;
+}
+
+static int policy_engine_process_data_msg(struct policy *p,
+				enum pe_event evt, struct pd_packet *pkt)
+{
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+	int data_len = PD_MSG_NUM_DATA_OBJS(&pkt->header);
+
+	log_dbg("Data msg received evt - %d\n", evt);
+	if (data_len > MAX_NUM_DATA_OBJ)
+		data_len = MAX_NUM_DATA_OBJ;
+
+	mutex_lock(&pe->pe_lock);
+	pe->is_pp_pd_capable = true;
+	switch (evt) {
+	case PE_EVT_RCVD_SRC_CAP:
+		if (pe->cur_state == PE_SNK_WAIT_FOR_CAPABILITIES) {
+			pe_cancel_timer(pe, SINK_WAIT_CAP_TIMER);
+			if (pe_is_timer_pending(pe, NO_RESPONSE_TIMER))
+				pe_cancel_timer(pe, NO_RESPONSE_TIMER);
+			pe->hard_reset_counter = 0;
+
+			memcpy(pe->pp_src_pdos.pdo,
+					pkt->data_obj, data_len * 4);
+			pe->pp_src_pdos.num_pdos = data_len;
+			pe_change_state(pe, PE_SNK_EVALUATE_CAPABILITY);
+
+		} else if (pe->cur_state == PE_SNK_READY) {
+			pe_cancel_timer(pe, SINK_REQUEST_TIMER);
+			memcpy(pe->pp_src_pdos.pdo,
+					pkt->data_obj, data_len * 4);
+			pe->pp_src_pdos.num_pdos = data_len;
+			pe_change_state(pe, PE_SNK_EVALUATE_CAPABILITY);
+
+		} else if (pe->cur_state == PE_DR_SRC_GET_SOURCE_CAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			memcpy(pe->pp_src_pdos.pdo,
+					pkt->data_obj, data_len * 4);
+			pe->pp_src_pdos.num_pdos = data_len;
+			pe_store_port_partner_caps(pe, &pe->pp_src_pdos);
+			pe_change_state(pe, PE_SRC_READY);
+
+		} else {
+			log_warn("SrcCaps received in wrong state=%d\n",
+					pe->cur_state);
+		}
+		break;
+	case PE_EVT_RCVD_REQUEST:
+		if (pe->cur_state == PE_SRC_SEND_CAPABILITIES) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			memcpy(&pe->pp_sink_req_caps,
+				&pkt->data_obj[0],
+				sizeof(struct pd_fixed_var_rdo));
+			pe_change_state(pe, PE_SRC_NEGOTIATE_CAPABILITY);
+		} else {
+			log_warn("Request msg received in wrong state=%d",
+					pe->cur_state);
+		}
+		break;
+
+	case PE_EVT_RCVD_VDM:
+		if (pe_is_timer_pending(pe, VMD_RESPONSE_TIMER))
+			pe_cancel_timer(pe, VMD_RESPONSE_TIMER);
+		pe_handle_vendor_msg(pe, pkt);
+		break;
+
+	case PE_EVT_RCVD_SNK_CAP:
+		if (pe->cur_state == PE_SRC_GET_SINK_CAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			/* TODO: Process sink caps */
+			pe_change_state(pe, PE_SRC_READY);
+		} else if (pe->cur_state == PE_DR_SNK_GET_SINK_CAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			/* TODO: Process sink caps */
+			pe_change_state(pe, PE_SNK_READY);
+		}
+		memcpy(pe->pp_snk_pdos.pdo,
+					pkt->data_obj, data_len * 4);
+		pe->pp_snk_pdos.num_pdos = data_len;
+		pe_store_port_partner_caps(pe, &pe->pp_snk_pdos);
+		break;
+
+	case PE_EVT_RCVD_BIST:
+	default:
+		log_warn("Invalid data msg, event=%d\n", evt);
+		pe_dump_data_msg(pkt);
+	}
+
+	mutex_unlock(&pe->pe_lock);
+	return 0;
+}
+
+static void pe_handle_gcrc_received(struct policy_engine *pe)
+{
+	switch (pe->cur_state) {
+
+	case PE_SNK_SELECT_CAPABILITY:
+	case PE_SRC_GET_SINK_CAP:
+	case PE_DR_SNK_GET_SINK_CAP:
+	case PE_DR_SRC_GET_SOURCE_CAP:
+	case PE_DRS_DFP_UFP_SEND_DR_SWAP:
+	case PE_DRS_UFP_DFP_SEND_DR_SWAP:
+	case PE_PRS_SRC_SNK_SEND_PR_SWAP:
+	case PE_PRS_SNK_SRC_SEND_PR_SWAP:
+		/* Start sender response timer */
+		pe_start_timer(pe, SENDER_RESPONSE_TIMER,
+					PE_TIME_SENDER_RESPONSE);
+		break;
+
+	case PE_SNK_GIVE_SINK_CAP:
+	case PE_SNK_GET_SOURCE_CAP:
+	case PE_DR_SNK_GIVE_SOURCE_CAP:
+		pe_change_state(pe, PE_SNK_READY);
+		break;
+
+	case PE_SRC_GIVE_SOURCE_CAP:
+	case PE_DR_SRC_GIVE_SINK_CAP:
+		pe_change_state(pe, PE_SRC_READY);
+		break;
+
+	case PE_SRC_SEND_CAPABILITIES:
+		if (pe_is_timer_pending(pe, NO_RESPONSE_TIMER))
+			pe_cancel_timer(pe, NO_RESPONSE_TIMER);
+		pe->hard_reset_counter = 0;
+		pe->src_caps_couner = 0;
+		break;
+
+	case PE_SRC_NEGOTIATE_CAPABILITY:
+		if (pe->last_sent_evt == PE_EVT_SEND_REJECT)
+			pe_change_state(pe, PE_SRC_CAPABILITY_RESPONSE);
+		else if (pe->last_sent_evt == PE_EVT_SEND_ACCEPT)
+			pe_change_state(pe, PE_SRC_TRANSITION_SUPPLY);
+		else
+			log_warn("Error PE_SRC_NEGOTIATE_CAPABILITY");
+		break;
+
+	case PE_SRC_TRANSITION_SUPPLY:
+		log_info("PD Configured in Source Mode !!!!!!");
+		pe->pd_explicit_contract = true;
+		/* Move to ready state */
+		pe_change_state(pe, PE_SRC_READY);
+		break;
+
+	case PE_DRS_DFP_UFP_ACCEPT_DR_SWAP:
+		/* GCRC received for accept, switch to ufp*/
+		pe_change_state(pe, PE_DRS_DFP_UFP_CHANGE_TO_UFP);
+		break;
+
+	case PE_DRS_UFP_DFP_ACCEPT_DR_SWAP:
+		/* GCRC received for accept, switch to dfp*/
+		pe_change_state(pe, PE_DRS_UFP_DFP_CHANGE_TO_DFP);
+		break;
+
+	case PE_PRS_SRC_SNK_REJECT_PR_SWAP:
+		pe_change_state(pe, PE_SRC_READY);
+		break;
+
+	case PE_PRS_SNK_SRC_REJECT_PR_SWAP:
+		pe_change_state(pe, PE_SNK_READY);
+		break;
+
+	case PE_PRS_SRC_SNK_ACCEPT_PR_SWAP:
+		pe_change_state(pe, PE_PRS_SRC_SNK_TRANSITION_TO_OFF);
+		break;
+
+	case PE_PRS_SNK_SRC_ACCEPT_PR_SWAP:
+		pe_change_state(pe, PE_PRS_SNK_SRC_TRANSITION_TO_OFF);
+		break;
+
+	case PE_PRS_SRC_SNK_WAIT_SOURCE_ON:
+		pe_start_timer(pe, PS_SOURCE_ON_TIMER,
+						PE_TIME_PS_SOURCE_ON);
+		break;
+
+	case PE_PRS_SNK_SRC_SOURCE_ON:
+		log_info("PR_SWAP SNK -> SRC success!!");
+		pe_change_state(pe, PE_SRC_STARTUP);
+		break;
+
+	case PE_DFP_UFP_VDM_IDENTITY_REQUEST:
+	case PE_DFP_VDM_SVIDS_REQUEST:
+	case PE_DFP_VDM_MODES_REQUEST:
+	case PE_DFP_VDM_MODES_ENTRY_REQUEST:
+	case PE_DFP_VDM_STATUS_REQUEST:
+	case PE_DFP_VDM_CONF_REQUEST:
+		pe_start_timer(pe, VMD_RESPONSE_TIMER,
+				PE_TIME_VDM_SENDER_RESPONSE);
+		break;
+	default:
+		log_warn("GCRC received in wrong state=%d",
+					pe->cur_state);
+	}
+}
+
+static int policy_engine_process_ctrl_msg(struct policy *p,
+				enum pe_event evt, struct pd_packet *pkt)
+{
+	int ret = 0;
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+
+	log_dbg("Ctrl msg received evt - %d\n", evt);
+
+	mutex_lock(&pe->pe_lock);
+	pe->is_pp_pd_capable = true;
+	switch (evt) {
+	case PE_EVT_RCVD_GOODCRC:
+		pe_cancel_timer(pe, CRC_RECEIVE_TIMER);
+		pe->is_gcrc_received = true;
+		pe_handle_gcrc_received(pe);
+
+		break;
+
+	case PE_EVT_RCVD_ACCEPT:
+		if (pe->cur_state == PE_SNK_SELECT_CAPABILITY) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_SNK_TRANSITION_SINK);
+
+		} else if (pe->cur_state == PE_DRS_DFP_UFP_SEND_DR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_DRS_DFP_UFP_CHANGE_TO_UFP);
+
+		} else if (pe->cur_state == PE_DRS_UFP_DFP_SEND_DR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_DRS_UFP_DFP_CHANGE_TO_DFP);
+
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_SEND_PR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_PRS_SRC_SNK_TRANSITION_TO_OFF);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_SEND_PR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_PRS_SNK_SRC_TRANSITION_TO_OFF);
+
+		} else {
+			log_warn("Accept received in wrong state=%d",
+					pe->cur_state);
+		}
+		break;
+	case PE_EVT_RCVD_REJECT:
+		if (pe->cur_state == PE_SNK_SELECT_CAPABILITY) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe->last_rcv_evt = evt;
+			if (pe->pd_explicit_contract)
+				pe_change_state(pe, PE_SNK_READY);
+			else
+				pe_change_state(pe,
+					PE_SNK_WAIT_FOR_CAPABILITIES);
+
+		} else if (pe->cur_state == PE_DRS_DFP_UFP_SEND_DR_SWAP ||
+				pe->cur_state == PE_DRS_UFP_DFP_SEND_DR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			log_info("DR SWAP Rejected");
+			pe_change_state_to_snk_or_src_ready(pe);
+
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_SEND_PR_SWAP) {
+			log_dbg("PR SWAP Rejected");
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe->is_pr_swap_rejected = true;
+			pe_change_state(pe, PE_SRC_READY);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_SEND_PR_SWAP) {
+			log_dbg("PR SWAP Rejected");
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_SNK_READY);
+
+		} else {
+			log_warn("Reject received in wrong state=%d",
+					pe->cur_state);
+		}
+		break;
+	case PE_EVT_RCVD_WAIT:
+		if (pe->cur_state == PE_SNK_SELECT_CAPABILITY) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe->last_rcv_evt = evt;
+			if (pe->pd_explicit_contract)
+				pe_change_state(pe, PE_SNK_READY);
+			else
+				pe_change_state(pe,
+					PE_SNK_WAIT_FOR_CAPABILITIES);
+
+		} else if (pe->cur_state == PE_DRS_DFP_UFP_SEND_DR_SWAP ||
+				pe->cur_state == PE_DRS_UFP_DFP_SEND_DR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state_to_snk_or_src_ready(pe);
+
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_SEND_PR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_SRC_READY);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_SEND_PR_SWAP) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			pe_change_state(pe, PE_SNK_READY);
+
+		} else {
+			log_warn("Wait received in wrong state=%d",
+					pe->cur_state);
+		}
+		break;
+	case PE_EVT_RCVD_PS_RDY:
+		if (pe->cur_state == PE_SNK_TRANSITION_SINK) {
+			pe_cancel_timer(pe, PS_TRANSITION_TIMER);
+			if (pe->prev_state == PE_SNK_SELECT_CAPABILITY)
+				pe->pd_explicit_contract = true;
+			pe_change_state(pe, PE_SNK_READY);
+			log_info("Sink Negotiation Success !!!!");
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_WAIT_SOURCE_ON) {
+			log_info("PR_SWAP SRC -> SNK Success!!");
+			pe_cancel_timer(pe, PS_SOURCE_ON_TIMER);
+			pe_set_power_role(pe, POWER_ROLE_SINK);
+			pe_change_state(pe, PE_SNK_STARTUP);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_TRANSITION_TO_OFF) {
+			log_dbg("PS_RDY received from src during pr_swap");
+			pe_cancel_timer(pe, PS_SOURCE_OFF_TIMER);
+			pe_change_state(pe, PE_PRS_SNK_SRC_ASSERT_RP);
+
+		} else {
+			log_warn("PsRdy received in wrong state=%d",
+					pe->cur_state);
+		}
+		break;
+	case PE_EVT_RCVD_GET_SINK_CAP:
+		if (pe->cur_state == PE_SNK_READY)
+			pe_change_state(pe, PE_SNK_GIVE_SINK_CAP);
+		else if (pe->cur_state == PE_SRC_READY)
+			pe_change_state(pe, PE_DR_SRC_GIVE_SINK_CAP);
+		else
+			pe_send_self_sink_caps(pe);
+		break;
+
+	case PE_EVT_RCVD_GET_SRC_CAP:
+		if (pe->cur_state == PE_SNK_READY)
+			pe_change_state(pe, PE_DR_SNK_GIVE_SOURCE_CAP);
+		else if (pe->cur_state == PE_SRC_READY)
+			pe_change_state(pe, PE_SRC_GIVE_SOURCE_CAP);
+		else
+			log_info("Cannot provide source caps in state=%d",
+							pe->cur_state);
+		break;
+
+	case PE_EVT_RCVD_DR_SWAP:
+		if (pe->is_modal_operation) {
+			log_warn(" DR Swap cannot perform in modal operation");
+			pe_change_state_to_snk_or_src_reset(pe);
+
+		} else if (pe->cur_state == PE_SRC_READY
+				|| pe->cur_state == PE_SNK_READY) {
+			if (pe->cur_drole == DATA_ROLE_DFP)
+				pe_change_state(pe,
+					PE_DRS_DFP_UFP_EVALUATE_DR_SWAP);
+			else if (pe->cur_drole == DATA_ROLE_UFP)
+				pe_change_state(pe,
+					PE_DRS_UFP_DFP_EVALUATE_DR_SWAP);
+			else {
+				log_err("Error in current data role %d",
+					pe->cur_drole);
+				pe_change_state_to_snk_or_src_reset(pe);
+			}
+		} else {
+			log_info("PE not ready to process DR_SWAP, state=%d",
+					pe->cur_state);
+			pe_send_packet(pe, NULL, 0,
+				PD_CTRL_MSG_WAIT, PE_EVT_SEND_WAIT);
+		}
+		break;
+
+	case PE_EVT_RCVD_PR_SWAP:
+		if (pe->cur_state == PE_SRC_READY)
+			pe_change_state(pe, PE_PRS_SRC_SNK_EVALUATE_PR_SWAP);
+		else if (pe->cur_state == PE_SNK_READY)
+			pe_change_state(pe, PE_PRS_SNK_SRC_EVALUATE_PR_SWAP);
+		else
+			/* Send Wait */
+			pe_send_packet(pe, NULL, 0,
+					PD_CTRL_MSG_WAIT, PE_EVT_SEND_WAIT);
+		break;
+
+	case PE_EVT_RCVD_GOTOMIN:
+	case PE_EVT_RCVD_PING:
+	case PE_EVT_RCVD_VCONN_SWAP:
+		break;
+	default:
+		log_warn("Not a valid ctrl msg to process, event=%d\n", evt);
+		pe_dump_header(&pkt->header);
+	}
+	mutex_unlock(&pe->pe_lock);
+	return ret;
+}
+
+static int policy_engine_process_cmd(struct policy *p,
+				enum pe_event evt)
+{
+	int ret = 0;
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+
+	log_dbg("cmd %d\n", evt);
+	mutex_lock(&pe->pe_lock);
+	switch (evt) {
+	case PE_EVT_RCVD_HARD_RESET:
+		if (pe->cur_prole == POWER_ROLE_SOURCE) {
+			log_dbg("HardReset received in Src role");
+			pe_change_state(pe, PE_SRC_HARD_RESET_RECEIVED);
+		} else if (pe->cur_prole == POWER_ROLE_SINK) {
+			log_dbg("HardReset received in Sink role");
+			pe_change_state(pe, PE_SNK_HARD_RESET_RECEIVED);
+		} else
+			log_err("HardReset received in unknown role=%d",
+					pe->cur_prole);
+		break;
+	case PE_EVT_RCVD_HARD_RESET_COMPLETE:
+		if (pe->cur_state == PE_SNK_HARD_RESET) {
+			pe_cancel_timer(pe, HARD_RESET_COMPLETE_TIMER);
+			pe_change_state(pe, PE_SNK_TRANSITION_TO_DEFAULT);
+		}
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	mutex_unlock(&pe->pe_lock);
+	return ret;
+}
+
+
+static void pe_dump_header(struct pd_pkt_header *header)
+{
+#ifdef DBG
+	if (!header) {
+		pr_err("PE: No Header information available...\n");
+		return;
+	}
+	pr_info("========== POLICY ENGINE: HEADER INFO ==========\n");
+	pr_info("PE: Message Type - 0x%x\n", header->msg_type);
+	pr_info("PE: Reserved B4 - 0x%x\n", header->rsvd_a);
+	pr_info("PE: Port Data Role - 0x%x\n", header->data_role);
+	pr_info("PE: Specification Revision - 0x%x\n", header->rev_id);
+	pr_info("PE: Port Power Role - 0x%x\n", header->pwr_role);
+	pr_info("PE: Message ID - 0x%x\n", header->msg_id);
+	pr_info("PE: Number of Data Objects - 0x%x\n", header->num_data_obj);
+	pr_info("PE: Reserved B15 - 0x%x\n", header->rsvd_b);
+	pr_info("=============================================");
+#endif /* DBG */
+}
+
+static void pe_dump_data_msg(struct pd_packet *pkt)
+{
+#ifdef DBG
+	int num_data_objs = PD_MSG_NUM_DATA_OBJS(&pkt->header);
+	unsigned int data_buf[num_data_objs];
+	int i;
+
+	memset(data_buf, 0, num_data_objs);
+	memcpy(data_buf, &pkt->data_obj, PD_MSG_LEN(&pkt->header));
+
+	for (i = 0; i < num_data_objs; i++) {
+		pr_info("PE: Data Message - data[%d]: 0x%08x\n",
+					i+1, data_buf[i]);
+	}
+#endif /* DBG */
+}
+
+
+int pe_send_packet(struct policy_engine *pe, void *data, int len,
+				u8 msg_type, enum pe_event evt)
+{
+	int ret = 0;
+	bool is_crc_timer_req = true;
+
+	switch (evt) {
+	case PE_EVT_SEND_GOTOMIN:
+	case PE_EVT_SEND_ACCEPT:
+	case PE_EVT_SEND_REJECT:
+	case PE_EVT_SEND_WAIT:
+	case PE_EVT_SEND_PING:
+	case PE_EVT_SEND_PS_RDY:
+	case PE_EVT_SEND_GET_SRC_CAP:
+	case PE_EVT_SEND_GET_SINK_CAP:
+	case PE_EVT_SEND_SRC_CAP:
+	case PE_EVT_SEND_SNK_CAP:
+	case PE_EVT_SEND_DR_SWAP:
+	case PE_EVT_SEND_PR_SWAP:
+	case PE_EVT_SEND_VCONN_SWAP:
+	case PE_EVT_SEND_REQUEST:
+	case PE_EVT_SEND_BIST:
+	case PE_EVT_SEND_VDM:
+	case PE_EVT_SEND_SOFT_RESET:
+		break;
+	case PE_EVT_SEND_HARD_RESET:
+	case PE_EVT_SEND_PROTOCOL_RESET:
+		is_crc_timer_req = false;
+		break;
+	default:
+		ret = -EINVAL;
+		log_err("Cannot send Unknown evt=%d", evt);
+		goto snd_pkt_err;
+	}
+
+	/* Send the pd_packet to protocol */
+	pe->is_gcrc_received = false;
+	log_dbg("Sending pkt, evt=%d", evt);
+	if (pe->p.prot && pe->p.prot->policy_fwd_pkt)
+		pe->p.prot->policy_fwd_pkt(pe->p.prot, msg_type, data, len);
+
+	pe->last_sent_evt = evt;
+	if (is_crc_timer_req)
+		pe_start_timer(pe, CRC_RECEIVE_TIMER, PE_TIME_RECEIVE);
+snd_pkt_err:
+	return ret;
+}
+
+void pe_change_state_to_snk_or_src_ready(struct policy_engine *pe)
+{
+	if (pe->cur_prole == POWER_ROLE_SOURCE)
+		pe_change_state(pe, PE_SRC_READY);
+	else if (pe->cur_prole == POWER_ROLE_SINK)
+		pe_change_state(pe, PE_SNK_READY);
+	else {
+		log_err("Unexpected power role %d!!", pe->cur_prole);
+		pe_change_state(pe, ERROR_RECOVERY);
+	}
+}
+
+static void pe_change_state_to_snk_or_src_reset(struct policy_engine *pe)
+{
+	if (pe->cur_prole == POWER_ROLE_SOURCE)
+		pe_change_state(pe, PE_SRC_HARD_RESET);
+	else if (pe->cur_prole == POWER_ROLE_SINK)
+		pe_change_state(pe, PE_SNK_HARD_RESET);
+	else {
+		log_err("Unexpected power role %d!!", pe->cur_prole);
+		pe_change_state(pe, ERROR_RECOVERY);
+	}
+}
+
+static inline void policy_prot_update_data_role(struct policy_engine *pe,
+				enum data_role drole)
+{
+	if (pe->p.prot->policy_update_data_role)
+		pe->p.prot->policy_update_data_role(pe->p.prot, drole);
+}
+
+static inline void policy_prot_update_power_role(struct policy_engine *pe,
+				enum pwr_role prole)
+{
+	if (pe->p.prot->policy_update_power_role)
+		pe->p.prot->policy_update_power_role(pe->p.prot, prole);
+}
+
+static enum data_role pe_get_data_role(struct policy *p)
+{
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+	enum data_role drole;
+
+	mutex_lock(&pe->pe_lock);
+	drole = pe->cur_drole;
+	mutex_unlock(&pe->pe_lock);
+	return drole;
+}
+
+static enum pwr_role pe_get_power_role(struct policy *p)
+{
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+	enum pwr_role prole;
+
+	mutex_lock(&pe->pe_lock);
+	prole = pe->cur_prole;
+	mutex_unlock(&pe->pe_lock);
+	return prole;
+}
+
+
+static void pe_set_data_role(struct policy_engine *pe, enum data_role role)
+{
+	if (pe->cur_drole == role)
+		return;
+
+	pe->cur_drole = role;
+	devpolicy_update_data_role(pe->p.dpm, role);
+	/* If role swap, no need to update protocol */
+	if (role != DATA_ROLE_SWAP) {
+		/* Update the protocol */
+		policy_prot_update_data_role(pe, role);
+	}
+}
+
+static void pe_set_power_role(struct policy_engine *pe, enum pwr_role role)
+{
+	if (pe->cur_prole == role)
+		return;
+
+	pe->cur_prole = role;
+	devpolicy_update_power_role(pe->p.dpm, role);
+	/* If role swap, no need to update protocol */
+	if (role != POWER_ROLE_SWAP) {
+		/* Update the protocol */
+		policy_prot_update_power_role(pe, role);
+	}
+}
+
+static void pe_handle_dpm_event(struct policy_engine *pe,
+					enum devpolicy_mgr_events evt)
+{
+
+	log_info("event - %d\n", evt);
+	mutex_lock(&pe->pe_lock);
+	switch (evt) {
+	case DEVMGR_EVENT_UFP_CONNECTED:
+		log_dbg(" UFP - Connected ");
+		pe_set_data_role(pe, DATA_ROLE_UFP);
+		pe_set_power_role(pe, POWER_ROLE_SINK);
+		if (pe->cur_state == PE_STATE_NONE) {
+			pe_change_state(pe, PE_SNK_STARTUP);
+		} else {
+			/*
+			 * This could occur due to error in pe state.
+			 * Transition to sink default.
+			 */
+			log_err("PE in wrong state=%d on ufp connect",
+					pe->cur_state);
+			pe_do_complete_reset(pe);
+			pe_do_dpm_reset_entry(pe);
+			pe_change_state(pe, PE_SNK_TRANSITION_TO_DEFAULT);
+		}
+		break;
+	case DEVMGR_EVENT_DFP_CONNECTED:
+		log_dbg(" DFP - Connected ");
+		pe_set_data_role(pe, DATA_ROLE_DFP);
+		pe_set_power_role(pe, POWER_ROLE_SOURCE);
+		if (pe->cur_state == PE_STATE_NONE) {
+			pe_change_state(pe, PE_SRC_STARTUP);
+		} else {
+			/*
+			 * This could occur due to error in pe state.
+			 * Transition to src default.
+			 */
+			log_err("PE in wrong state=%d on dfp connect",
+					pe->cur_state);
+			pe_do_complete_reset(pe);
+			pe_do_dpm_reset_entry(pe);
+			pe_change_state(pe, PE_SRC_TRANSITION_TO_DEFAULT);
+		}
+		break;
+	case DEVMGR_EVENT_UFP_DISCONNECTED:
+	case DEVMGR_EVENT_DFP_DISCONNECTED:
+		pe_change_state(pe, PE_STATE_NONE);
+		break;
+	case DEVMGR_EVENT_VBUS_OFF:
+		log_dbg("VBUS turned OFF");
+		if (pe->cur_state == PE_SNK_WAIT_FOR_HARD_RESET_VBUS_OFF) {
+			/* VBUS off due to reset, move to discovery*/
+			pe_cancel_timer(pe, VBUS_CHECK_TIMER);
+			pe_change_state(pe, PE_SNK_DISCOVERY);
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_TRANSITION_TO_OFF) {
+			pe_cancel_timer(pe, VBUS_CHECK_TIMER);
+			cancel_delayed_work(&pe->vbus_poll_work);
+			pe_change_state(pe, PE_PRS_SRC_SNK_ASSERT_RD);
+		}
+		break;
+
+	case DEVMGR_EVENT_VBUS_ON:
+		log_dbg("VBUS turned ON");
+		if (pe->cur_state == PE_SNK_DISCOVERY) {
+			/* VBUS on, wait for scrcap*/
+			cancel_delayed_work(&pe->vbus_poll_work);
+			pe_change_state(pe, PE_SNK_WAIT_FOR_CAPABILITIES);
+		} else if (pe->cur_state == PE_SRC_WAIT_FOR_VBUS) {
+			/* VBUS on, send SrcCap*/
+			pe_cancel_timer(pe, VBUS_CHECK_TIMER);
+			cancel_delayed_work(&pe->vbus_poll_work);
+			pe_change_state(pe, PE_SRC_SEND_CAPABILITIES);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_SOURCE_ON) {
+			/* VBUS on, send PS_RDY*/
+			pe_cancel_timer(pe, VBUS_CHECK_TIMER);
+			cancel_delayed_work(&pe->vbus_poll_work);
+			pe_send_packet(pe, NULL, 0,
+					PD_CTRL_MSG_PS_RDY, PE_EVT_SEND_PS_RDY);
+		}
+		break;
+
+	case DEVMGR_EVENT_DR_SWAP:
+		if (pe->is_modal_operation) {
+			log_warn("Cannot perform DR_SWAP in modal operation");
+		} else if (pe->cur_state == PE_SRC_READY
+				|| pe->cur_state == PE_SNK_READY) {
+			if (pe->cur_drole == DATA_ROLE_DFP)
+				pe_change_state(pe,
+					PE_DRS_DFP_UFP_SEND_DR_SWAP);
+			else if (pe->cur_drole == DATA_ROLE_UFP)
+				pe_change_state(pe,
+					PE_DRS_UFP_DFP_SEND_DR_SWAP);
+			else {
+				log_err("Error in current data role %d",
+					pe->cur_drole);
+				pe_change_state_to_snk_or_src_reset(pe);
+			}
+		} else
+			log_info("PE not ready to process DR_SWAP");
+		break;
+
+	case DEVMGR_EVENT_PR_SWAP:
+		if (pe->cur_state == PE_SNK_READY)
+			pe_change_state(pe, PE_PRS_SNK_SRC_SEND_PR_SWAP);
+		else if (pe->cur_state == PE_SRC_READY)
+			pe_change_state(pe, PE_PRS_SRC_SNK_SEND_PR_SWAP);
+		else
+			log_info("Cann't trigger PR_SWAP in state=%d",
+					pe->cur_state);
+		break;
+
+	default:
+		log_err("Unknown dpm event=%d\n", evt);
+	}
+	mutex_unlock(&pe->pe_lock);
+}
+
+static int pe_dpm_notification(struct policy *p,
+				enum devpolicy_mgr_events evt)
+{
+	struct policy_engine *pe = container_of(p, struct policy_engine, p);
+	pe_handle_dpm_event(pe, evt);
+	return 0;
+}
+
+/************************** PE helper functions ******************/
+
+static void pe_fill_default_self_sink_cap(struct policy_engine *pe)
+{
+	struct pd_sink_fixed_pdo pdo = { 0 };
+
+	/* setting default pdo as vsafe5V with 500mA*/
+	pdo.data_role_swap = FEATURE_SUPPORTED;
+	pdo.usb_comm = FEATURE_SUPPORTED;
+	pdo.ext_powered = FEATURE_NOT_SUPPORTED;
+	pdo.higher_cap = FEATURE_NOT_SUPPORTED;
+	pdo.dual_role_pwr = FEATURE_SUPPORTED;
+
+	pdo.supply_type = SUPPLY_TYPE_FIXED;
+	pdo.max_cur = CURRENT_TO_DATA_OBJ(500);
+	pdo.volt = (VOLT_TO_DATA_OBJ(5000) >>
+				SNK_FSPDO_VOLT_SHIFT);
+
+	memcpy(pe->self_snk_pdos.pdo, &pdo, sizeof(pdo));
+	pe->self_snk_pdos.num_pdos = 1;
+}
+
+
+static void pe_fetch_self_sink_cap(struct policy_engine *pe)
+{
+	int ret = 0;
+	int i;
+	struct power_caps pcaps;
+	struct pd_sink_fixed_pdo pdo[MAX_NUM_DATA_OBJ] = { {0} };
+
+
+	ret = devpolicy_get_snkpwr_caps(pe->p.dpm, &pcaps);
+	if (ret < 0 || pcaps.n_cap <= 0) {
+		log_info("No PDO's from dpm setting default vasafe5v\n");
+		pe_fill_default_self_sink_cap(pe);
+		return;
+	}
+
+	if (pcaps.n_cap > MAX_NUM_DATA_OBJ)
+		pcaps.n_cap = MAX_NUM_DATA_OBJ;
+
+	/*
+	 * As per PD v1.1 spec except first pdo, all other Fixed Supply Power
+	 * Data Objects shall set bits 29...20 to zero.
+	 */
+	/*
+	 * FIXME: DPM should provide info on USB capable and
+	 * higher power support required.
+	 */
+	pdo[0].data_role_swap = FEATURE_SUPPORTED;
+	pdo[0].usb_comm = FEATURE_SUPPORTED;
+	pdo[0].ext_powered = FEATURE_NOT_SUPPORTED;
+	pdo[0].higher_cap = FEATURE_SUPPORTED;
+	pdo[0].dual_role_pwr = FEATURE_SUPPORTED;
+
+	for (i = 0; i < pcaps.n_cap; i++) {
+
+		pdo[i].max_cur = CURRENT_TO_DATA_OBJ(pcaps.pcap[i].ma);
+		pdo[i].volt = (VOLT_TO_DATA_OBJ(pcaps.pcap[i].mv) >>
+					SNK_FSPDO_VOLT_SHIFT);
+		pdo[i].supply_type = pcaps.pcap[i].psy_type;
+	}
+
+	memcpy(pe->self_snk_pdos.pdo, pdo,
+			pcaps.n_cap * sizeof(struct pd_sink_fixed_pdo));
+	pe->self_snk_pdos.num_pdos = pcaps.n_cap;
+}
+
+static void pe_send_self_sink_caps(struct policy_engine *pe)
+{
+	int size, ret;
+
+	if (pe->self_snk_pdos.num_pdos <= 0)
+		pe_fetch_self_sink_cap(pe);
+
+	size = pe->self_snk_pdos.num_pdos * 4;
+
+	ret = pe_send_packet(pe, pe->self_snk_pdos.pdo, size,
+			PD_DATA_MSG_SINK_CAP, PE_EVT_SEND_SNK_CAP);
+	if (ret < 0)
+		log_err("Failed to send sink caps");
+	return;
+}
+
+/*
+ * This function will pick one to received caps fron source
+ * based on current sysntem required caps given by DPM.
+ */
+static int pe_sink_set_request_cap(struct policy_engine *pe,
+					struct pe_port_pdos *rcv_pdos,
+					struct power_cap *pcap,
+					struct pe_req_cap *rcap)
+{
+	int i;
+	int mv = 0;
+	int ma = 0;
+	bool is_mv_match = false;
+
+	rcap->cap_mismatch = true;
+
+	for (i = 0; i < rcv_pdos->num_pdos; i++) {
+		/*
+		 * FIXME: should be selected based on the power (V*I) cap.
+		 */
+		mv = DATA_OBJ_TO_VOLT(rcv_pdos->pdo[i]);
+		if (mv == pcap->mv) {
+			is_mv_match = true;
+			ma = DATA_OBJ_TO_CURRENT(rcv_pdos->pdo[i]);
+			if (ma == pcap->ma) {
+				rcap->cap_mismatch = false;
+				break;
+			} else if (ma > pcap->ma) {
+				/*
+				 * If the ma in the pdo is greater than the
+				 * required ma, exit from the loop as the pdo
+				 * capabilites are in ascending order.
+				 */
+				break;
+			}
+		} else if (mv > pcap->mv) {
+			/*
+			 * If the mv value in the pdo is greater than the
+			 * required mv, exit from the loop as the pdo
+			 * capabilites are in ascending order.
+			 */
+			break;
+		}
+	}
+
+	if (!is_mv_match)
+		i = 0; /* to select 1st pdo, Vsafe5V */
+
+	if (!rcap->cap_mismatch) {
+		rcap->obj_pos = i + 1; /* obj pos always starts from 1 */
+		rcap->max_ma = pcap->ma;
+		rcap->op_ma = pcap->ma;
+	} else  {
+		/* if cur is not match, select the previous pdo */
+		rcap->obj_pos = i ? i : 1;
+		rcap->op_ma =
+			DATA_OBJ_TO_CURRENT(rcv_pdos->pdo[rcap->obj_pos - 1]);
+
+		if (pcap->ma < rcap->op_ma) {
+			rcap->cap_mismatch = false;
+			rcap->max_ma = rcap->op_ma;
+			rcap->op_ma = pcap->ma;
+		} else {
+			rcap->max_ma = pcap->ma;
+		}
+	}
+
+	rcap->mv = DATA_OBJ_TO_VOLT(rcv_pdos->pdo[rcap->obj_pos - 1]);
+
+	return 0;
+}
+
+/*
+ * This function will read the port partner capabilities and
+ * save it for further use.
+ */
+static void pe_store_port_partner_caps(struct policy_engine *pe,
+					struct pe_port_pdos *pdos)
+{
+	struct pd_fixed_supply_pdo *pdo;
+	int num;
+
+	if (!pdos) {
+		log_err("No pdos");
+		return;
+	}
+	for (num = 0; num < pdos->num_pdos; num++) {
+		pdo = (struct pd_fixed_supply_pdo *) &pdos->pdo[num];
+
+		if (pdo->fixed_supply == SUPPLY_TYPE_FIXED) {
+			pe->pp_caps.pp_is_dual_prole = pdo->dual_role_pwr;
+			pe->pp_caps.pp_is_dual_drole = pdo->data_role_swap;
+			pe->pp_caps.pp_is_ext_pwrd = pdo->ext_powered;
+
+			log_info("dual_prole=%d, dual_drole=%d, ext_pwrd=%d",
+					pe->pp_caps.pp_is_dual_prole,
+					pe->pp_caps.pp_is_dual_drole,
+					pe->pp_caps.pp_is_ext_pwrd);
+			return;
+		}
+	}
+	log_warn("port partner is not fixed supply, num_pdos=%d",
+					pdos->num_pdos);
+}
+
+static void pe_sink_evaluate_src_caps(struct policy_engine *pe,
+					struct pe_port_pdos *src_pdos)
+{
+	struct pe_req_cap *rcap = &pe->self_sink_req_cap;
+	struct power_cap dpm_suggested_cap;
+	int ret;
+
+	/* Get current system required caps from DPM*/
+	ret = devpolicy_get_snkpwr_cap(pe->p.dpm, &dpm_suggested_cap);
+	if (ret) {
+		log_err("Error in getting max sink pwr cap from DPM %d",
+				ret);
+		goto error;
+	}
+
+	ret = pe_sink_set_request_cap(pe, src_pdos, &dpm_suggested_cap, rcap);
+	if (ret < 0) {
+		log_err("Failed to get request caps");
+		goto error;
+	}
+
+	log_dbg("Request index=%d, volt=%u, op-cur=%u, max-cur=%u",
+			rcap->obj_pos, rcap->mv, rcap->op_ma, rcap->max_ma);
+	return;
+
+error:
+	memset(rcap, 0, sizeof(struct pe_req_cap));
+	return;
+}
+
+static void pe_sink_create_request_msg_data(struct policy_engine *pe,
+						u32 *data)
+{
+	struct pd_fixed_var_rdo *rdo = (struct pd_fixed_var_rdo *)data;
+	struct pe_req_cap *rcap = &pe->self_sink_req_cap;
+
+	rdo->obj_pos = rcap->obj_pos;
+	rdo->cap_mismatch = rcap->cap_mismatch;
+	rdo->op_cur = CURRENT_TO_DATA_OBJ(rcap->op_ma);
+	rdo->max_cur = CURRENT_TO_DATA_OBJ(rcap->max_ma);
+
+}
+
+static int pe_send_srccap_cmd(struct policy_engine *pe)
+{
+	int ret;
+	struct pd_fixed_supply_pdo *pdo;
+	struct power_cap pcap;
+
+	log_dbg("Sending SrcCap");
+	/* TODO: Support multiple SrcCap PODs */
+	ret = devpolicy_get_srcpwr_cap(pe->p.dpm, &pcap);
+	if (ret) {
+		log_err("Error in getting power capabilities from DPM");
+		return ret;
+	}
+	pdo = (struct pd_fixed_supply_pdo *) &pe->self_src_pdos.pdo[0];
+	memset(pdo, 0, sizeof(struct pd_fixed_supply_pdo));
+	pdo->max_cur = CURRENT_TO_CAP_DATA_OBJ(pcap.ma); /* In 10mA units */
+	pdo->volt = VOLT_TO_CAP_DATA_OBJ(pcap.mv); /* In 50mV units */
+	pdo->peak_cur = 0; /* No peak current supported */
+	pdo->dual_role_pwr = 1; /* Dual pwr role supported */
+	pdo->data_role_swap = 1; /*Dual data role*/
+	pdo->usb_comm = 1; /* USB communication supported */
+
+	ret = pe_send_packet(pe, pdo, 4,
+				PD_DATA_MSG_SRC_CAP, PE_EVT_SEND_SRC_CAP);
+	pe->self_src_pdos.num_pdos = 1;
+	return ret;
+}
+
+/************************ TImer related functions *****************/
+static char *timer_to_str(enum pe_timers timer_type)
+{
+	switch (timer_type) {
+	case BIST_CONT_MODE_TIMER:
+		return "BIST_CONT_MODE_TIMER";
+	case BIST_RECEIVE_ERROR_TIMER:
+		return "BIST_RECEIVE_ERROR_TIMER";
+	case BIST_START_TIMER:
+		return "BIST_START_TIMER";
+	case CRC_RECEIVE_TIMER:
+		return "CRC_RECEIVE_TIMER";
+	case DISCOVER_IDENTITY_TIMER:
+		return "DISCOVER_IDENTITY_TIMER";
+	case HARD_RESET_COMPLETE_TIMER:
+		return "HARD_RESET_COMPLETE_TIMER";
+	case NO_RESPONSE_TIMER:
+		return "NO_RESPONSE_TIMER";
+	case PS_HARD_RESET_TIMER:
+		return "PS_HARD_RESET_TIMER";
+	case PS_SOURCE_OFF_TIMER:
+		return "PS_SOURCE_OFF_TIMER";
+	case PS_SOURCE_ON_TIMER:
+		return "PS_SOURCE_ON_TIMER";
+	case PS_TRANSITION_TIMER:
+		return "PS_TRANSITION_TIMER";
+	case SENDER_RESPONSE_TIMER:
+		return "SENDER_RESPONSE_TIMER";
+	case SINK_ACTIVITY_TIMER:
+		return "SINK_ACTIVITY_TIMER";
+	case SINK_REQUEST_TIMER:
+		return "SINK_REQUEST_TIMER";
+	case SINK_WAIT_CAP_TIMER:
+		return "SINK_WAIT_CAP_TIMER";
+	case SOURCE_ACTIVITY_TIMER:
+		return "SOURCE_ACTIVITY_TIMER";
+	case SOURCE_CAPABILITY_TIMER:
+		return "SOURCE_CAPABILITY_TIMER";
+	case SWAP_RECOVERY_TIMER:
+		return "SWAP_RECOVERY_TIMER";
+	case SWAP_SOURCE_START_TIMER:
+		return "SWAP_SOURCE_START_TIMER";
+	case VCONN_ON_TIMER:
+		return "VCONN_ON_TIMER";
+	case VDM_MODE_ENTRY_TIMER:
+		return "VDM_MODE_ENTRY_TIMER";
+	case VDM_MODE_EXIT_TIMER:
+		return "VDM_MODE_EXIT_TIMER";
+	case VMD_RESPONSE_TIMER:
+		return "VMD_RESPONSE_TIMER";
+	case VBUS_CHECK_TIMER:
+		return "VBUS_CHECK_TIMER";
+	case SRC_RESET_RECOVER_TIMER:
+		return "SRC_RESET_RECOVER_TIMER";
+	case SRC_TRANSITION_TIMER:
+		return "SRC_TRANSITION_TIMER";
+	default:
+		return "Unknown";
+	}
+}
+
+static struct pe_timer *pe_get_timer(struct policy_engine *pe,
+					enum pe_timers timer_type)
+{
+	struct pe_timer *cur_timer = &pe->timers[timer_type];
+
+	if (cur_timer->timer_type == timer_type)
+		return cur_timer;
+
+	log_err("Timer %d not found", timer_type);
+	return NULL;
+}
+
+static bool pe_is_timer_pending(struct policy_engine *pe,
+				enum pe_timers timer_type)
+{
+	struct pe_timer *cur_timer;
+
+	cur_timer = pe_get_timer(pe, timer_type);
+	if (cur_timer)
+		if (timer_pending(&cur_timer->timer))
+			return true;
+
+	return false;
+}
+
+static void pe_start_timer(struct policy_engine *pe,
+				enum pe_timers timer_type, unsigned time)
+{
+	struct pe_timer *cur_timer;
+	int ret;
+
+	cur_timer = pe_get_timer(pe, timer_type);
+	if (!cur_timer)
+		return;
+
+	if (timer_pending(&cur_timer->timer))
+		log_warn("Timer %d already pending!!", timer_type);
+	ret = mod_timer(&cur_timer->timer, jiffies + msecs_to_jiffies(time));
+	log_dbg("%s, time=%u , mod_timer ret=%d",
+			timer_to_str(timer_type), time, ret);
+
+}
+
+static void pe_cancel_timer(struct policy_engine *pe,
+				enum pe_timers timer_type)
+{
+	struct pe_timer *cur_timer;
+
+	cur_timer = pe_get_timer(pe, timer_type);
+	if (!cur_timer)
+		return;
+	if (!del_timer(&cur_timer->timer))
+		log_warn("Timer %s not active!!", timer_to_str(timer_type));
+}
+
+static void pe_timer_expire_worker(struct work_struct *work)
+{
+	struct pe_timer *cur_timer = container_of(work,
+					struct pe_timer, work);
+	struct policy_engine *pe = (struct policy_engine *) cur_timer->data;
+	enum pe_timers type = cur_timer->timer_type;
+
+	mutex_lock(&pe->pe_lock);
+	log_dbg("%s expiration handling!!!",
+			timer_to_str(type));
+	switch (type) {
+
+	case SENDER_RESPONSE_TIMER:
+		if (pe->cur_state == PE_SRC_SEND_CAPABILITIES) {
+			if (pe->is_gcrc_received)
+				pe_change_state(pe, PE_SRC_HARD_RESET);
+			else
+				pe_change_state(pe, PE_SRC_DISCOVERY);
+			break;
+		} else if (pe->cur_state == PE_DRS_UFP_DFP_SEND_DR_SWAP ||
+				pe->cur_state == PE_DRS_DFP_UFP_SEND_DR_SWAP) {
+			log_info("Didn't get response for dr_swap");
+			pe_change_state_to_snk_or_src_ready(pe);
+			break;
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_SEND_PR_SWAP ||
+				pe->cur_state == PE_PRS_SNK_SRC_SEND_PR_SWAP) {
+			log_info("Didn't get response for pr_swap");
+			pe_change_state_to_snk_or_src_ready(pe);
+			break;
+		} else if (pe->cur_state == PE_SRC_GET_SINK_CAP
+				|| pe->cur_state == PE_DR_SRC_GET_SOURCE_CAP) {
+			pe_change_state(pe, PE_SRC_READY);
+			break;
+		} else if (pe->cur_state == PE_DR_SNK_GET_SINK_CAP) {
+			pe_change_state(pe, PE_SNK_READY);
+			break;
+		}
+
+		log_warn("%s expired move to hard reset",
+				timer_to_str(type));
+		pe_change_state_to_snk_or_src_reset(pe);
+		break;
+
+	case SINK_WAIT_CAP_TIMER:
+		if (pe->cur_state == PE_SNK_WAIT_FOR_CAPABILITIES) {
+			log_dbg("%s expired,HardResetCount=%d",
+					timer_to_str(type),
+					pe->hard_reset_counter);
+			if (pe->hard_reset_counter <= PE_N_HARD_RESET_COUNT)
+				pe_change_state(pe, PE_SNK_HARD_RESET);
+			else
+				pe_change_state(pe, ERROR_RECOVERY);
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case SINK_REQUEST_TIMER:
+			if (pe->cur_state == PE_SNK_READY) {
+				/* As source didnt send SrcCaps,
+				 * resend the request.
+				 */
+				pe_change_state(pe, PE_SNK_SELECT_CAPABILITY);
+			} else
+				log_warn("%s expired in wrong state=%d",
+					timer_to_str(type), pe->cur_state);
+		break;
+
+	case NO_RESPONSE_TIMER:
+		log_info("%s Expired!!, state=%d",
+				timer_to_str(type), pe->cur_state);
+		pe->is_no_response_timer_expired = true;
+
+		if (pe->hard_reset_counter <= PE_N_HARD_RESET_COUNT)
+			pe_change_state_to_snk_or_src_reset(pe);
+		else
+				pe_change_state(pe, ERROR_RECOVERY);
+
+		break;
+
+	case PS_TRANSITION_TIMER:
+			if (pe->cur_state == PE_SNK_TRANSITION_SINK)
+				pe_change_state(pe, PE_SNK_HARD_RESET);
+			else
+				log_warn("%s expired in wrong state=%d",
+					timer_to_str(type), pe->cur_state);
+		break;
+
+	case PS_HARD_RESET_TIMER:
+		if (pe->cur_state == PE_SRC_HARD_RESET
+			|| pe->cur_state == PE_SRC_HARD_RESET_RECEIVED)
+			pe_change_state(pe, PE_SRC_TRANSITION_TO_DEFAULT);
+		else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case HARD_RESET_COMPLETE_TIMER:
+		if (pe->cur_state == PE_SNK_HARD_RESET)
+			pe_change_state(pe, PE_SNK_TRANSITION_TO_DEFAULT);
+		else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case BIST_CONT_MODE_TIMER:
+		break;
+
+	case BIST_RECEIVE_ERROR_TIMER:
+		break;
+
+	case BIST_START_TIMER:
+		break;
+
+	case CRC_RECEIVE_TIMER:
+		if (pe->cur_state == PE_SRC_SEND_CAPABILITIES) {
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+			if (!pe->is_pp_pd_capable) {
+				pe_change_state(pe, PE_SRC_DISCOVERY);
+				break;
+			}
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_WAIT_SOURCE_ON) {
+			log_err("PS_RDY Sent fail during pr_swap");
+			pe_change_state(pe, PE_ERROR_RECOVERY);
+			break;
+		}
+		if (pe_is_timer_pending(pe, SENDER_RESPONSE_TIMER))
+			pe_cancel_timer(pe, SENDER_RESPONSE_TIMER);
+		/* TODO: Trigger SoftReset before HardReset */
+		log_warn("%s expired in state=%d",
+				timer_to_str(type), pe->cur_state);
+		pe_change_state_to_snk_or_src_reset(pe);
+		break;
+
+	case DISCOVER_IDENTITY_TIMER:
+		break;
+
+	case PS_SOURCE_OFF_TIMER:
+		if (pe->cur_state == PE_PRS_SNK_SRC_TRANSITION_TO_OFF) {
+			log_err("No PS_RDY from src during pr swap!!");
+			pe_change_state(pe, PE_ERROR_RECOVERY);
+		}
+		break;
+
+	case PS_SOURCE_ON_TIMER:
+		if (pe->cur_state == PE_PRS_SRC_SNK_WAIT_SOURCE_ON) {
+			log_err("PR_SWAP: PS_RDY not received from sink");
+			pe_change_state(pe, PE_ERROR_RECOVERY);
+		}
+		break;
+
+	case SINK_ACTIVITY_TIMER:
+		break;
+
+	case SOURCE_ACTIVITY_TIMER:
+		break;
+
+	case SOURCE_CAPABILITY_TIMER:
+		if (pe->cur_state == PE_SRC_DISCOVERY) {
+			if (pe->src_caps_couner <= PE_N_CAPS_COUNT)
+				pe_change_state(pe, PE_SRC_SEND_CAPABILITIES);
+			else
+				pe_change_state(pe, PE_SRC_DISABLED);
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case SWAP_RECOVERY_TIMER:
+	case SWAP_SOURCE_START_TIMER:
+	case VCONN_ON_TIMER:
+	case VDM_MODE_ENTRY_TIMER:
+	case VDM_MODE_EXIT_TIMER:
+		break;
+	case VMD_RESPONSE_TIMER:
+		if (pe->cur_state == PE_DFP_UFP_VDM_IDENTITY_REQUEST
+			|| pe->cur_state == PE_DFP_VDM_SVIDS_REQUEST
+			|| pe->cur_state == PE_DFP_VDM_MODES_REQUEST
+			|| pe->cur_state == PE_DFP_VDM_MODES_ENTRY_REQUEST
+			|| pe->cur_state == PE_DFP_VDM_STATUS_REQUEST
+			|| pe->cur_state == PE_DFP_VDM_CONF_REQUEST) {
+			log_warn("no response for VDM");
+			pe_change_state_to_snk_or_src_ready(pe);
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case VBUS_CHECK_TIMER:
+		if (pe->cur_state == PE_SRC_WAIT_FOR_VBUS) {
+			log_warn("System did not enable vbus in source mode");
+			cancel_delayed_work(&pe->vbus_poll_work);
+			if (pe->hard_reset_counter > PE_N_HARD_RESET_COUNT)
+				pe_change_state(pe, ERROR_RECOVERY);
+			else
+				pe_change_state(pe, PE_SRC_HARD_RESET);
+
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_TRANSITION_TO_OFF) {
+			cancel_delayed_work(&pe->vbus_poll_work);
+			log_warn("VBUS not off!!!");
+			pe_change_state(pe, ERROR_RECOVERY);
+
+		} else if (pe->cur_state == PE_PRS_SNK_SRC_SOURCE_ON) {
+			cancel_delayed_work(&pe->vbus_poll_work);
+			log_warn("VBUS not on!!!");
+			pe_change_state(pe, PE_ERROR_RECOVERY);
+
+		} else if (pe->cur_state ==
+				PE_SNK_WAIT_FOR_HARD_RESET_VBUS_OFF) {
+			log_warn("Vbus not off for %dmsec, moving to state=%d",
+					T_SRC_RECOVER_MIN, PE_SNK_DISCOVERY);
+			cancel_delayed_work(&pe->vbus_poll_work);
+			pe_change_state(pe, PE_SNK_DISCOVERY);
+
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+	case SRC_RESET_RECOVER_TIMER:
+		if (pe->cur_state == PE_SRC_TRANSITION_TO_DEFAULT) {
+			pe_process_state_pe_src_transition_to_default_exit(pe);
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	case SRC_TRANSITION_TIMER:
+		if (pe->cur_state == PE_SRC_TRANSITION_SUPPLY) {
+			log_info("Sending PS_RDY");
+			pe_send_packet(pe, NULL, 0,
+				PD_CTRL_MSG_PS_RDY, PE_EVT_SEND_PS_RDY);
+
+		} else if (pe->cur_state == PE_PRS_SRC_SNK_TRANSITION_TO_OFF) {
+			pe->vbus_status = devpolicy_get_vbus_state(pe->p.dpm);
+			/* Turn off the VBUS */
+			devpolicy_set_vbus_state(pe->p.dpm, false);
+
+			pe_start_timer(pe, VBUS_CHECK_TIMER, T_SAFE_0V_MAX);
+			/* WA as DPM doesnt have VBUS notifier */
+			schedule_delayed_work(&pe->vbus_poll_work,
+					msecs_to_jiffies(VBUS_POLL_TIME));
+			pe_set_power_role(pe, POWER_ROLE_SWAP);
+		} else
+			log_warn("%s expired in wrong state=%d",
+				timer_to_str(type), pe->cur_state);
+		break;
+
+	default:
+		log_warn("Unknown timer=%d expired!!!",
+				cur_timer->timer_type);
+	}
+	mutex_unlock(&pe->pe_lock);
+}
+
+static void pe_timer_expire_callback(unsigned long data)
+{
+	struct pe_timer *cur_timer = (struct pe_timer *) data;
+
+	log_dbg("%s expired!!!", timer_to_str(cur_timer->timer_type));
+	schedule_work(&cur_timer->work);
+}
+
+/************************ Sink State Handlers ********************/
+
+static void pe_process_state_pe_snk_startup(struct policy_engine *pe)
+{
+	/* Reset protocol layer */
+	pe_send_packet(pe, NULL, 0, PD_CMD_PROTOCOL_RESET,
+				PE_EVT_SEND_PROTOCOL_RESET);
+	/* TODO: Check condition for PE_DB_CP_CHECK_FOR_VBUS */
+
+	/*
+	 * When PE come to this state from hard reset, then pe
+	 * should wait for source to off VBUS on reset.
+	 */
+	if (pe->prev_state == PE_SNK_TRANSITION_TO_DEFAULT) {
+		/* This is due to hard reaset, wait for vbus off*/
+		pe_change_state(pe, PE_SNK_WAIT_FOR_HARD_RESET_VBUS_OFF);
+	} else {
+		/*This is due to connect */
+		pe_change_state(pe, PE_SNK_DISCOVERY);
+	}
+}
+
+static void
+pe_process_state_pe_snk_wait_for_hard_reset_vbus_off(struct policy_engine *pe)
+{
+	pe->vbus_status = devpolicy_get_vbus_state(pe->p.dpm);
+
+	if (pe->vbus_status) {
+		log_info("Vbus present, wait for VBUS off due to reset");
+		/* WA as DPM doesnt have VBUS notifier */
+		schedule_delayed_work(&pe->vbus_poll_work,
+				msecs_to_jiffies(VBUS_POLL_TIME));
+
+		pe_start_timer(pe, VBUS_CHECK_TIMER, T_SRC_RECOVER_MIN);
+		return;
+	}
+	log_dbg("VBUS off, Move to PE_SNK_DISCOVERY");
+	pe_change_state(pe, PE_SNK_DISCOVERY);
+}
+
+static void pe_process_state_pe_snk_discovery(struct policy_engine *pe)
+{
+	pe->vbus_status = devpolicy_get_vbus_state(pe->p.dpm);
+
+	if (!pe->vbus_status) {
+		log_info("Vbus not present, wait for VBUS On");
+		return;
+	}
+	log_dbg("VBUS present, Move to PE_SNK_WAIT_FOR_CAPABILITIES");
+	pe_change_state(pe, PE_SNK_WAIT_FOR_CAPABILITIES);
+}
+
+static void
+pe_process_state_pe_snk_wait_for_capabilities(struct policy_engine *pe)
+{
+	unsigned time_out;
+
+	/* Check condition for ERROR_RECOVERY */
+	if (pe->is_typec_port
+		&& pe->is_no_response_timer_expired
+		&& pe->hard_reset_counter > PE_N_HARD_RESET_COUNT) {
+
+		log_info("ERROR_RECOVERY condition!!!");
+		pe_change_state(pe, ERROR_RECOVERY);
+		return;
+	}
+
+	if (!pe->is_pd_enabled) {
+		devpolicy_enable_pd(pe->p.dpm, true);
+		pe->is_pd_enabled = true;
+	}
+
+	/* Start SinkWaitCapTimer */
+	if (pe->is_typec_port)
+		time_out = PE_TIME_TYPEC_SINK_WAIT_CAP;
+	else
+		time_out = PE_TIME_SINK_WAIT_CAP;
+	pe_start_timer(pe, SINK_WAIT_CAP_TIMER, time_out);
+}
+
+static void
+pe_process_state_pe_snk_evaluate_capabilities(struct policy_engine *pe)
+{
+	pe_sink_evaluate_src_caps(pe, &pe->pp_src_pdos);
+
+	/* As caps got evaluated, mode to pe_snk_select_capability*/
+	pe_change_state(pe, PE_SNK_SELECT_CAPABILITY);
+}
+
+static void
+pe_process_state_pe_snk_select_capability(struct policy_engine *pe)
+{
+	int ret;
+	u32 data = 0;
+
+	/* make request message and send to PE -> protocol */
+	pe_sink_create_request_msg_data(pe, &data);
+
+	ret = pe_send_packet(pe, &data, sizeof(data),
+				PD_DATA_MSG_REQUEST, PE_EVT_SEND_REQUEST);
+	if (ret < 0) {
+		log_err("Error in sending request packet!\n");
+		return;
+	}
+	log_dbg("PD_DATA_MSG_REQUEST Sent, %x\n", data);
+}
+
+static void
+pe_process_state_pe_snk_transition_sink(struct policy_engine *pe)
+{
+	/* Start PS_TRANSITION_TIMER and wait for PS_RDY */
+	pe_start_timer(pe, PS_TRANSITION_TIMER, PE_TIME_PS_TRANSITION);
+	devpolicy_update_charger(pe->p.dpm, 0, true);
+
+	/* TODO: Intimate DPM about new power supply params */
+	return;
+}
+
+static void
+pe_process_state_pe_snk_ready(struct policy_engine *pe)
+{
+	struct pe_req_cap *rcap = &pe->self_sink_req_cap;
+
+	log_info("In SNK_READY");
+
+	/*
+	 * If Wait received for request msg then start
+	 * sink_request_timer to resend the request.
+	 */
+	if (pe->prev_state == PE_SNK_SELECT_CAPABILITY
+		&& pe->last_rcv_evt == PE_EVT_RCVD_WAIT) {
+		pe_start_timer(pe, SINK_REQUEST_TIMER,
+				PE_TIME_SINK_REQUEST);
+	} else if (pe->prev_state == PE_SNK_TRANSITION_SINK) {
+		/* Set new inlimit */
+		devpolicy_update_charger(pe->p.dpm, rcap->op_ma, 0);
+		pe_store_port_partner_caps(pe, &pe->pp_src_pdos);
+		schedule_delayed_work(&pe->post_ready_work,
+				msecs_to_jiffies(PE_AUTO_TRIGGERING_DELAY));
+	} else
+		schedule_delayed_work(&pe->post_ready_work, 0);
+}
+
+static void
+pe_process_state_pe_snk_hard_reset(struct policy_engine *pe)
+{
+	pe_do_self_reset(pe);
+	pe->hard_reset_counter++;
+	pe_send_packet(pe, NULL, 0, PD_CMD_HARD_RESET,
+			PE_EVT_SEND_HARD_RESET);
+	pe_start_timer(pe, HARD_RESET_COMPLETE_TIMER,
+		PE_TIME_HARD_RESET + PE_TIME_HARD_RESET_COMPLETE);
+}
+
+static void
+pe_process_state_pe_snk_transition_to_default(struct policy_engine *pe)
+{
+	/* TODO: Notify DPM about reset to default roles and power*/
+	/*TODO: set data role to UFP, VCONN off*/
+	pe_do_dpm_reset_entry(pe);
+	pe_do_dpm_reset_complete(pe);
+	pe_start_timer(pe, NO_RESPONSE_TIMER, PE_TIME_NO_RESPONSE);
+	pe_change_state(pe, PE_SNK_STARTUP);
+}
+
+static void
+pe_process_state_pe_snk_give_sink_cap(struct policy_engine *pe)
+{
+
+	pe_send_self_sink_caps(pe);
+}
+
+static void
+pe_process_state_pe_snk_get_source_cap(struct policy_engine *pe)
+{
+
+	pe_send_packet(pe, NULL, 0, PD_CTRL_MSG_GET_SINK_CAP,
+					PE_EVT_SEND_GET_SINK_CAP);
+}
+
+static void
+pe_process_state_pe_snk_hard_reset_received(struct policy_engine *pe)
+{
+	pe_do_self_reset(pe);
+	pe_change_state(pe, PE_SNK_TRANSITION_TO_DEFAULT);
+}
+
+/********** Source Port State Handlers **********************/
+static void pe_process_state_pe_src_wait_for_vbus(struct policy_engine *pe)
+{
+	pe->vbus_status = devpolicy_get_vbus_state(pe->p.dpm);
+
+	if (!pe->vbus_status) {
+		log_dbg("VBUS not present, Start %s",
+				timer_to_str(VBUS_CHECK_TIMER));
+		pe_start_timer(pe, VBUS_CHECK_TIMER, T_SAFE_5V_MAX);
+		/* WA as DPM doesnt have VBUS notifier */
+		schedule_delayed_work(&pe->vbus_poll_work,
+				msecs_to_jiffies(VBUS_POLL_TIME));
+	} else {
+		log_dbg("VBUS present, move to PE_SRC_SEND_CAPABILITIES");
+		pe_change_state(pe, PE_SRC_SEND_CAPABILITIES);
+	}
+}
+
+static void pe_process_state_pe_src_startup(struct policy_engine *pe)
+{
+	/* Reset caps counter */
+	pe->src_caps_couner = 0;
+	/* Reset protocol layer */
+	pe_send_packet(pe, NULL, 0, PD_CMD_PROTOCOL_RESET,
+				PE_EVT_SEND_PROTOCOL_RESET);
+
+	/* TODO: start swap src start timer ( only after pw swap) */
+	/* Move to PE_SRC_WAIT_FOR_VBUS to check VBUS before sending SrcCap*/
+	pe_change_state(pe, PE_SRC_WAIT_FOR_VBUS);
+}
+
+static void pe_process_state_pe_src_discovery(struct policy_engine *pe)
+{
+	unsigned time_out;
+
+	if ((!pe->is_typec_port
+		|| (pe->is_typec_port && !pe->is_pp_pd_capable))
+		&& pe->is_no_response_timer_expired
+		&& (pe->hard_reset_counter > PE_N_HARD_RESET_COUNT)) {
+		/* Treat port partner as non PD */
+		pe_change_state(pe, PE_SRC_DISABLED);
+		return;
+	}
+	if (pe->is_typec_port)
+		time_out = PE_TIME_TYPEC_SEND_SOURCE_CAP;
+	else
+		time_out = PE_TIME_SEND_SOURCE_CAP;
+	pe_start_timer(pe, SOURCE_CAPABILITY_TIMER, time_out);
+
+}
+
+static void
+pe_process_state_pe_src_send_capabilities(struct policy_engine *pe)
+{
+	if ((!pe->is_typec_port
+		|| (pe->is_typec_port && !pe->is_pp_pd_capable))
+			&& pe->is_no_response_timer_expired
+			&& (pe->hard_reset_counter > PE_N_HARD_RESET_COUNT)) {
+		/* Treat port partner as non PD */
+		pe_change_state(pe, PE_SRC_DISABLED);
+		return;
+	}
+	if (pe->is_typec_port
+		&& pe->is_pp_pd_capable
+		&& pe->is_no_response_timer_expired
+		&& (pe->hard_reset_counter > PE_N_HARD_RESET_COUNT)) {
+		/* Port partner as PD capable but in error state*/
+		pe_change_state(pe, ERROR_RECOVERY);
+		return;
+	}
+
+	if (!pe->is_pd_enabled) {
+		devpolicy_enable_pd(pe->p.dpm, true);
+		pe->is_pd_enabled = true;
+	}
+	pe->src_caps_couner++;
+
+	if (pe_send_srccap_cmd(pe)) {
+		/* failed to send Srccap */
+		log_dbg("Failed to send SrcCap");
+		pe_change_state(pe, PE_SRC_DISCOVERY);
+		return;
+	}
+	/* SrcCap sent successfuly, start sender response timer*/
+	pe_start_timer(pe, SENDER_RESPONSE_TIMER, PE_TIME_SENDER_RESPONSE);
+
+}
+
+static void
+pe_process_state_pe_src_negotiate_capability(struct policy_engine *pe)
+{
+	struct pd_fixed_var_rdo *snk_rdo = &pe->pp_sink_req_caps;
+	struct pd_fixed_supply_pdo *src_pdo =
+		(struct pd_fixed_supply_pdo *) &pe->self_src_pdos.pdo[0];
+
+	/* TODO: Support multiple request PDOs*/
+	if (snk_rdo->cap_mismatch)
+		log_warn("Capability mismatch!!\n");
+
+	if (snk_rdo->op_cur <= src_pdo->max_cur) {
+		log_dbg("Requested current is less than source cap");
+		pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_ACCEPT, PE_EVT_SEND_ACCEPT);
+		/*Inform DMP to update the new supply*/
+		/*Currently system support only one supply*/
+	} else {
+		log_err("Requested current is more than source cap");
+		pe_send_packet(pe, NULL, 0,
+				PD_CTRL_MSG_REJECT, PE_EVT_SEND_REJECT);
+	}
+}
+
+
+static void
+pe_process_state_pe_src_transition_supply(struct policy_engine *pe)
+{
+	/* Wait for tSrcTransition time to sink to settle*/
+	pe_start_timer(pe, SRC_TRANSITION_TIMER, T_SRC_TRANSITION);
+}
+
+static void
+pe_process_state_pe_src_ready(struct policy_engine *pe)
+{
+	unsigned delay = 0;
+
+	log_dbg("In PE_SRC_READY");
+
+	if (pe->prev_state == PE_SRC_TRANSITION_SUPPLY)
+		delay = PE_AUTO_TRIGGERING_DELAY;
+	schedule_delayed_work(&pe->post_ready_work,
+					msecs_to_jiffies(delay));
+}
+
+static void
+pe_process_state_pe_src_capability_response(struct policy_engine *pe)
+{
+	/*
+	 * As system currently supports one PDO and sink is not accepting it,
+	 * mode to disable state
+	 */
+	 pe_change_state(pe, PE_SRC_DISABLED);
+}
+static void pe_process_state_pe_src_disabled(struct policy_engine *pe)
+{
+	log_err("Source port in disable mode!!");
+}
+
+static void pe_process_state_pe_src_hard_reset(struct policy_engine *pe)
+{
+	if (pe->hard_reset_counter > PE_N_HARD_RESET_COUNT) {
+		log_info("Max Hard Reset Count reached!!");
+		if (pe->is_pp_pd_capable)
+			pe_change_state(pe, ERROR_RECOVERY);
+		else
+			pe_change_state(pe, PE_SRC_DISABLED);
+	}
+	pe_send_packet(pe, NULL, 0,
+		PD_CMD_HARD_RESET, PE_EVT_SEND_HARD_RESET);
+	pe->hard_reset_counter++;
+	pe_do_self_reset(pe);
+	pe_start_timer(pe, PS_HARD_RESET_TIMER, PE_TIME_PS_HARD_RESET_MIN);
+}
+
+static void
+pe_process_state_pe_src_hard_reset_received(struct policy_engine *pe)
+{
+
+	pe_do_self_reset(pe);
+	pe_start_timer(pe, PS_HARD_RESET_TIMER, PE_TIME_PS_HARD_RESET_MIN);
+
+}
+
+static void
+pe_process_state_pe_src_transition_to_default(struct policy_engine *pe)
+{
+	pe_do_dpm_reset_entry(pe);
+	pe_start_timer(pe, SRC_RESET_RECOVER_TIMER,
+			T_SRC_RECOVER_MIN);
+}
+
+static void
+pe_process_state_pe_src_transition_to_default_exit(struct policy_engine *pe)
+{
+	pe_do_dpm_reset_complete(pe);
+	pe_start_timer(pe, NO_RESPONSE_TIMER, PE_TIME_NO_RESPONSE);
+	pe_change_state(pe, PE_SRC_STARTUP);
+}
+
+static void
+pe_process_state_pe_src_give_source_cap(struct policy_engine *pe)
+{
+	pe_send_srccap_cmd(pe);
+}
+
+static void
+pe_process_state_pe_src_get_sink_cap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0, PD_CTRL_MSG_GET_SINK_CAP,
+				PE_EVT_SEND_GET_SINK_CAP);
+}
+
+/******************* DR_SWAP State handlers ***********************/
+
+static void
+pe_process_state_pe_drs_dfp_ufp_evaluate_dr_swap(struct policy_engine *pe)
+{
+	if (pe->cur_drole == DATA_ROLE_DFP) {
+		/* Always accept dr_swap */
+		pe_change_state(pe, PE_DRS_DFP_UFP_ACCEPT_DR_SWAP);
+		return;
+	}
+	log_err("Error data role=%d", pe->cur_drole);
+	pe_change_state_to_snk_or_src_reset(pe);
+}
+
+static void
+pe_process_state_pe_drs_dfp_ufp_accept_dr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_ACCEPT, PE_EVT_SEND_ACCEPT);
+}
+
+static void
+pe_process_state_pe_drs_dfp_ufp_change_to_ufp(struct policy_engine *pe)
+{
+	/* Change data tole to UFP */
+	pe_set_data_role(pe, DATA_ROLE_UFP);
+	log_err(" DR_SWAP Success: DFP -> UFP");
+	pe_change_state_to_snk_or_src_ready(pe);
+}
+
+static void
+pe_process_state_pe_drs_dfp_ufp_send_dr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_DR_SWAP, PE_EVT_SEND_DR_SWAP);
+}
+
+static void
+pe_process_state_pe_drs_ufp_dfp_evaluate_dr_swap(struct policy_engine *pe)
+{
+	if (pe->cur_drole == DATA_ROLE_UFP) {
+		/* Always accept dr_swap */
+		pe_change_state(pe, PE_DRS_UFP_DFP_ACCEPT_DR_SWAP);
+		return;
+	}
+	log_err("Error data role=%d", pe->cur_drole);
+	pe_change_state_to_snk_or_src_reset(pe);
+
+}
+
+static void
+pe_process_state_pe_drs_ufp_dfp_accept_dr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_ACCEPT, PE_EVT_SEND_ACCEPT);
+}
+
+static void
+pe_process_state_pe_drs_ufp_dfp_change_to_dfp(struct policy_engine *pe)
+{
+	/* Change data tole to DFP */
+	pe_set_data_role(pe, DATA_ROLE_DFP);
+	log_err(" DR_SWAP Success: UFP -> DFP");
+	pe_change_state_to_snk_or_src_ready(pe);
+}
+
+static void
+pe_process_state_pe_drs_ufp_dfp_send_dr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_DR_SWAP, PE_EVT_SEND_DR_SWAP);
+}
+
+static void
+pe_process_state_pe_prs_src_snk_send_pr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+				PD_CTRL_MSG_PR_SWAP, PE_EVT_SEND_PR_SWAP);
+}
+
+static void
+pe_process_state_pe_prs_src_snk_evaluate_pr_swap(struct policy_engine *pe)
+{
+	int ret;
+
+	ret = devpolicy_is_pr_swap_support(pe->p.dpm, pe->cur_prole);
+	if (ret > 0) {
+		/* Accept PR_SWAP*/
+		pe_change_state(pe, PE_PRS_SRC_SNK_ACCEPT_PR_SWAP);
+
+	} else {
+		/* Wait PR_SWAP*/
+		pe_change_state(pe, PE_PRS_SRC_SNK_REJECT_PR_SWAP);
+	}
+}
+
+static void
+pe_process_state_pe_prs_src_snk_accept_pr_swap(struct policy_engine *pe)
+{
+	log_dbg("Accepting PR_SWAP");
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_ACCEPT, PE_EVT_SEND_ACCEPT);
+
+}
+
+static void
+pe_process_state_pe_prs_src_snk_reject_pr_swap(struct policy_engine *pe)
+{
+	log_dbg("Sending Wait PR_SWAP");
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_WAIT, PE_EVT_SEND_WAIT);
+}
+
+static void
+pe_process_state_pe_prs_src_snk_transition_to_off(struct policy_engine *pe)
+{
+	/* Wait for tSrcTransition time to sink to settle*/
+	pe_start_timer(pe, SRC_TRANSITION_TIMER, T_SRC_TRANSITION);
+
+}
+
+static void
+pe_process_state_pe_prs_src_snk_assert_rd(struct policy_engine *pe)
+{
+	/* Pull-Down acc line */
+	devpolicy_set_cc_pu_pd(pe->p.dpm, TYPEC_CC_PULL_DOWN);
+	pe_change_state(pe, PE_PRS_SRC_SNK_WAIT_SOURCE_ON);
+}
+
+static void
+pe_process_state_pe_prs_src_snk_wait_source_on(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_PS_RDY, PE_EVT_SEND_PS_RDY);
+}
+
+
+static void
+pe_process_state_pe_prs_snk_src_send_pr_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_PR_SWAP, PE_EVT_SEND_PR_SWAP);
+}
+
+static void
+pe_process_state_pe_prs_snk_src_evaluate_swap(struct policy_engine *pe)
+{
+	int ret;
+
+	ret = devpolicy_is_pr_swap_support(pe->p.dpm, pe->cur_prole);
+	if (ret > 0) {
+		/* Accept PR_SWAP*/
+		pe_change_state(pe, PE_PRS_SNK_SRC_ACCEPT_PR_SWAP);
+
+	} else {
+		/* Wait PR_SWAP*/
+		pe_change_state(pe, PE_PRS_SNK_SRC_REJECT_PR_SWAP);
+	}
+}
+
+static void
+pe_process_state_pe_prs_snk_src_reject_swap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_REJECT, PE_EVT_SEND_REJECT);
+
+}
+
+static void
+pe_process_state_pe_prs_snk_src_accept_pr_swap(struct policy_engine *pe)
+{
+	log_dbg("Accepting PR_SWAP");
+	pe_send_packet(pe, NULL, 0,
+			PD_CTRL_MSG_ACCEPT, PE_EVT_SEND_ACCEPT);
+
+}
+
+static void
+pe_process_state_pe_prs_snk_src_transition_to_off(struct policy_engine *pe)
+{
+	/* Wait for PS_RDY from source*/
+	pe_start_timer(pe, PS_SOURCE_OFF_TIMER, PE_TIME_PS_SOURCE_OFF);
+
+	/* power sink off */
+	pe_set_power_role(pe, POWER_ROLE_SWAP);
+}
+
+static void
+pe_process_state_pe_prs_snk_src_assert_rp(struct policy_engine *pe)
+{
+	/* Pull-Up  CC line */
+	devpolicy_set_cc_pu_pd(pe->p.dpm, TYPEC_CC_PULL_UP);
+	pe_change_state(pe, PE_PRS_SNK_SRC_SOURCE_ON);
+}
+
+static void
+pe_process_state_pe_prs_snk_src_source_on(struct policy_engine *pe)
+{
+	pe->vbus_status = devpolicy_get_vbus_state(pe->p.dpm);
+	/* Turn on the VBUS */
+	devpolicy_set_vbus_state(pe->p.dpm, true);
+	pe_set_power_role(pe, POWER_ROLE_SOURCE);
+
+	pe_start_timer(pe, VBUS_CHECK_TIMER, T_SAFE_5V_MAX);
+	/* WA as DPM doesnt have VBUS notifier */
+	schedule_delayed_work(&pe->vbus_poll_work,
+			msecs_to_jiffies(VBUS_POLL_TIME));
+}
+
+/******* Dual Role state ************/
+
+static void
+pe_process_state_pe_dr_src_get_source_cap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0, PD_CTRL_MSG_GET_SRC_CAP,
+					PE_EVT_SEND_GET_SRC_CAP);
+}
+
+static void
+pe_process_state_pe_dr_src_give_sink_cap(struct policy_engine *pe)
+{
+	pe_send_self_sink_caps(pe);
+}
+
+static void
+pe_process_state_pe_dr_snk_get_sink_cap(struct policy_engine *pe)
+{
+	pe_send_packet(pe, NULL, 0, PD_CTRL_MSG_GET_SINK_CAP,
+					PE_EVT_SEND_GET_SINK_CAP);
+}
+
+static void
+pe_process_state_pe_dr_snk_give_source_cap(struct policy_engine *pe)
+{
+	pe_send_srccap_cmd(pe);
+}
+
+
+/* Alternate Mode State handlers*/
+static void
+pe_process_state_pe_dfp_ufp_vdm_identity_request(struct policy_engine *pe)
+{
+	pe_send_discover_identity(pe);
+}
+
+static void
+pe_process_state_pe_dfp_vdm_svids_request(struct policy_engine *pe)
+{
+	pe_send_discover_svid(pe);
+}
+
+static void
+pe_process_state_pe_dfp_vdm_status_request(struct policy_engine *pe)
+{
+	/* pin assign and index will picked from pp_alt_caps*/
+	pe_send_display_status(pe);
+}
+
+static void
+pe_process_state_pe_dfp_vdm_conf_request(struct policy_engine *pe)
+{
+	/* pin assign and index will picked from pp_alt_caps*/
+	pe_send_display_configure(pe);
+}
+
+static void
+pe_process_state_pe_dfp_vdm_modes_entry_request(struct policy_engine *pe)
+{
+	int index;
+
+	if (pe->pp_alt_caps.dmode_2x_index) {
+		log_info("Selecting 2X DP");
+		index = pe->pp_alt_caps.dmode_2x_index;
+		pe->pp_alt_caps.dp_mode = TYPEC_DP_TYPE_2X;
+
+	} else if (pe->pp_alt_caps.dmode_4x_index) {
+		log_info("Selecting 4X DP");
+		index = pe->pp_alt_caps.dmode_4x_index;
+		pe->pp_alt_caps.dp_mode = TYPEC_DP_TYPE_4X;
+	} else {
+		log_warn("Port neither support 2X nor 4X!!");
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		pe_change_state_to_snk_or_src_ready(pe);
+		return;
+	}
+
+	pe->pp_alt_caps.dmode_cur_index = index;
+	pe_send_enter_mode(pe, index);
+
+}
+
+static void
+pe_process_state_pe_dfp_vdm_modes_request(struct policy_engine *pe)
+{
+	/* In DFP, only VESA_SVID supported */
+	pe_send_discover_mode(pe);
+}
+
+/* Error Recovery state handlers */
+
+static void
+pe_process_state_error_recovery(struct policy_engine *pe)
+{
+	log_warn("PE moved to ERROR RECOVERY!!");
+	pe_do_self_reset(pe);
+}
+
+
+static void
+pe_process_state_pe_error_recovery(struct policy_engine *pe)
+{
+
+	log_info("Issue disconnect and connect");
+	pe_change_state(pe, PE_STATE_NONE);
+}
+
+static void
+pe_process_state_pe_state_none(struct policy_engine *pe)
+{
+	pe_do_complete_reset(pe);
+	/* VBUS Off */
+	devpolicy_set_vbus_state(pe->p.dpm, false);
+	/*VCONN off */
+	devpolicy_set_vconn_state(pe->p.dpm, false);
+	pe_set_data_role(pe, DATA_ROLE_NONE);
+	pe_set_power_role(pe, POWER_ROLE_NONE);
+	if (pe->prev_state == PE_ERROR_RECOVERY)
+		devpolicy_set_cc_pu_pd(pe->p.dpm, TYPEC_CC_PULL_NONE);
+}
+
+static void pe_state_change_worker(struct work_struct *work)
+{
+	struct policy_engine *pe = container_of(work,
+					struct policy_engine,
+					policy_state_work);
+	unsigned state;
+
+	mutex_lock(&pe->pe_lock);
+	state = pe->cur_state;
+	log_dbg("Processing state %d", state);
+	switch (state) {
+	case ERROR_RECOVERY:
+		pe_process_state_error_recovery(pe);
+		break;
+	case PE_ERROR_RECOVERY:
+		pe_process_state_pe_error_recovery(pe);
+	case PE_STATE_NONE:
+		pe_process_state_pe_state_none(pe);
+		break;
+	/* Sink Port State */
+	case PE_SNK_STARTUP:
+		pe_process_state_pe_snk_startup(pe);
+		break;
+	case PE_SNK_WAIT_FOR_HARD_RESET_VBUS_OFF:
+		pe_process_state_pe_snk_wait_for_hard_reset_vbus_off(pe);
+		break;
+	case PE_SNK_DISCOVERY:
+		pe_process_state_pe_snk_discovery(pe);
+		break;
+	case PE_SNK_WAIT_FOR_CAPABILITIES:
+		pe_process_state_pe_snk_wait_for_capabilities(pe);
+		break;
+	case PE_SNK_EVALUATE_CAPABILITY:
+		pe_process_state_pe_snk_evaluate_capabilities(pe);
+		break;
+	case PE_SNK_SELECT_CAPABILITY:
+		pe_process_state_pe_snk_select_capability(pe);
+		break;
+	case PE_SNK_TRANSITION_SINK:
+		pe_process_state_pe_snk_transition_sink(pe);
+		break;
+	case PE_SNK_READY:
+		pe_process_state_pe_snk_ready(pe);
+		break;
+	case PE_SNK_HARD_RESET:
+		pe_process_state_pe_snk_hard_reset(pe);
+		break;
+	case PE_SNK_TRANSITION_TO_DEFAULT:
+		pe_process_state_pe_snk_transition_to_default(pe);
+		break;
+
+	case PE_SNK_GIVE_SINK_CAP:
+		pe_process_state_pe_snk_give_sink_cap(pe);
+		break;
+
+	case PE_SNK_GET_SOURCE_CAP:
+		pe_process_state_pe_snk_get_source_cap(pe);
+		break;
+
+	case PE_SNK_HARD_RESET_RECEIVED:
+		pe_process_state_pe_snk_hard_reset_received(pe);
+		break;
+
+	/* Soirce Port States */
+	case PE_SRC_STARTUP:
+		pe_process_state_pe_src_startup(pe);
+		break;
+	case PE_SRC_WAIT_FOR_VBUS:
+		pe_process_state_pe_src_wait_for_vbus(pe);
+		break;
+	case PE_SRC_DISCOVERY:
+		pe_process_state_pe_src_discovery(pe);
+		break;
+	case PE_SRC_SEND_CAPABILITIES:
+		pe_process_state_pe_src_send_capabilities(pe);
+		break;
+	case PE_SRC_NEGOTIATE_CAPABILITY:
+		pe_process_state_pe_src_negotiate_capability(pe);
+		break;
+	case PE_SRC_TRANSITION_SUPPLY:
+		pe_process_state_pe_src_transition_supply(pe);
+		break;
+	case PE_SRC_READY:
+		pe_process_state_pe_src_ready(pe);
+		break;
+	case PE_SRC_DISABLED:
+		pe_process_state_pe_src_disabled(pe);
+		break;
+	case PE_SRC_CAPABILITY_RESPONSE:
+		pe_process_state_pe_src_capability_response(pe);
+		break;
+	case PE_SRC_HARD_RESET:
+		pe_process_state_pe_src_hard_reset(pe);
+		break;
+	case PE_SRC_HARD_RESET_RECEIVED:
+		pe_process_state_pe_src_hard_reset_received(pe);
+		break;
+	case PE_SRC_TRANSITION_TO_DEFAULT:
+		pe_process_state_pe_src_transition_to_default(pe);
+		break;
+	case PE_SRC_GIVE_SOURCE_CAP:
+		pe_process_state_pe_src_give_source_cap(pe);
+		break;
+	case PE_SRC_GET_SINK_CAP:
+		pe_process_state_pe_src_get_sink_cap(pe);
+		break;
+
+	/* DR_SWAP state */
+	case PE_DRS_DFP_UFP_EVALUATE_DR_SWAP:
+		pe_process_state_pe_drs_dfp_ufp_evaluate_dr_swap(pe);
+		break;
+	case PE_DRS_DFP_UFP_ACCEPT_DR_SWAP:
+		pe_process_state_pe_drs_dfp_ufp_accept_dr_swap(pe);
+		break;
+	case PE_DRS_DFP_UFP_CHANGE_TO_UFP:
+		pe_process_state_pe_drs_dfp_ufp_change_to_ufp(pe);
+		break;
+	case PE_DRS_DFP_UFP_SEND_DR_SWAP:
+		pe_process_state_pe_drs_dfp_ufp_send_dr_swap(pe);
+		break;
+	case PE_DRS_DFP_UFP_REJECT_DR_SWAP:
+		log_warn("Not Processing PE_DRS_DFP_UFP_REJECT_DR_SWAP");
+		break;
+	case PE_DRS_UFP_DFP_EVALUATE_DR_SWAP:
+		pe_process_state_pe_drs_ufp_dfp_evaluate_dr_swap(pe);
+		break;
+	case PE_DRS_UFP_DFP_ACCEPT_DR_SWAP:
+		pe_process_state_pe_drs_ufp_dfp_accept_dr_swap(pe);
+		break;
+	case PE_DRS_UFP_DFP_CHANGE_TO_DFP:
+		pe_process_state_pe_drs_ufp_dfp_change_to_dfp(pe);
+		break;
+	case PE_DRS_UFP_DFP_SEND_DR_SWAP:
+		pe_process_state_pe_drs_ufp_dfp_send_dr_swap(pe);
+		break;
+	case PE_DRS_UFP_DFP_REJECT_DR_SWAP:
+		log_warn("Not Processing PE_DRS_UFP_DFP_REJECT_DR_SWAP");
+		break;
+	/* Source to Sink Power Role Swap States */
+	case PE_PRS_SRC_SNK_EVALUATE_PR_SWAP:
+		pe_process_state_pe_prs_src_snk_evaluate_pr_swap(pe);
+		break;
+	case PE_PRS_SRC_SNK_ACCEPT_PR_SWAP:
+		pe_process_state_pe_prs_src_snk_accept_pr_swap(pe);
+		break;
+	case PE_PRS_SRC_SNK_TRANSITION_TO_OFF:
+		pe_process_state_pe_prs_src_snk_transition_to_off(pe);
+		break;
+	case PE_PRS_SRC_SNK_ASSERT_RD:
+		pe_process_state_pe_prs_src_snk_assert_rd(pe);
+		break;
+	case PE_PRS_SRC_SNK_WAIT_SOURCE_ON:
+		pe_process_state_pe_prs_src_snk_wait_source_on(pe);
+		break;
+	case PE_PRS_SRC_SNK_SEND_PR_SWAP:
+		pe_process_state_pe_prs_src_snk_send_pr_swap(pe);
+		break;
+	case PE_PRS_SRC_SNK_REJECT_PR_SWAP:
+		pe_process_state_pe_prs_src_snk_reject_pr_swap(pe);
+		break;
+
+	/* Sink to Source Power Role Swap States */
+	case PE_PRS_SNK_SRC_EVALUATE_PR_SWAP:
+		pe_process_state_pe_prs_snk_src_evaluate_swap(pe);
+		break;
+	case PE_PRS_SNK_SRC_ACCEPT_PR_SWAP:
+		pe_process_state_pe_prs_snk_src_accept_pr_swap(pe);
+		break;
+	case PE_PRS_SNK_SRC_TRANSITION_TO_OFF:
+		pe_process_state_pe_prs_snk_src_transition_to_off(pe);
+		break;
+	case PE_PRS_SNK_SRC_ASSERT_RP:
+		pe_process_state_pe_prs_snk_src_assert_rp(pe);
+		break;
+	case PE_PRS_SNK_SRC_SOURCE_ON:
+		pe_process_state_pe_prs_snk_src_source_on(pe);
+		break;
+	case PE_PRS_SNK_SRC_SEND_PR_SWAP:
+		pe_process_state_pe_prs_snk_src_send_pr_swap(pe);
+		break;
+	case PE_PRS_SNK_SRC_REJECT_PR_SWAP:
+		pe_process_state_pe_prs_snk_src_reject_swap(pe);
+		break;
+
+	/* Alternate Mode States */
+	case PE_DFP_UFP_VDM_IDENTITY_REQUEST:
+		pe_process_state_pe_dfp_ufp_vdm_identity_request(pe);
+		break;
+	case PE_DFP_VDM_SVIDS_REQUEST:
+		pe_process_state_pe_dfp_vdm_svids_request(pe);
+		break;
+
+	case PE_DFP_VDM_MODES_REQUEST:
+		pe_process_state_pe_dfp_vdm_modes_request(pe);
+		break;
+
+	case PE_DFP_VDM_MODES_ENTRY_REQUEST:
+		pe_process_state_pe_dfp_vdm_modes_entry_request(pe);
+		break;
+
+	case PE_DFP_VDM_STATUS_REQUEST:
+		pe_process_state_pe_dfp_vdm_status_request(pe);
+		break;
+
+	case PE_DFP_VDM_CONF_REQUEST:
+		pe_process_state_pe_dfp_vdm_conf_request(pe);
+		break;
+
+
+	case PE_DR_SRC_GET_SOURCE_CAP:
+		pe_process_state_pe_dr_src_get_source_cap(pe);
+		break;
+	case PE_DR_SRC_GIVE_SINK_CAP:
+		pe_process_state_pe_dr_src_give_sink_cap(pe);
+		break;
+	case PE_DR_SNK_GET_SINK_CAP:
+		pe_process_state_pe_dr_snk_get_sink_cap(pe);
+		break;
+	case PE_DR_SNK_GIVE_SOURCE_CAP:
+		pe_process_state_pe_dr_snk_give_source_cap(pe);
+		break;
+
+	default:
+		log_info("Cannot process unknown state %d", state);
+	}
+	mutex_unlock(&pe->pe_lock);
+	log_dbg("Processing state %d complete\n", state);
+}
+
+static void pe_vbus_pole_handler(struct work_struct *work)
+{
+	struct policy_engine *pe = container_of(work, struct policy_engine,
+					vbus_poll_work.work);
+	bool vbus;
+	enum devpolicy_mgr_events evt = DEVMGR_EVENT_NONE;
+
+	vbus = devpolicy_get_vbus_state(pe->p.dpm);
+	mutex_lock(&pe->pe_lock);
+	if (pe->cur_state == PE_STATE_NONE) {
+		mutex_unlock(&pe->pe_lock);
+		return;
+	}
+
+	log_dbg("VBUS=%d", vbus);
+	if (vbus != pe->vbus_status) {
+		log_dbg("VBUS changed %d -> %d", pe->vbus_status, vbus);
+		pe->vbus_status = vbus;
+		if (vbus)
+			evt = DEVMGR_EVENT_VBUS_ON;
+		else
+			evt = DEVMGR_EVENT_VBUS_OFF;
+	}
+	mutex_unlock(&pe->pe_lock);
+	schedule_delayed_work(&pe->vbus_poll_work,
+				msecs_to_jiffies(VBUS_POLL_TIME));
+
+	if (evt != DEVMGR_EVENT_NONE)
+		pe_handle_dpm_event(pe, evt);
+}
+
+static void
+pe_alt_mode_initiator(struct policy_engine *pe)
+{
+
+	switch (pe->alt_state) {
+	case PE_ALT_STATE_NONE:
+		pe_change_state(pe, PE_DFP_UFP_VDM_IDENTITY_REQUEST);
+		break;
+
+	case PE_ALT_STATE_DI_ACKED:
+		pe_change_state(pe, PE_DFP_VDM_SVIDS_REQUEST);
+		break;
+
+	case PE_ALT_STATE_SVID_ACKED:
+		pe_change_state(pe, PE_DFP_VDM_MODES_REQUEST);
+		break;
+
+	case PE_ALT_STATE_DMODE_ACKED:
+		pe_change_state(pe, PE_DFP_VDM_MODES_ENTRY_REQUEST);
+		break;
+
+	case PE_ALT_STATE_EMODE_ACKED:
+		pe_change_state(pe, PE_DFP_VDM_STATUS_REQUEST);
+		break;
+
+	case PE_ALT_STATE_STATUS_ACKED:
+		pe_change_state(pe, PE_DFP_VDM_CONF_REQUEST);
+		break;
+
+	default:
+		log_warn("Cannot trigger VDM in alt_state=%d",
+					pe->alt_state);
+	}
+}
+
+static void pe_post_ready_worker(struct work_struct *work)
+{
+	struct policy_engine *pe = container_of(work, struct policy_engine,
+							post_ready_work.work);
+
+	mutex_lock(&pe->pe_lock);
+	switch (pe->cur_state) {
+
+	case PE_SRC_READY:
+		if (pe->pp_snk_pdos.num_pdos == 0
+			&& pe->retry_counter < PE_MAX_RETRY) {
+			/* Get port partner's sink caps */
+			pe->retry_counter++;
+			log_dbg("Auto request sink cap");
+			pe_change_state(pe, PE_SRC_GET_SINK_CAP);
+			goto ready_work_done;
+		}
+		if ((!pe->is_pr_swap_rejected)
+			&& pe->pp_caps.pp_is_ext_pwrd) {
+			log_info("Auto triggering PR_SWAP");
+			pe_change_state(pe, PE_PRS_SRC_SNK_SEND_PR_SWAP);
+			goto ready_work_done;
+		}
+		break;
+
+	case PE_SNK_READY:
+		break;
+
+	default:
+		log_info("PE not in SNK/SRC READY state, quit!!");
+		goto ready_work_done;
+	}
+
+	if (pe->cur_drole != DATA_ROLE_DFP
+		|| pe->alt_state == PE_ALT_STATE_ALT_MODE_FAIL
+		|| pe->alt_state == PE_ALT_STATE_ALT_MODE_SUCCESS)
+		goto ready_work_done;
+
+	log_dbg("Start alternate mode CMDs");
+	pe_alt_mode_initiator(pe);
+
+ready_work_done:
+	mutex_unlock(&pe->pe_lock);
+}
+
+static struct pe_operations ops = {
+	.get_power_role = pe_get_power_role,
+	.get_data_role = pe_get_data_role,
+	.process_data_msg = policy_engine_process_data_msg,
+	.process_ctrl_msg = policy_engine_process_ctrl_msg,
+	.process_cmd = policy_engine_process_cmd,
+	.notify_dpm_evt = pe_dpm_notification,
+};
+
+static void pe_init_timers(struct policy_engine *pe)
+{
+	int i;
+	struct pe_timer *cur_timer;
+
+	for (i = 0; i < PE_TIMER_CNT; i++) {
+		cur_timer = &pe->timers[i];
+		cur_timer->timer_type = i;
+		cur_timer->data = pe;
+		INIT_WORK(&cur_timer->work, pe_timer_expire_worker);
+		setup_timer(&cur_timer->timer,
+			pe_timer_expire_callback,
+			(unsigned long)cur_timer);
+	}
+}
+
+static void pe_init_policy(struct work_struct *work)
+{
+	struct policy_engine *pe;
+
+	pe = container_of(work, struct policy_engine, policy_init_work);
+	mutex_init(&pe->pe_lock);
+	pe->cur_drole = DATA_ROLE_NONE;
+	pe->cur_prole = POWER_ROLE_NONE;
+	pe->is_typec_port = true;
+
+	/* Initialize pe timers */
+	pe_init_timers(pe);
+	INIT_WORK(&pe->policy_state_work, pe_state_change_worker);
+	INIT_DELAYED_WORK(&pe->vbus_poll_work, pe_vbus_pole_handler);
+	INIT_DELAYED_WORK(&pe->post_ready_work, pe_post_ready_worker);
+	pe->cur_state = PE_STATE_NONE;
+	pe->alt_state = PE_ALT_STATE_NONE;
+
+	pe->p.ops = &ops;
+	return;
+}
+
+
+int policy_engine_bind_dpm(struct devpolicy_mgr *dpm)
+{
+	int ret;
+	struct policy_engine *pe;
+
+	if (!dpm)
+		return -EINVAL;
+
+	if (dpm->p)
+		return -EEXIST;
+
+	pe = devm_kzalloc(dpm->phy->dev, sizeof(struct policy_engine),
+				GFP_KERNEL);
+	if (!pe)
+		return -ENOMEM;
+
+	pe->p.dpm = dpm;
+
+	ret = protocol_bind_pe(&pe->p);
+	if (ret) {
+		log_err("Failed to bind pe to protocol\n");
+		ret = -EINVAL;
+		goto bind_error;
+	}
+
+	dpm->p = &pe->p;
+
+	INIT_WORK(&pe->policy_init_work, pe_init_policy);
+	schedule_work(&pe->policy_init_work);
+	log_info("Policy engine bind success\n");
+	return 0;
+
+bind_error:
+	kfree(pe);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(policy_engine_bind_dpm);
+
+void policy_engine_unbind_dpm(struct devpolicy_mgr *dpm)
+{
+	struct policy_engine *pe;
+	struct policy *p;
+
+	if (!dpm || !dpm->p)
+		return;
+	p = dpm->p;
+	pe = container_of(p, struct policy_engine, p);
+	mutex_lock(&pe->pe_lock);
+	/*
+	 * Remove the pe ops to avoid further external
+	 * notifications and callbacks.
+	 */
+	p->ops = NULL;
+
+	pe_do_complete_reset(pe);
+
+	/* Unbind from protocol layer */
+	protocol_unbind_pe(&pe->p);
+	mutex_unlock(&pe->pe_lock);
+
+	kfree(pe);
+}
+EXPORT_SYMBOL_GPL(policy_engine_unbind_dpm);
+
+MODULE_AUTHOR("Kotakonda, Venkataramana <venkataramana.kotakonda@intel.com>");
+MODULE_DESCRIPTION("PD Policy Engine");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/usb/typec/pd/policy_engine.h b/drivers/usb/typec/pd/policy_engine.h
new file mode 100644
index 0000000..ae4017f
--- /dev/null
+++ b/drivers/usb/typec/pd/policy_engine.h
@@ -0,0 +1,573 @@
+/*
+ * policy_engine.h : Intel USB Power Delivery Policy Engine Header
+ *
+ * Copyright (C) 2015 Intel Corporation
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Seee the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ * Author: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
+ */
+
+#ifndef __POLICY_ENGINE_H__
+#define __POLICY_ENGINE_H__
+
+#include "protocol.h"
+#include "pd_policy.h"
+
+/* Policy engine time values in mSec*/
+#define PE_TIME_ATTENTION_AVERAGE	10000
+#define PE_TIME_ATTENTION_BURST_SPACING	100
+#define PE_TIME_ATTENTION_SPACING	250
+#define PE_TIME_BIST_MODE		300
+#define PE_TIME_BIST_CONT_MODE		60
+#define PE_TIME_BIST_RECEIVE		1
+#define PE_TIME_BIST_RESPONSE		15
+#define PE_TIME_DISCOVER_IDENTITY	50
+#define PE_TIME_DR_SWAP_HARD_RESET	15
+#define PE_TIME_FIRST_SOURCE_CAP	250
+#define PE_TIME_HARD_RESET		5
+#define PE_TIME_HARD_RESET_COMPLETE	5
+#define PE_TIME_NO_RESPONSE		5500
+#define PE_TIME_PS_HARD_RESET_MAX	35
+#define PE_TIME_PS_HARD_RESET_MIN	25
+#define PE_TIME_PS_SOURCE_OFF		920
+#define PE_TIME_PS_SOURCE_ON		480
+#define PE_TIME_PS_TRANSITION		550
+/* tReceive is time to receive gcrc and it is 1.1mS as per spec */
+/* Due to software delay, here it is defines as 10mS */
+#define PE_TIME_RECEIVE			20
+#define PE_TIME_RECEIVER_RESPONSE	15
+#define PE_TIME_SENDER_RESPONSE		30
+#define PE_TIME_SEND_SOURCE_CAP		2000
+#define PE_TIME_SINK_ACTIVITY		150
+#define PE_TIME_SINK_REQUEST		100
+#define PE_TIME_SINK_WAIT_CAP		2500
+#define PE_TIME_SOFT_RESET		15
+#define PE_TIME_SOURCE_ACTIVITY		50
+#define PE_TIME_SWAP_SINK_READY		15
+#define PE_TIME_SWAP_SOURCE_START	20
+#define PE_TIME_TYPEC_SEND_SOURCE_CAP	200
+#define PE_TIME_TYPEC_SINK_WAIT_CAP	620
+#define PE_TIME_VCONN_SOURCE_OFF	25
+#define PE_TIME_VCONN_SOURCE_ON		100
+#define PE_TIME_VDM_BUSSY		50
+#define PE_TIME_VDM_ENTER_MODE		25
+#define PE_TIME_VDM_EXIT_MODE		25
+#define PE_TIME_VDM_RECEIVER_RESPONSE	15
+#define PE_TIME_VDM_SENDER_RESPONSE	30
+#define PE_TIME_VDM_WAIT_MODE_ENTRY	50
+#define PE_TIME_VDM_WAIT_MODE_EXIT	50
+
+/* Policy engine time values in uSec*/
+#define PE_TIME_CABLE_MESSAGE		750
+#define PE_TIME_RETRY			75
+#define PE_TIME_TRANSMIT		195
+
+/* PE misc time values */
+#define VBUS_POLL_TIME			10 /* 10 mSec */
+
+/* Counter values */
+#define PE_N_ATTENTION_COUNT		10
+#define PE_N_BUSY_COUNT			5
+#define PE_N_CAPS_COUNT			50
+#define PE_N_DISCOVER_IDENTITY_COUNT	20
+#define PE_N_HARD_RESET_COUNT		2
+#define PE_N_MESSAGE_ID_COUNT		7
+#define PE_N_RETRY_COUNT		3
+#define PE_N_VBUS_CHECK_COUNT		20
+
+#define SNK_FSPDO_VOLT_SHIFT		10
+#define SNK_FSPDO_FIXED_SUPPLY		(3 << 30)
+#define SNK_FSPDO_DUAL_ROLE_PWR		(1 << 29)
+#define SNK_FSPDO_HIGHTER_CAPABILITY	(1 << 28)
+#define SNK_FSPDO_EXT_POWERED		(1 << 27)
+#define SNK_FSPDO_USB_COMM_CAPABLE	(1 << 26)
+#define SNK_FSPDO_DATA_ROLE_SWAP	(1 << 25)
+#define SNK_FSPDO_RESERVED		(3 << 20)
+#define SNK_FSPDO_VOLTAGE		(0x3FF << SNK_FSPDO_VOLT_SHIFT)
+#define SNK_FSPDO_MAX_CURRENT		(0x3FF << 0)
+
+/*Electrical Requirements, Time in mSec */
+#define T_SAFE_0V_MAX			650
+#define T_SAFE_5V_MAX			275
+#define T_SRC_RECOVER_MIN		660
+#define T_SRC_RECOVER_MAX		1000
+#define T_SNK_WAIT_FOR_VBUS_OFF		(PE_TIME_PS_HARD_RESET_MAX + \
+					T_SAFE_0V_MAX)
+#define T_SNK_WAIT_FOR_VBUS_ON		(T_SRC_RECOVER_MAX + \
+					T_SAFE_5V_MAX)
+#define T_SRC_TURN_ON			275
+#define T_SRC_TRANSITION		35
+
+/* returns in mV */
+#define DATA_OBJ_TO_VOLT(x)	(((x & SNK_FSPDO_VOLTAGE) >>	\
+					SNK_FSPDO_VOLT_SHIFT) * 50)
+/* returns in mA */
+#define DATA_OBJ_TO_CURRENT(x)	((x & SNK_FSPDO_MAX_CURRENT) * 10)
+
+#define VOLT_TO_DATA_OBJ(x)	(((x / 50) << SNK_FSPDO_VOLT_SHIFT) &	\
+					SNK_FSPDO_VOLTAGE)
+#define CURRENT_TO_DATA_OBJ(x)	((x / 10) & SNK_FSPDO_MAX_CURRENT)
+#define VOLT_TO_CAP_DATA_OBJ(x)		(x / 50)
+#define CURRENT_TO_CAP_DATA_OBJ(x)	(x / 10)
+
+#define FEATURE_SUPPORTED	1
+#define FEATURE_NOT_SUPPORTED	0
+
+#define LOG_TAG "PE"
+#define log_info(format, ...) \
+	pr_info(LOG_TAG":%s:"format"\n", __func__, ##__VA_ARGS__)
+#define log_dbg(format, ...) \
+	pr_debug(LOG_TAG":%s:"format"\n", __func__, ##__VA_ARGS__)
+#define log_err(format, ...) \
+	pr_err(LOG_TAG":%s:"format"\n", __func__, ##__VA_ARGS__)
+#define log_warn(format, ...) \
+	pr_warn(LOG_TAG":%s:"format"\n", __func__, ##__VA_ARGS__)
+
+#define PE_MAX_RETRY			20
+#define PE_AUTO_TRIGGERING_DELAY	100 /* 100 mSec */
+
+enum pe_states {
+
+	PE_STATE_NONE,
+
+	/* Source Port (1 - 14 ) */
+	PE_SRC_STARTUP,
+	PE_SRC_DISCOVERY,
+	PE_SRC_SEND_CAPABILITIES,
+	PE_SRC_NEGOTIATE_CAPABILITY,
+	PE_SRC_TRANSITION_SUPPLY,
+	PE_SRC_READY,
+	PE_SRC_DISABLED,
+	PE_SRC_CAPABILITY_RESPONSE,
+	PE_SRC_HARD_RESET,
+	PE_SRC_HARD_RESET_RECEIVED,
+	PE_SRC_TRANSITION_TO_DEFAULT,
+	PE_SRC_GIVE_SOURCE_CAP,
+	PE_SRC_GET_SINK_CAP,
+	PE_SRC_WAIT_NEW_CAPABILITIES,
+	/* Extra state added to check VBUS before sending SrcCap */
+	PE_SRC_WAIT_FOR_VBUS,
+	/* Reserved for src port state enhancements */
+	PE_SRC_STATES_RESERVED = 22,
+
+	/* Sink Port (23 - 34*/
+	PE_SNK_STARTUP,
+	PE_SNK_DISCOVERY,
+	PE_SNK_WAIT_FOR_CAPABILITIES,
+	PE_SNK_EVALUATE_CAPABILITY,
+	PE_SNK_SELECT_CAPABILITY,
+	PE_SNK_TRANSITION_SINK,
+	PE_SNK_READY,
+	PE_SNK_HARD_RESET,
+	PE_SNK_TRANSITION_TO_DEFAULT,
+	PE_SNK_GIVE_SINK_CAP,
+	PE_SNK_GET_SOURCE_CAP,
+	PE_SNK_HARD_RESET_RECEIVED,
+	/* When PE come to this state from hard reset, then pe
+	 * should wait for source to off VBUS on reset*/
+	PE_SNK_WAIT_FOR_HARD_RESET_VBUS_OFF,
+	/* Reserved for sink state enhancements */
+	PE_SNK_STATES_RESERVED = 40,
+
+	/* Source Port Soft Reset (41, 42) */
+	PE_SRC_SEND_SOFT_RESET,
+	PE_SRC_SOFT_RESET,
+
+	/* Sink Port Soft Reset (43, 44) */
+	PE_SNK_SEND_SOFT_RESET,
+	PE_SNK_SOFT_RESET,
+
+	/* Source Port Ping (45)*/
+	PE_SRC_PING,
+
+	/* Type-A/B Dual-Role (initially Source Port) Ping (46) */
+	PE_PRS_SRC_SNK_PING,
+
+	/* Type-A/B Dual-Role (initially Sink Port) Ping (47) */
+	PE_PRS_SNK_SRC_PING,
+
+	/* Type-A/B Hard Reset of P/C in Sink Role (48, 49) */
+	PE_PC_SNK_HARD_RESET,
+	PE_PC_SNK_SWAP_RECOVERY,
+
+	/* Type-A/B Hard Reset of C/P in Source Role (50, 51) */
+	PE_CP_SRC_HARD_RESET,
+	PE_CP_SRC_TRANSITION_TO_OFF,
+
+	/* Type-A/B C/P Dead Battery/Power Loss (52 - 58) */
+	PE_DB_CP_CHECK_FOR_VBUS,
+	PE_DB_CP_POWER_VBUS_DB,
+	PE_DB_CP_WAIT_FOR_BIT_STREAM,
+	PE_DB_CP_POWER_VBUS_5V,
+	PE_DB_CP_WAIT_BIT_STREAM_STOP,
+	PE_DB_CP_UNPOWER_VBUS,
+	PE_DB_CP_PS_DISCHARGE,
+
+	/* Type-A/B P/C Dead Battery/Power Loss (59 - 63) */
+	PE_DB_PC_UNPOWERED,
+	PE_DB_PC_CHECK_POWER,
+	PE_DB_PC_SEND_BIT_STREAM,
+	PE_DB_PC_WAIT_TO_DETECT,
+	PE_DB_PC_WAIT_TO_START,
+
+	/* Reserved State for DB (63 - 70) */
+	PE_DB_PC_RESERVED = 70,
+
+	/* Type-C DFP to UFP Data Role Swap (71 - 75) */
+	PE_DRS_DFP_UFP_EVALUATE_DR_SWAP,
+	PE_DRS_DFP_UFP_ACCEPT_DR_SWAP,
+	PE_DRS_DFP_UFP_CHANGE_TO_UFP,
+	PE_DRS_DFP_UFP_SEND_DR_SWAP,
+	PE_DRS_DFP_UFP_REJECT_DR_SWAP,
+
+	/* Type-C UFP to DFP Data Role Swap (76 - 80) */
+	PE_DRS_UFP_DFP_EVALUATE_DR_SWAP,
+	PE_DRS_UFP_DFP_ACCEPT_DR_SWAP,
+	PE_DRS_UFP_DFP_CHANGE_TO_DFP,
+	PE_DRS_UFP_DFP_SEND_DR_SWAP,
+	PE_DRS_UFP_DFP_REJECT_DR_SWAP,
+
+	/* Reserved State for DR_SWAP (81 - 85) */
+	PE_DSR_RESERVED = 85,
+
+	/* Source to Sink Power Role Swap (86 - 92) */
+	PE_PRS_SRC_SNK_EVALUATE_PR_SWAP,
+	PE_PRS_SRC_SNK_ACCEPT_PR_SWAP,
+	PE_PRS_SRC_SNK_TRANSITION_TO_OFF,
+	PE_PRS_SRC_SNK_ASSERT_RD,
+	PE_PRS_SRC_SNK_WAIT_SOURCE_ON,
+	PE_PRS_SRC_SNK_SEND_PR_SWAP,
+	PE_PRS_SRC_SNK_REJECT_PR_SWAP,
+
+	/* Sink to Source Power Role Swap (93 - 99) */
+	PE_PRS_SNK_SRC_EVALUATE_PR_SWAP,
+	PE_PRS_SNK_SRC_ACCEPT_PR_SWAP,
+	PE_PRS_SNK_SRC_TRANSITION_TO_OFF,
+	PE_PRS_SNK_SRC_ASSERT_RP,
+	PE_PRS_SNK_SRC_SOURCE_ON,
+	PE_PRS_SNK_SRC_SEND_PR_SWAP,
+	PE_PRS_SNK_SRC_REJECT_PR_SWAP,
+
+	/* Reserved State for PR_SWAP (100 - 105) */
+	PE_PSR_RESERVED = 105,
+
+	/* Dual-Role Source Port Get Source Capabilities (106) */
+	PE_DR_SRC_GET_SOURCE_CAP,
+
+	/* Dual-Role Source Port Give Sink Capabilities (107) */
+	PE_DR_SRC_GIVE_SINK_CAP,
+
+	/* Dual-Role Sink Port Get Sink Capabilities (108) */
+	PE_DR_SNK_GET_SINK_CAP,
+
+	/* Dual-Role Sink Port Give Source Capabilities (109) */
+	PE_DR_SNK_GIVE_SOURCE_CAP,
+
+	/* Type-C VCONN Swap (110 - 117) */
+	/*VCONN Swap states as per PD V1.1 */
+	PE_VCS_SEND_SWAP,
+	PE_VCS_EVALUATE_SWAP,
+	PE_VCS_ACCEPT_SWAP,
+	PE_VCS_REJECT_SWAP,
+	PE_VCS_WAIT_FOR_VCONN,
+	PE_VCS_TURN_OFF_VCONN,
+	PE_VCS_TURN_ON_VCONN,
+	PE_VCS_SEND_PS_RDY,
+
+	/* Reserved State 1 (118 - 125) */
+	PE_RESERVED_1 = 125,
+
+	/* UFP VDM (126 - 140) */
+	PE_UFP_VDM_GET_IDENTITY,
+	PE_UFP_VDM_SEND_IDENTITY,
+	PE_UFP_VDM_GET_IDENTITY_NAK,
+	PE_UFP_VDM_GET_SVIDS,
+	PE_UFP_VDM_SEND_SVIDS,
+	PE_UFP_VDM_GET_SVIDS_NAK,
+	PE_UFP_VDM_GET_MODES,
+	PE_UFP_VDM_SEND_MODES,
+	PE_UFP_VDM_GET_MODES_NAK,
+	PE_UFP_VDM_EVALUATE_MODE_ENTRY,
+	PE_UFP_VDM_MODE_ENTRY_ACK,
+	PE_UFP_VDM_MODE_ENTRY_NAK,
+	PE_UFP_VDM_MODE_EXIT,
+	PE_UFP_VDM_MODE_EXIT_ACK,
+	PE_UFP_VDM_MODE_EXIT_NAK,
+
+	/* UFP VDM Attention (141) */
+	PE_UFP_VDM_ATTENTION_REQUEST,
+
+	/* UFP VDM Reserved States  (142 - 150) */
+	PE_UFP_VDM_RESERVED = 150,
+
+	/* DFP -UFP VDM Discover Identity (151 - 153) */
+	PE_DFP_UFP_VDM_IDENTITY_REQUEST,
+	PE_DFP_UFP_VDM_IDENTITY_ACKED,
+	PE_DFP_UFP_VDM_IDENTITY_NAKED,
+
+	/* DFP - Cable VDM Discover Identity (154 - 156) */
+	PE_DFP_CBL_VDM_IDENTITY_REQUEST,
+	PE_DFP_CBL_VDM_IDENTITY_ACKED,
+	PE_DFP_CNL_VDM_IDENTITY_NAKED,
+
+	/* DFP VDM Discover SVIDs (157 - 159) */
+	PE_DFP_VDM_SVIDS_REQUEST,
+	PE_DFP_VDM_SVIDS_ACKED,
+	PE_DFP_VDM_SVIDS_NAKED,
+
+	/* DFP VDM Discover Modes (160 - 162) */
+	PE_DFP_VDM_MODES_REQUEST,
+	PE_DFP_VDM_MODES_ACKED,
+	PE_DFP_VDM_MODES_NAKED,
+
+	/* DFP VDM Mode Entry (163 - 165) */
+	PE_DFP_VDM_MODES_ENTRY_REQUEST,
+	PE_DFP_VDM_MODES_ENTRY_ACKED,
+	PE_DFP_VDM_MODES_ENTRY_NAKED,
+
+	/* DFP VDM Mode Exit (166, 167) */
+	PE_DFP_VDM_MODE_EXIT_REQUEST,
+	PE_DFP_VDM_MODE_EXIT_ACKED,
+
+	/* DFP VDM Status (168 - 170) */
+	PE_DFP_VDM_STATUS_REQUEST,
+	PE_DFP_VDM_STATUS_ACKED,
+	PE_DFP_VDM_STATUS_NAKED,
+
+	/* DFP VDM Configure (171 - 173) */
+	PE_DFP_VDM_CONF_REQUEST,
+	PE_DFP_VDM_CONF_ACKED,
+	PE_DFP_VDM_CONF_NAKED,
+
+	/* DFP VDM Attention (174) */
+	PE_DFP_VDM_ATTENTION_REQUEST,
+
+	/* DFP VDM Reserved States  (175 - 195 */
+	PE_DFP_VDM_RESERVED = 195,
+
+	/* USB to USB Cable (196 - 207) */
+	PE_CBL_READY,
+	PE_CBL_GET_IDENTITY,
+	PE_CBL_SEND_IDENTITY,
+	PE_CBL_GEG_SVIDS,
+	PE_CBL_SEND_SVIDS,
+	PE_CBL_GEG_MODES,
+	PE_CBL_SEND_MODES,
+	PE_CBL_EVALUATE_MODE_ENTRY,
+	PE_CBL_MODE_ENTRY_ACK,
+	PE_CBL_MODE_ENTRY_NAK,
+	PE_CBL_MODE_EXUIT,
+	PE_CBL_MODE_EXIT_ACK,
+
+	/* Cable Soft Reset (208) */
+	PE_CBL_SOFT_RESET,
+
+	/* Cable Hard Reset (209) */
+	PE_CBL_HARD_RESET,
+
+	/* CBL Reserved States  (210 - 215 */
+	PE_CBL_RESERVED = 215,
+
+	/* BIST Receive Mode (216, 217) */
+	PE_BIST_RECEIVE_MODE,
+	PE_BIST_FRAME_RECEIVED,
+
+	/* BIST Transmit Mode (218, 219) */
+	PE_BIST_TRANSMIT_MODE,
+	PE_BIST_SEND_FRAME,
+
+	/* BIST Carrier Mode and Eye Pattern (220 - 224) */
+	PE_BIST_EYE_PATTERN_MODE,
+	PE_BIST_CARRIER_MODE_0,
+	PE_BIST_CARRIER_MODE_1,
+	PE_BIST_CARRIER_MODE_2,
+	PE_BIST_CARRIER_MODE_3,
+
+	/* Type-C referenced states (500) */
+	ERROR_RECOVERY = 500,
+
+	/* This is PE internal error recovery state, in which
+	 * a disconnect will be issued and toggling will be
+	 * started to start the detection process.
+	 * hence move to PE_STATE_NONE.
+	 */
+	PE_ERROR_RECOVERY = 501,
+};
+
+
+struct pe_port_partner_caps {
+	unsigned pp_is_dual_drole:1;
+	unsigned pp_is_dual_prole:1;
+	unsigned pp_is_ext_pwrd:1;
+};
+
+enum pe_timers {
+	/* 0 - 4 */
+	BIST_CONT_MODE_TIMER,
+	BIST_RECEIVE_ERROR_TIMER,
+	BIST_START_TIMER,
+	CRC_RECEIVE_TIMER,
+	DISCOVER_IDENTITY_TIMER,
+	/*5 - 9 */
+	HARD_RESET_COMPLETE_TIMER,
+	NO_RESPONSE_TIMER,
+	PS_HARD_RESET_TIMER,
+	PS_SOURCE_OFF_TIMER,
+	PS_SOURCE_ON_TIMER,
+	/* 10 - 14 */
+	PS_TRANSITION_TIMER,
+	SENDER_RESPONSE_TIMER,
+	SINK_ACTIVITY_TIMER,
+	SINK_REQUEST_TIMER,
+	SINK_WAIT_CAP_TIMER,
+
+	/* 15 - 19 */
+	SOURCE_ACTIVITY_TIMER,
+	SOURCE_CAPABILITY_TIMER,
+	SWAP_RECOVERY_TIMER,
+	SWAP_SOURCE_START_TIMER,
+	VCONN_ON_TIMER,
+	/* 20 - 22 */
+	VDM_MODE_ENTRY_TIMER,
+	VDM_MODE_EXIT_TIMER,
+	VMD_RESPONSE_TIMER,
+	/*23, Misc timer */
+	VBUS_CHECK_TIMER,
+	SRC_RESET_RECOVER_TIMER,
+	SRC_TRANSITION_TIMER,
+
+	/* timer count */
+	PE_TIMER_CNT,
+};
+
+struct pe_pp_alt_caps {
+	unsigned short dmode_2x_index;
+	unsigned short dmode_4x_index;
+	unsigned short dmode_cur_index;
+	unsigned short dp_mode;
+	u8 pin_assign;
+
+	unsigned usb_dev_support:1;
+	unsigned usb_host_support:1;
+	unsigned pp_hpd_state:1;
+	unsigned hpd_state:1;
+};
+
+enum pe_alt_mode_state {
+	PE_ALT_STATE_NONE,
+	PE_ALT_STATE_DI_SENT,
+	PE_ALT_STATE_DI_ACKED,
+	PE_ALT_STATE_SVID_SENT,
+	PE_ALT_STATE_SVID_ACKED,
+	PE_ALT_STATE_DMODE_SENT,
+	PE_ALT_STATE_DMODE_ACKED,
+	PE_ALT_STATE_EMODE_SENT,
+	PE_ALT_STATE_EMODE_ACKED,
+	PE_ALT_STATE_STATUS_SENT,
+	PE_ALT_STATE_STATUS_ACKED,
+	PE_ALT_STATE_CONF_SENT,
+	PE_ALT_STATE_ALT_MODE_SUCCESS,
+	PE_ALT_STATE_ALT_MODE_FAIL,
+};
+
+
+struct pe_timer {
+	enum pe_timers timer_type;
+	unsigned timer_val; /* mSec unit */
+	struct timer_list timer;
+	struct work_struct work;
+	void *data;
+};
+
+struct pe_port_pdos {
+	int num_pdos;
+	u32 pdo[MAX_NUM_DATA_OBJ];
+};
+
+struct pe_req_cap {
+	u8 obj_pos;
+	u32 op_ma;
+	u32 max_ma;
+	u32 mv;
+	bool cap_mismatch;
+};
+
+struct policy_engine {
+	struct policy p;
+
+	struct mutex pe_lock;
+	struct mutex dpm_evt_lock;
+
+	struct work_struct policy_init_work;
+	struct work_struct policy_state_work;
+
+	struct pe_port_pdos pp_snk_pdos;
+	struct pe_port_pdos pp_src_pdos;
+	struct pe_port_pdos self_snk_pdos;
+	struct pe_port_pdos self_src_pdos;
+
+	struct pe_port_partner_caps pp_caps;
+	struct pe_req_cap self_sink_req_cap;
+	/* port partner's sink request caps*/
+	struct pd_fixed_var_rdo pp_sink_req_caps;
+
+	/* Timer structs for pe_timers */
+	struct pe_timer timers[PE_TIMER_CNT];
+
+	enum pe_states cur_state;
+	enum pe_states prev_state;
+	enum data_role	cur_drole;
+	enum pwr_role cur_prole;
+	enum pe_event last_rcv_evt;
+	enum pe_event last_sent_evt;
+
+	/* WA as DPM doesnt have VBUS notification*/
+	struct delayed_work vbus_poll_work;
+	bool vbus_status;
+
+	struct delayed_work post_ready_work;
+	enum pe_alt_mode_state	alt_state;
+	struct pe_pp_alt_caps pp_alt_caps;
+
+	/* PE counters */
+	unsigned src_caps_couner;
+	unsigned discover_identity_couner;
+	unsigned hard_reset_counter;
+	unsigned vdm_busy_couner;
+	unsigned retry_counter;
+
+	/* bool variables */
+	unsigned is_typec_port:1;
+	unsigned is_pp_pd_capable:1;
+	unsigned is_no_response_timer_expired:1;
+	unsigned is_gcrc_received:1;
+	unsigned pd_explicit_contract:1;
+	unsigned is_modal_operation:1;
+	unsigned is_pr_swap_rejected:1;
+	unsigned is_pd_enabled:1;
+};
+
+
+extern int dpm_register_pe(struct policy *p, int port);
+extern void dpm_unregister_pe(struct policy *p);
+extern int protocol_bind_pe(struct policy *p);
+extern void protocol_unbind_pe(struct policy *p);
+
+void pe_change_state_to_snk_or_src_ready(struct policy_engine *pe);
+int pe_send_packet(struct policy_engine *pe, void *data, int len,
+				u8 msg_type, enum pe_event evt);
+#endif /*  __POLICY_ENGINE_H__ */
diff --git a/drivers/usb/typec/pd/protocol.h b/drivers/usb/typec/pd/protocol.h
index 804cf2e..17fea38 100644
--- a/drivers/usb/typec/pd/protocol.h
+++ b/drivers/usb/typec/pd/protocol.h
@@ -7,6 +7,7 @@
 #include <linux/extcon.h>
 #include <linux/usb_typec_phy.h>
 #include "message.h"
+#include "pd_policy.h"
 
 #define PD_MAX_MSG_ID	7
 
diff --git a/drivers/usb/typec/pd/vdm_process.c b/drivers/usb/typec/pd/vdm_process.c
new file mode 100644
index 0000000..1d5cfd8
--- /dev/null
+++ b/drivers/usb/typec/pd/vdm_process.c
@@ -0,0 +1,647 @@
+/*
+ * vdm_process.c: Intel USB Power Delivery VDM Message Processor
+ *
+ * Copyright (C) 2015 Intel Corporation
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Seee the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ * Author: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
+ */
+
+#include <linux/delay.h>
+#include "policy_engine.h"
+
+#define PD_SID          0xff00
+#define VESA_SVID       0xff01
+#define STRUCTURED_VDM	1
+
+/* Display port pin assignments */
+#define DISP_PORT_PIN_ASSIGN_A	(1 << 0)
+#define DISP_PORT_PIN_ASSIGN_B	(1 << 1)
+#define DISP_PORT_PIN_ASSIGN_C	(1 << 2)
+#define DISP_PORT_PIN_ASSIGN_D	(1 << 3)
+#define DISP_PORT_PIN_ASSIGN_E	(1 << 4)
+#define DISP_PORT_PIN_ASSIGN_F	(1 << 5)
+
+/* Display configuration */
+#define DISP_CONFIG_USB			0
+#define DISP_CONFIG_UFPU_AS_DFP_D	1
+#define DISP_CONFIG_UFPU_AS_UFP_D	2
+#define DISP_CONFIG_RESERVED		3
+
+/* Display port signaling for transport */
+#define DISP_PORT_SIGNAL_UNSPEC		0
+#define DISP_PORT_SIGNAL_DP_1P3		1
+#define DISP_PORT_SIGNAL_GEN2		2
+
+#define DP_HPD_LONG_PULSE_TIME		5 /* 5 mSec */
+#define DP_HPD_SHORT_PULSE_TIME		2   /* 2 mSec */
+#define DP_HPD_AUTO_TRIGGER_TIME	200 /* 200 mSec */
+
+static void pe_prepare_vdm_header(struct vdm_header *v_hdr, enum vdm_cmd cmd,
+					int svid, int obj_pos)
+{
+	v_hdr->cmd = cmd;
+	v_hdr->cmd_type = INITIATOR;
+	v_hdr->obj_pos = obj_pos;
+	v_hdr->str_vdm_version = 0x0; /* 0 = version 1.0 */
+	v_hdr->vdm_type = STRUCTURED_VDM; /* Structured VDM */
+	v_hdr->svid = svid;
+
+}
+
+int pe_send_discover_identity(struct policy_engine *pe)
+{
+	struct vdm_header v_hdr = { 0 };
+	int ret;
+
+	pe_prepare_vdm_header(&v_hdr, DISCOVER_IDENTITY,
+						PD_SID, 0);
+	ret = pe_send_packet(pe, &v_hdr, 4,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_discover_identity_rnak(struct policy_engine *pe)
+{
+	struct vdm_header v_hdr = { 0 };
+	int ret;
+
+	v_hdr.cmd = DISCOVER_IDENTITY;
+	v_hdr.cmd_type = REP_NACK;
+	v_hdr.vdm_type = STRUCTURED_VDM; /* Structured VDM */
+	v_hdr.svid = PD_SID;
+
+	ret = pe_send_packet(pe, &v_hdr, 4,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_discover_svid(struct policy_engine *pe)
+{
+	struct vdm_header v_hdr = { 0 };
+	int ret;
+
+	pe_prepare_vdm_header(&v_hdr, DISCOVER_SVID,
+						PD_SID, 0);
+	ret = pe_send_packet(pe, &v_hdr, 4,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_discover_mode(struct policy_engine *pe)
+{
+	struct vdm_header v_hdr = { 0 };
+	int ret;
+
+	pe_prepare_vdm_header(&v_hdr, DISCOVER_MODE,
+						VESA_SVID, 0);
+	ret = pe_send_packet(pe, &v_hdr, 4,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_enter_mode(struct policy_engine *pe, int index)
+{
+	struct vdm_header v_hdr = { 0 };
+	int ret;
+
+	pe_prepare_vdm_header(&v_hdr, ENTER_MODE,
+						VESA_SVID, index);
+	ret = pe_send_packet(pe, &v_hdr, 4,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_display_status(struct policy_engine *pe)
+{
+	struct pd_packet pkt;
+	struct dis_port_status *stat;
+	struct vdm_header *v_hdr;
+	int ret, index;
+
+	if (pe->pp_alt_caps.dp_mode == TYPEC_DP_TYPE_2X)
+		index = pe->pp_alt_caps.dmode_2x_index;
+
+	else if (pe->pp_alt_caps.dp_mode == TYPEC_DP_TYPE_4X)
+		index = pe->pp_alt_caps.dmode_4x_index;
+	else {
+		log_err("Invalid mode index=%d!!!\n", pe->pp_alt_caps.dp_mode);
+		return -EINVAL;
+	}
+
+	memset(&pkt, 0, sizeof(pkt));
+	v_hdr = (struct vdm_header *) &pkt.data_obj[0];
+	pe_prepare_vdm_header(v_hdr, DP_STATUS_UPDATE,
+						VESA_SVID, index);
+
+	stat = (struct dis_port_status *) &pkt.data_obj[1];
+	stat->dev_connected = 1;
+
+	ret = pe_send_packet(pe, v_hdr, 8,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+	return ret;
+}
+
+int pe_send_display_configure(struct policy_engine *pe)
+{
+	struct pd_packet pkt;
+	struct disp_config dconf = { 0 };
+	int ret, index;
+
+
+	dconf.conf_sel = DISP_CONFIG_UFPU_AS_UFP_D;
+	dconf.trans_sig = DISP_PORT_SIGNAL_DP_1P3;
+
+	if (pe->pp_alt_caps.dp_mode == TYPEC_DP_TYPE_2X) {
+		dconf.dfp_pin = DISP_PORT_PIN_ASSIGN_D;
+		index = pe->pp_alt_caps.dmode_2x_index;
+		log_dbg("DP 2X with pin assign D");
+
+	} else if (pe->pp_alt_caps.dp_mode == TYPEC_DP_TYPE_4X) {
+		if (pe->pp_alt_caps.pin_assign
+				& DISP_PORT_PIN_ASSIGN_E) {
+			dconf.dfp_pin = DISP_PORT_PIN_ASSIGN_E;
+			log_dbg("DP 4X with pin assign E");
+
+		} else if (pe->pp_alt_caps.pin_assign
+				& DISP_PORT_PIN_ASSIGN_C) {
+			log_dbg("DP 4X with pin assign C");
+			dconf.dfp_pin = DISP_PORT_PIN_ASSIGN_C;
+		} else {
+			log_err("Unknown 4X pin assign=%x\n",
+					pe->pp_alt_caps.pin_assign);
+			ret = -EINVAL;
+			goto config_error;
+		}
+		index = pe->pp_alt_caps.dmode_4x_index;
+
+	} else {
+		log_err("Invalid dp_mode=%d\n", pe->pp_alt_caps.dp_mode);
+		ret = -EINVAL;
+		goto config_error;
+	}
+
+	pkt.data_obj[0] = 0;
+	pe_prepare_vdm_header((struct vdm_header *)&pkt.data_obj[0],
+					DP_CONFIGURE, VESA_SVID, index);
+	memcpy(&pkt.data_obj[1], &dconf, sizeof(dconf));
+
+	ret = pe_send_packet(pe, &pkt.data_obj[0], 8,
+				PD_DATA_MSG_VENDOR_DEF, PE_EVT_SEND_VDM);
+
+config_error:
+	return ret;
+}
+
+static int pe_handle_discover_identity(struct policy_engine *pe,
+							struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	unsigned short cmd_type = vdm_hdr->cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported, Sending NAK\n");
+		pe_send_discover_identity_rnak(pe);
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_UFP_VDM_IDENTITY_REQUEST) {
+		log_warn("DI RACK received in wrong state,state=%d\n",
+				pe->cur_state);
+		return -EINVAL;
+	}
+	switch (cmd_type) {
+	case REP_ACK:
+		/* TODO: Process the port partner's DI */
+		log_dbg(" DI Acked ");
+		pe->alt_state = PE_ALT_STATE_DI_ACKED;
+		break;
+	case REP_NACK:
+		log_dbg(" DI Nacked!!! ");
+		log_err("Responder doesn't support alternate mode\n");
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+	case REP_BUSY:
+		log_info("Responder BUSY!!. Retry Discover Identity\n");
+		pe->alt_state = PE_ALT_STATE_NONE;
+		break;
+	}
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+
+static int pe_handle_discover_svid(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct dis_svid_response_pkt *svid_pkt;
+	unsigned short cmd_type;
+	int num_modes = 0;
+	int i, mode = 0;
+
+	svid_pkt = (struct dis_svid_response_pkt *)pkt;
+	cmd_type = svid_pkt->vdm_hdr.cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported, Sending NAK\n");
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_VDM_SVIDS_REQUEST) {
+		log_warn("SVID RACK received in wrong state,state=%d\n",
+				pe->cur_state);
+		return -EINVAL;
+	}
+	switch (cmd_type) {
+	case REP_ACK:
+		/* 2 modes per VDO*/
+		num_modes = (svid_pkt->msg_hdr.num_data_obj - 1) * 2;
+		log_dbg("SVID_ACK-> This Display supports %d modes\n",
+				num_modes);
+		for (i = 0; i < num_modes; i++) {
+			log_dbg("vdo[%d].svid0=0x%x, svid1=0x%x\n",
+				i, svid_pkt->vdo[i].svid0,
+				svid_pkt->vdo[i].svid1);
+			if ((svid_pkt->vdo[i].svid0 == VESA_SVID)
+				|| (svid_pkt->vdo[i].svid1 == VESA_SVID)) {
+				mode = VESA_SVID;
+				break;
+			}
+		}
+		/* Currently we support only VESA */
+		if (mode == VESA_SVID) {
+			log_dbg("This Display supports VESA\n");
+			pe->alt_state = PE_ALT_STATE_SVID_ACKED;
+			break;
+		} else
+			log_err("This Display doesn't supports VESA\n");
+		/* Stop the display detection process */
+	case REP_NACK:
+		log_warn("Responder doesn't support alternate mode\n");
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+	case REP_BUSY:
+		log_info("Responder BUSY!!. Retry Discover SVID\n");
+		pe->alt_state = PE_ALT_STATE_DI_ACKED;
+		break;
+	}
+
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+static void pe_process_dp_modes(struct policy_engine *pe,
+				struct dis_mode_response_pkt *dmode_pkt)
+{
+	int i;
+	int index_2x = 0;
+	int index_4x = 0;
+
+	for (i = 0; i < dmode_pkt->msg_hdr.num_data_obj - 1; i++) {
+		if (!index_4x) {
+			if (dmode_pkt->mode[i].ufp_pin
+					& DISP_PORT_PIN_ASSIGN_E
+				|| dmode_pkt->mode[i].dfp_pin
+					& DISP_PORT_PIN_ASSIGN_E) {
+				/* Mode intex starts from 1 */
+				index_4x = i + 1;
+				pe->pp_alt_caps.pin_assign |=
+					DISP_PORT_PIN_ASSIGN_E;
+				log_dbg("Port supports Pin Assign E\n");
+			}
+			if (dmode_pkt->mode[i].ufp_pin
+					& DISP_PORT_PIN_ASSIGN_C
+				|| dmode_pkt->mode[i].dfp_pin
+					& DISP_PORT_PIN_ASSIGN_C) {
+				/* Mode intex starts from 1 */
+				index_4x = i + 1;
+				pe->pp_alt_caps.pin_assign |=
+					DISP_PORT_PIN_ASSIGN_C;
+				log_dbg("Port supports Pin Assign C\n");
+			}
+		}
+		if (!index_2x) {
+			if (dmode_pkt->mode[i].ufp_pin
+					& DISP_PORT_PIN_ASSIGN_D
+				|| dmode_pkt->mode[i].dfp_pin
+					& DISP_PORT_PIN_ASSIGN_D) {
+				/* Mode intex starts from 1 */
+				index_2x = i + 1;
+				pe->pp_alt_caps.pin_assign |=
+					DISP_PORT_PIN_ASSIGN_D;
+				log_dbg("Port supports Pin Assign D\n");
+			}
+		}
+		if (index_2x && index_4x)
+			break;
+	}
+	pe->pp_alt_caps.dmode_4x_index = index_4x;
+	pe->pp_alt_caps.dmode_2x_index = index_2x;
+	log_dbg("4x_index=%d, 2x_index=%d\n", index_4x, index_2x);
+}
+
+static int pe_handle_discover_mode(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct dis_mode_response_pkt *dmode_pkt;
+	unsigned short cmd_type;
+
+	dmode_pkt = (struct dis_mode_response_pkt *)pkt;
+	cmd_type = dmode_pkt->vdm_hdr.cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported\n");
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_VDM_MODES_REQUEST) {
+		log_warn("DiscMode RACK received in wrong state=%d\n",
+					pe->cur_state);
+		return -EINVAL;
+	}
+
+	switch (cmd_type) {
+
+	case REP_ACK:
+		log_dbg("Discover Mode Acked\n");
+		pe_process_dp_modes(pe, dmode_pkt);
+		/* First check for 2X, Mode D */
+		if (pe->pp_alt_caps.dmode_2x_index
+			|| pe->pp_alt_caps.dmode_4x_index) {
+			pe->alt_state = PE_ALT_STATE_DMODE_ACKED;
+			break;
+		}
+		log_warn("This Display doesn't supports neither 2X nor 4X\n");
+		/* Stop the display detection process */
+
+	case REP_NACK:
+		log_warn("Responder doesn't support alternate mode\n");
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+	case REP_BUSY:
+		log_warn("Responder BUSY!!. Retry Discover Mode\n");
+		pe->alt_state = PE_ALT_STATE_SVID_ACKED;
+		break;
+	}
+
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+
+static int pe_handle_enter_mode(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	unsigned short cmd_type = vdm_hdr->cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported, Sending NAK\n");
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_VDM_MODES_ENTRY_REQUEST) {
+		log_warn("Entermode RACK received in wrong state,state=%d\n",
+				pe->cur_state);
+		return -EINVAL;
+	}
+	switch (cmd_type) {
+
+	case REP_ACK:
+		log_info("EnterMode Success, dp_mode=%d\n",
+				pe->pp_alt_caps.dp_mode);
+		pe->is_modal_operation = true;
+		pe->alt_state = PE_ALT_STATE_EMODE_ACKED;
+		break;
+
+	case REP_NACK:
+		log_warn("Display falied to enter dp mode %d\n",
+				pe->pp_alt_caps.dp_mode);
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+
+	case REP_BUSY:
+		log_warn("Responder BUSY!!. Retry Enter Mode\n");
+		pe->alt_state = PE_ALT_STATE_DMODE_ACKED;
+		break;
+	}
+
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+static int pe_handle_display_status(struct policy_engine *pe,
+							struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	struct dis_port_status *disp_stat;
+	unsigned short cmd_type = vdm_hdr->cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported, Sending NAK\n");
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_VDM_STATUS_REQUEST) {
+		log_warn("DI Status received in wrong state,state=%d\n",
+				pe->cur_state);
+		return -EINVAL;
+	}
+
+	switch (cmd_type) {
+	case REP_ACK:
+		log_dbg(" Status Acked ");
+		pe->alt_state = PE_ALT_STATE_STATUS_ACKED;
+		disp_stat = (struct dis_port_status *)&pkt->data_obj[1];
+		pe->pp_alt_caps.pp_hpd_state = disp_stat->hpd_state;
+		break;
+
+	case REP_NACK:
+		log_err(" Status Nacked!!! ");
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+
+	case REP_BUSY:
+		log_warn("Responder BUSY!!. Retry Status\n");
+		pe->alt_state = PE_ALT_STATE_EMODE_ACKED;
+		break;
+	}
+
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+static int pe_handle_display_configure(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	unsigned short cmd_type = vdm_hdr->cmd_type;
+
+	if (cmd_type == INITIATOR) {
+		log_warn("UFP alternate mode not supported, Sending NAK\n");
+		return 0;
+	}
+	if (pe->cur_state != PE_DFP_VDM_CONF_REQUEST) {
+		log_warn("DP CONFIG ACK received in wrong state,state=%d\n",
+				pe->cur_state);
+		return -EINVAL;
+	}
+	switch (cmd_type) {
+
+	case REP_ACK:
+		log_info("DP Config success!!, dp_mode=%d\n",
+					pe->pp_alt_caps.dp_mode);
+
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_SUCCESS;
+		pe->pp_alt_caps.hpd_state = true;
+		devpolicy_set_dp_state(pe->p.dpm, CABLE_ATTACHED,
+						pe->pp_alt_caps.dp_mode);
+		break;
+	case REP_NACK:
+		log_warn("NAK for display config cmd %d\n",
+					pe->pp_alt_caps.dp_mode);
+		pe->alt_state = PE_ALT_STATE_ALT_MODE_FAIL;
+		break;
+
+	case REP_BUSY:
+		log_warn("Responder BUSY!!. Retry CONFIG\n");
+		pe->alt_state = PE_ALT_STATE_STATUS_ACKED;
+		break;
+	}
+
+	pe_change_state_to_snk_or_src_ready(pe);
+	return 0;
+}
+
+static void pe_generate_hpd_pulse(struct policy_engine *pe, int pulse_time)
+{
+	log_dbg("Pulling HPD low\n");
+	devpolicy_set_hpd_state(pe->p.dpm, 0);
+	udelay(pulse_time * 1000);
+	log_dbg("Raising HPD to high\n");
+	devpolicy_set_hpd_state(pe->p.dpm, 1);
+}
+
+static void pe_handle_hpd_state_change(struct policy_engine *pe, bool hpd)
+{
+	if (pe->pp_alt_caps.hpd_state != hpd) {
+		pe->pp_alt_caps.hpd_state = hpd;
+		if (hpd)
+			devpolicy_set_dp_state(pe->p.dpm, CABLE_ATTACHED,
+						pe->pp_alt_caps.dp_mode);
+		else
+			devpolicy_set_dp_state(pe->p.dpm, CABLE_DETACHED,
+						TYPEC_DP_TYPE_NONE);
+	} else {
+		/*
+		 * Some dp cable which doesnt support status update cmd
+		 * which is expected after EnterMode. Due to this the hpd
+		 * status cannot be known after EnterMode. To fix this by
+		 * default hpd will be triggered after dp configure.
+		 * So, if previous hpd is true then retrigger with
+		 * short hpd pulse.
+		 */
+		if (pe->pp_alt_caps.hpd_state) {
+			/* Generate hpd long pulse */
+			log_dbg("Generating short pulse for display re-detect");
+			pe_generate_hpd_pulse(pe,
+					DP_HPD_SHORT_PULSE_TIME);
+		}
+	}
+}
+
+static int pe_handle_display_attention(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	struct dis_port_status *dstat =
+				(struct dis_port_status *)&pkt->data_obj[1];
+
+	switch (vdm_hdr->cmd_type) {
+	case INITIATOR:
+		log_warn("Attention received\n");
+		log_info("pp_hpd_status=%d\n", dstat->hpd_state);
+		if (pe->pp_alt_caps.pp_hpd_state != dstat->hpd_state) {
+			log_dbg("Change in port partner's HPD status\n");
+			pe->pp_alt_caps.pp_hpd_state = dstat->hpd_state;
+			pe_handle_hpd_state_change(pe, dstat->hpd_state);
+		}
+
+		if (dstat->irq_hpd) {
+			log_dbg("Got IRQ HPD\n");
+			if (pe->pp_alt_caps.hpd_state) {
+				/* Generate hpd short pulse */
+				pe_generate_hpd_pulse(pe,
+						DP_HPD_SHORT_PULSE_TIME);
+			}
+		}
+
+		break;
+	case REP_ACK:
+		log_warn("ACK received for attention!!!!!\n");
+		break;
+	case REP_NACK:
+		log_warn("NACK received for attention!!!!!\n");
+	case REP_BUSY:
+		log_warn("BUSY received for attention!!!!!\n");
+	}
+	return 0;
+}
+
+int pe_handle_vendor_msg(struct policy_engine *pe,
+						struct pd_packet *pkt)
+{
+	struct vdm_header *vdm_hdr = (struct vdm_header *)&pkt->data_obj[0];
+	int ret = 0;
+
+	switch (vdm_hdr->cmd) {
+	case DISCOVER_IDENTITY:
+		ret = pe_handle_discover_identity(pe, pkt);
+		break;
+	case DISCOVER_SVID:
+		ret = pe_handle_discover_svid(pe, pkt);
+		break;
+	case DISCOVER_MODE:
+		ret = pe_handle_discover_mode(pe, pkt);
+		break;
+	case ENTER_MODE:
+		ret = pe_handle_enter_mode(pe, pkt);
+		break;
+	case EXIT_MODE:
+		log_info("EXIT DP mode request received\n");
+		/* TODO: Handle the exit mode */
+		break;
+	case DP_STATUS_UPDATE:
+		log_dbg("Received display status from port partner=%x\n",
+					pkt->data_obj[1]);
+		ret = pe_handle_display_status(pe, pkt);
+		break;
+	case DP_CONFIGURE:
+		ret = pe_handle_display_configure(pe, pkt);
+		break;
+	case ATTENTION:
+		log_dbg("Received display attention from port partner=%x\n",
+					pkt->data_obj[1]);
+		ret = pe_handle_display_attention(pe, pkt);
+		break;
+	default:
+		ret = -EINVAL;
+		log_err("Not a valid vendor msg to handle\n");
+	}
+	return ret;
+}
diff --git a/drivers/usb/typec/pd/vdm_process_helper.h b/drivers/usb/typec/pd/vdm_process_helper.h
new file mode 100644
index 0000000..0aa00d6
--- /dev/null
+++ b/drivers/usb/typec/pd/vdm_process_helper.h
@@ -0,0 +1,35 @@
+/*
+ * vdm_process_helper.h: Intel USB PD VDM Helper Functions Header
+ *
+ * Copyright (C) 2015 Intel Corporation
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Seee the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ * Author: Venkataramana Kotakonda <venkataramana.kotakonda@intel.com>
+ */
+
+#ifndef VDM_PROCESS_HELPER_H
+#define VDM_PROCESS_HELPER_H
+int pe_handle_vendor_msg(struct policy_engine *pe, struct pd_packet *pkt);
+int pe_send_discover_identity(struct policy_engine *pe);
+int pe_send_discover_identity_rnak(struct policy_engine *pe);
+int pe_send_discover_svid(struct policy_engine *pe);
+int pe_send_discover_mode(struct policy_engine *pe);
+int pe_send_enter_mode(struct policy_engine *pe, int index);
+int pe_send_display_status(struct policy_engine *pe);
+int pe_send_display_configure(struct policy_engine *pe);
+
+#endif /* VDM_PROCESS_HELPER_H */
-- 
1.9.1

