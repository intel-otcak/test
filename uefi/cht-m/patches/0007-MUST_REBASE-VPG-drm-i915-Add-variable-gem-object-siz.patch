From c67c9116b2c26d46be8c0cb09af61dccefdcf9d7 Mon Sep 17 00:00:00 2001
Message-Id: <c67c9116b2c26d46be8c0cb09af61dccefdcf9d7.1428642548.git.chang-joon.lee@intel.com>
In-Reply-To: <bf1030973d60050117965176cfa2e18cdaf9c0a1.1428642548.git.chang-joon.lee@intel.com>
References: <bf1030973d60050117965176cfa2e18cdaf9c0a1.1428642548.git.chang-joon.lee@intel.com>
From: Arun Siluvery <arun.siluvery@linux.intel.com>
Date: Mon, 23 Mar 2015 16:37:18 +0000
Subject: [PATCH 7/7] MUST_REBASE [VPG]: drm/i915: Add variable gem object
 size support to i915

This patch adds support to have gem objects of variable size.
The size of the gem object is always constant and it is tightly coupled
in the driver; this implementation allows to vary its effective size
using an interface similar to fallocate().

A new extended ioctl() is introduced to mark a range as scratch/usable.
A region is marked as scratch by uncommitting it, then the associated
backing store is released and it is filled with scratch pages.
When the region is committed again at a later point new backing pages
are created. The range can be anywhere within the object space; it can
have multiple ranges possibly overlapping forming a large contiguous range.

This feature is useful where the exact size of the object is not clear
at the time of its creation, in such case we usually create an object
with more than the required size but end up using it partially. This
unused space cannot be used for any other purpose and simply gets wasted.
This ioctl allows to release unused backing store thus reducing the
memory pressure on the kernel especially useful in devices with tight
memory constraints.

Userspace can query the availablity of this feature at runtime using
I915_PARAM_HAS_GEM_FALLOCATE parameter.

Many thanks to Akash, Daniel, ChrisW, Tvrtko, Bob for the idea and
feedback on this implementation.

v2: fix holes in error handling and use consistent data types (Tvrtko)
 - If page allocation fails simply return error; do not try to invoke
   shrinker to free backing store.
 - Release new pages created by us in case of error during page allocation
   or sg_table update.
 - Use 64-bit data types for start and length values to avoid truncation.

v3: Address review comments (Brad V)
 - Add shmem check as we need shmem backing
 - correct rounding errors in start, end
 - use correct locking function for struct_mutex

v4: Modify to work with PPGTT, address review comments (Akash) and rebase.
Review comments and other changes in this revision include,
- Correctly handle object reference to avoid leaking memory
- Before fallocate we unbind the object but it is not bound again after falloc;
  it is deferred to the point when it is actually used again.
- We use a single scratch page to replace a range of real backing store.
  This will cause an issue when we destroy the object as we decrement it's
  reference count multiple times causing it to go -ve. This is prevented by
  incrementing scratch page reference count for each replacement and it is
  decremented when replaced with real page.
- Object pages are manipulated in two stages; this is done mainly to handle
  all possible failure case before we actually modify the object. This causes
  code duplication but the main advantage is if we hit a failure we can return
  without modifiying the object state, it is not possible to achieve this
  without two-pass approach.
- add comments
- ensure parameters are page-aligned, change mode values to simplify validation
  and modify conditions accordingly, correct description for ioctl parameters
  to reflect driver behaviour (Andrzej)
- handle release of page reference counts for all error cases, simply argument
  checks (Thomas)

At the moment this is an internal requirement; unless there is an opensource
user it cannot be merged upstream, hence marked as MUST_REBASE.

For: GMINL-7525
Change-Id: I65d517a220aef626de3468ada17629653dbb7888
Signed-off-by: Arun Siluvery <arun.siluvery@linux.intel.com>
---
 drivers/gpu/drm/i915/i915_dma.c       |    3 +
 drivers/gpu/drm/i915/i915_drv.h       |    2 +
 drivers/gpu/drm/i915/i915_ext_ioctl.c |    2 +
 drivers/gpu/drm/i915/i915_gem.c       |  290 +++++++++++++++++++++++++++++++++
 include/uapi/drm/i915_drm.h           |   29 ++++
 5 files changed, 326 insertions(+)

diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index c52cc38..5e621c6 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1047,6 +1047,9 @@ static int i915_getparam(struct drm_device *dev, void *data,
 	case I915_PARAM_MMAP_VERSION:
 		value = 1;
 		break;
+	case I915_PARAM_HAS_GEM_FALLOCATE:
+		value = 1;
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 940005d..3862fce 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -2630,6 +2630,8 @@ int i915_gem_get_tiling(struct drm_device *dev, void *data,
 int i915_gem_init_userptr(struct drm_device *dev);
 int i915_gem_userptr_ioctl(struct drm_device *dev, void *data,
 			   struct drm_file *file);
+int i915_gem_fallocate_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file);
 int i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
 				struct drm_file *file_priv);
 int i915_gem_wait_ioctl(struct drm_device *dev, void *data,
diff --git a/drivers/gpu/drm/i915/i915_ext_ioctl.c b/drivers/gpu/drm/i915/i915_ext_ioctl.c
index 4debb27..d8fa22f 100644
--- a/drivers/gpu/drm/i915/i915_ext_ioctl.c
+++ b/drivers/gpu/drm/i915/i915_ext_ioctl.c
@@ -158,6 +158,8 @@ err_i1:
 const struct drm_ioctl_desc i915_ext_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_EXT_USERDATA, i915_gem_userdata_ioctl,
 			  DRM_UNLOCKED|DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_FALLOCATE, i915_gem_fallocate_ioctl,
+			  DRM_UNLOCKED|DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW),
 };
 
 int i915_max_ext_ioctl = ARRAY_SIZE(i915_ext_ioctls);
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index d99503d..aaa0ac9 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -4800,6 +4800,296 @@ unlock:
 	return ret;
 }
 
+static int
+__i915_gem_obj_fallocate(struct drm_i915_gem_object *obj,
+		       uint32_t mode, uint64_t start, uint64_t end)
+{
+	int i;
+	int ret;
+	uint64_t start_page, end_page;
+	uint32_t num_pages;
+	bool update_sg_table = false;
+	unsigned long scratch_pfn;
+	struct page *scratch;
+	struct page **pages;
+	struct sg_table *sg = NULL;
+	struct sg_page_iter sg_iter;
+	struct drm_i915_private *dev_priv;
+
+	dev_priv = obj->base.dev->dev_private;
+	start_page = start >> PAGE_SHIFT;
+	end_page = end >> PAGE_SHIFT;
+	num_pages = obj->base.size >> PAGE_SHIFT;
+
+	pages = drm_malloc_ab(num_pages, sizeof(*pages));
+	if (pages == NULL)
+		return -ENOMEM;
+
+	scratch = dev_priv->gtt.base.scratch.page;
+	scratch_pfn = page_to_pfn(scratch);
+
+	/*
+	 * The pages of obj are manipulated in two stages;
+	 * This is done mainly to handle all possible failure cases before
+	 * we actually start modifying it's pages; If we hit any failure
+	 * in between we can return without changing the state of the obj.
+	 *
+	 * At the end of first pass we create a new sg table with pages
+	 * in the given range pointing to scratch page in case of uncommit op
+	 * or real pages in case of commit operation.
+	 */
+	i = 0;
+	for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0)
+		pages[i++] = sg_page_iter_page(&sg_iter);
+
+	if (mode == I915_GEM_FALLOC_UNCOMMIT) {
+		for (i = start_page; i < end_page; ++i) {
+			if (scratch_pfn == page_to_pfn(pages[i]))
+				continue;
+
+			update_sg_table = true;
+			pages[i] = scratch;
+		}
+	} else if (mode == I915_GEM_FALLOC_COMMIT) {
+		gfp_t gfp;
+		struct page *page;
+		struct address_space *mapping;
+
+		mapping = file_inode(obj->base.filp)->i_mapping;
+		gfp = mapping_gfp_mask(mapping);
+		gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;
+		gfp &= ~(__GFP_IO | __GFP_WAIT);
+
+		for (i = start_page; i < end_page; ++i) {
+			if (scratch_pfn != page_to_pfn(pages[i]))
+				continue;
+
+			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
+			if (IS_ERR(page)) {
+				ret = PTR_ERR(page);
+				goto err_alloc;
+			}
+
+			update_sg_table = true;
+			/*
+			 * As we are not updating the domain of the object here
+			 * and since shmem zeroes out the newly allocated page,
+			 * need to do an inline flush of cachelines for the
+			 * newly acquired pages
+			 */
+			if (!HAS_LLC(dev_priv->dev))
+				drm_clflush_pages(&page, 1);
+
+			pages[i] = page;
+		}
+	}
+
+	if (update_sg_table == false) {
+		ret = 0;
+		goto out;
+	}
+
+	sg = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
+	if (!sg) {
+		ret = -ENOMEM;
+		goto err_alloc;
+	}
+
+	ret = sg_alloc_table_from_pages(sg, pages, num_pages, 0,
+					num_pages << PAGE_SHIFT, GFP_KERNEL);
+	if (ret)
+		goto err_alloc_sgt;
+
+	/*
+	 * In the second pass we release page reference counts of obj and
+	 * in case of uncommit operation we release any mappings to obj pages
+	 * and finally release backing store
+	 *
+	 */
+	if (mode == I915_GEM_FALLOC_UNCOMMIT) {
+		struct page *page;
+		uint32_t page_count = end_page - start_page;
+
+		__sg_page_iter_start(&sg_iter, obj->pages->sgl,
+				     sg_nents(obj->pages->sgl), start_page);
+		while (page_count-- && __sg_page_iter_next(&sg_iter)) {
+			page = sg_page_iter_page(&sg_iter);
+
+			if (scratch_pfn == page_to_pfn(page))
+				continue;
+
+			page_cache_release(page);
+
+			/*
+			 * We are using a single scratch page to replace all of
+			 * the real pages in the range but when we destroy the
+			 * object we decrement reference count multiple times
+			 * even though they all refer to the same page, this
+			 * will take the count to -ve value.
+			 * To counter this we increment scratch page ref count
+			 * everytime it is used and it is decremented when
+			 * it is replaced with a real page.
+			 */
+			page_cache_get(scratch);
+		}
+		/*
+		 * Invalidate the CPU mappings on this object as
+		 * the pages being uncommited could be mapped already
+		 * in process’s address space
+		 */
+		unmap_mapping_range(file_inode(obj->base.filp)->i_mapping,
+				    start, end - start, 0);
+		/*
+		 * Release the uncommited pages from shmem’s page cache and
+		 * immediately hand them over to kernel
+		 */
+		shmem_truncate_range(file_inode(obj->base.filp),
+				     start, end - 1);
+
+	} else if (mode == I915_GEM_FALLOC_COMMIT) {
+		struct page *page;
+		uint32_t page_count = end_page - start_page;
+
+		__sg_page_iter_start(&sg_iter, obj->pages->sgl,
+				     sg_nents(obj->pages->sgl), start_page);
+		while (page_count-- && __sg_page_iter_next(&sg_iter)) {
+			page = sg_page_iter_page(&sg_iter);
+
+			if (scratch_pfn != page_to_pfn(page))
+				continue;
+
+			/* decrease scratch page ref count */
+			page_cache_release(page);
+		}
+	}
+
+	if (pages)
+		drm_free_large(pages);
+
+	if (obj->pages) {
+		sg_free_table(obj->pages);
+		kfree(obj->pages);
+	}
+	obj->pages = sg;
+
+	return 0;
+
+err_alloc_sgt:
+	kfree(sg);
+err_alloc:
+	if (mode == I915_GEM_FALLOC_COMMIT) {
+		/*
+		 * Release page references for pages whose allocation is
+		 * successful as the driver would have taken a ref count
+		 */
+		while (--i >= start_page)
+			page_cache_release(pages[i]);
+	}
+
+out:
+	if (pages)
+		drm_free_large(pages);
+
+	return ret;
+}
+
+/**
+ * This ioctl allows to change the effective size of an existing gem object.
+ * The object size is always constant and this fact is tightly coupled in our
+ * driver. This ioctl() allows user to define certain ranges in the obj to be
+ * marked as usable/scratch thus modifying the effective size of the object.
+ *
+ * This is mainly useful in situations where object size requirements are
+ * not clear at the time of it's creation. In such case it is helpful to
+ * create a larger object and release the excess backing store. This space
+ * can be claimed back when required by unmarking the region.
+ *
+ * I915_GEM_FALLOC_UNCOMMIT: specified range of pages are replaced
+ * by scratch page. If the backing store is not yet created for the object
+ * then it is created first.
+ *
+ * I915_GEM_FALLOC_COMMIT: specified range of scratch pages are
+ * replaced by real backing store.
+ */
+int i915_gem_fallocate_ioctl(struct drm_device *dev, void *data,
+			     struct drm_file *file)
+{
+	int ret;
+	uint64_t start, end;
+	struct i915_vma *vma, *next;
+	struct drm_i915_gem_object *obj;
+	struct drm_i915_gem_fallocate *args = data;
+
+	if (args->mode > I915_GEM_FALLOC_COMMIT)
+		return -EINVAL;
+
+	if (!IS_ALIGNED(args->start, PAGE_SIZE) ||
+	    !IS_ALIGNED(args->length, PAGE_SIZE)) {
+		DRM_DEBUG_DRIVER("start/length are not page aligned\n");
+		return -EINVAL;
+	}
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	if (!obj->base.filp) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	start = args->start;
+	end = args->start + args->length;
+	if (start >= end || end > obj->base.size) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/*
+	 * The procedure is simple:
+	 *
+	 * If the object is not bound we first allocate real backing store
+	 * If the object is bound we simply unbind object from all VMAs
+	 * fallocate on the object pages in the given range
+	 *  - uncommit operation - replace real pages with scratch page
+	 *  - commit operation - replace scratch page with real page
+	 * binding the object after fallocate is deferred
+	 */
+	if (!i915_gem_obj_bound_any(obj)) {
+		ret = i915_gem_object_get_pages(obj);
+		if (ret)
+			goto out;
+	} else {
+		list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
+			ret = i915_vma_unbind(vma);
+			if (ret) {
+				DRM_ERROR("Failed to unbind object\n");
+				goto out;
+			}
+		}
+	}
+
+	ret = __i915_gem_obj_fallocate(obj, args->mode, start, end);
+	if (ret) {
+		DRM_ERROR("fallocate failed for pages %llu to %llu, ret=%d\n",
+			  start >> PAGE_SHIFT, end >> PAGE_SHIFT, ret);
+		goto out;
+	}
+
+out:
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
 void i915_gem_object_init(struct drm_i915_gem_object *obj,
 			  const struct drm_i915_gem_object_ops *ops)
 {
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index d01095e..90e8a03a 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -313,9 +313,12 @@ struct i915_ext_ioctl_data {
 
 /* Extended ioctl definitions */
 #define DRM_I915_EXT_USERDATA		0x0
+#define DRM_I915_GEM_FALLOCATE		0x1
 
 #define DRM_IOCTL_I915_EXT_USERDATA \
 	DRM_IOWR(DRM_I915_EXT_USERDATA, struct drm_i915_gem_userdata_blk)
+#define DRM_IOCTL_I915_GEM_FALLOCATE \
+	DRM_IOW(DRM_I915_GEM_FALLOCATE,	struct drm_i915_gem_fallocate)
 
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
@@ -472,6 +475,7 @@ typedef struct drm_i915_irq_wait {
 #define I915_PARAM_HAS_DPST              0x800
 #define I915_PARAM_EU_COUNT              0x801
 #define I915_PARAM_HAS_RS		 0x802
+#define I915_PARAM_HAS_GEM_FALLOCATE     0x803
 
 typedef struct drm_i915_getparam {
 	int param;
@@ -569,6 +573,31 @@ struct drm_i915_gem_create {
 	__u32 pad;
 };
 
+struct drm_i915_gem_fallocate {
+	/**
+	 * Start position of the range
+	 *
+	 * This should be page-aligned.
+	 */
+	__u64 start;
+	/**
+	 * Length of the range
+	 *
+	 * This should be page-aligned.
+	 */
+	__u64 length;
+	/**
+	 * Mode applied to the range
+	 */
+	__u32 mode;
+#define I915_GEM_FALLOC_UNCOMMIT    0
+#define I915_GEM_FALLOC_COMMIT      1
+	/**
+	 * handle for the object being manipulated
+	 */
+	__u32 handle;
+};
+
 struct drm_i915_gem_pread {
 	/** Handle for the object being read. */
 	__u32 handle;
-- 
1.7.9.5

