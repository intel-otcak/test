From 5f32426c2cddcf9fa3cbbe377d7840961846f76d Mon Sep 17 00:00:00 2001
From: mwalews <marcin.walewski@intel.com>
Date: Wed, 9 Dec 2015 10:20:23 +0100
Subject: [PATCH 2/3] usb: rndis: W/A: Change maximum packets per transfer for
 DL aggregation and queue length multiplier at HS/SS.

This patch improves performance RNDIS with on HS

RNDIS_DL_MAX_PKT_PER_XFER and QMULT_DEFAULT change performanceof
transfer ethernet packet via USB. This change causes that
ethernet packet are collected to larger USB package. After this
change KASAN showed overload in memcpy in eth_start_xmit function.
This patch also fix this overload.
The aggregation will be removed from source in future.

Change-Id: I423c0575b5942a8c6fc024d01786bc25738732af
Signed-off-by: Marcin Walewski <marcin.walewski@intel.com>
Tracked-On: https://jira01.devtools.intel.com/browse/OAM-19471
Reviewed-on: https://android.intel.com:443/446142
---
 drivers/usb/gadget/f_rndis.c |   2 +-
 drivers/usb/gadget/u_ether.c | 160 +++++++++++++++++++++++--------------------
 2 files changed, 85 insertions(+), 77 deletions(-)

diff --git a/drivers/usb/gadget/f_rndis.c b/drivers/usb/gadget/f_rndis.c
index a5e7478..2ea8676 100644
--- a/drivers/usb/gadget/f_rndis.c
+++ b/drivers/usb/gadget/f_rndis.c
@@ -69,7 +69,7 @@
  *   - MS-Windows drivers sometimes emit undocumented requests.
  */
 
-static unsigned int rndis_dl_max_pkt_per_xfer = 3;
+static unsigned int rndis_dl_max_pkt_per_xfer = 10;
 module_param(rndis_dl_max_pkt_per_xfer, uint, S_IRUGO | S_IWUSR);
 MODULE_PARM_DESC(rndis_dl_max_pkt_per_xfer,
 	"Maximum packets per transfer for DL aggregation");
diff --git a/drivers/usb/gadget/u_ether.c b/drivers/usb/gadget/u_ether.c
index 9587c78..1f17716 100644
--- a/drivers/usb/gadget/u_ether.c
+++ b/drivers/usb/gadget/u_ether.c
@@ -75,6 +75,7 @@ struct eth_dev {
 	unsigned		header_len;
 	unsigned		ul_max_pkts_per_xfer;
 	unsigned		dl_max_pkts_per_xfer;
+	unsigned		dl_max_pkts_size;
 	struct sk_buff		*(*wrap)(struct gether *, struct sk_buff *skb);
 	int			(*unwrap)(struct gether *,
 						struct sk_buff *skb,
@@ -578,12 +579,14 @@ static void alloc_tx_buffer(struct eth_dev *dev)
 	struct list_head	*act;
 	struct usb_request	*req;
 
-	dev->tx_req_bufsize = (dev->dl_max_pkts_per_xfer *
-				(dev->net->mtu
+	dev->dl_max_pkts_size = dev->net->mtu
 				+ sizeof(struct ethhdr)
 				/* size of rndis_packet_msg_type */
 				+ 44
-				+ 22));
+				+ 22;
+
+	dev->tx_req_bufsize = (dev->dl_max_pkts_per_xfer *
+				dev->dl_max_pkts_size);
 
 	list_for_each(act, &dev->tx_reqs) {
 		req = container_of(act, struct usb_request, list);
@@ -603,6 +606,7 @@ static netdev_tx_t eth_start_xmit(struct sk_buff *skb,
 	unsigned long		flags;
 	struct usb_ep		*in;
 	u16			cdc_filter;
+	int			collect_skb = 1;
 
 	spin_lock_irqsave(&dev->lock, flags);
 	if (dev->port_usb) {
@@ -679,92 +683,96 @@ static netdev_tx_t eth_start_xmit(struct sk_buff *skb,
 			goto drop;
 	}
 
-	spin_lock_irqsave(&dev->req_lock, flags);
-	dev->tx_skb_hold_count++;
-	spin_unlock_irqrestore(&dev->req_lock, flags);
+	while (collect_skb) {
+		if (((skb->len + req->length) > dev->tx_req_bufsize)
+		&& dev->port_usb->multi_pkt_xfer) {
+			spin_lock_irqsave(&dev->lock, flags);
+			dev->no_tx_req_used++;
+			dev->tx_skb_hold_count = 0;
+			spin_unlock_irqrestore(&dev->lock, flags);
+			length = req->length;
+		} else {
+			collect_skb = 0;
+			spin_lock_irqsave(&dev->lock, flags);
+			dev->tx_skb_hold_count++;
+			spin_unlock_irqrestore(&dev->lock, flags);
 
-	if (dev->port_usb->multi_pkt_xfer) {
-		memcpy(req->buf + req->length, skb->data, skb->len);
-		req->length = req->length + skb->len;
-		length = req->length;
-		dev_kfree_skb_any(skb);
+			if (dev->port_usb->multi_pkt_xfer) {
+				memcpy(req->buf + req->length, skb->data,
+					skb->len);
+				req->length = req->length + skb->len;
+				length = req->length;
+				dev_kfree_skb_any(skb);
 
-		spin_lock_irqsave(&dev->req_lock, flags);
-		if (dev->tx_skb_hold_count < dev->dl_max_pkts_per_xfer) {
-			if (dev->no_tx_req_used > TX_REQ_THRESHOLD) {
-				list_add(&req->list, &dev->tx_reqs);
+				spin_lock_irqsave(&dev->req_lock, flags);
+				if ((dev->tx_skb_hold_count <
+					dev->dl_max_pkts_per_xfer)
+				&& (dev->no_tx_req_used > TX_REQ_THRESHOLD)
+				&& (dev->tx_req_bufsize
+				>= (length + dev->dl_max_pkts_size))) {
+					list_add(&req->list, &dev->tx_reqs);
+					spin_unlock_irqrestore(&dev->req_lock,
+						flags);
+					goto success;
+				}
+
+				dev->no_tx_req_used++;
 				spin_unlock_irqrestore(&dev->req_lock, flags);
-				goto success;
+
+				spin_lock_irqsave(&dev->lock, flags);
+				dev->tx_skb_hold_count = 0;
+				spin_unlock_irqrestore(&dev->lock, flags);
+			} else {
+				length = skb->len;
+				req->buf = skb->data;
+				req->context = skb;
 			}
 		}
 
-		dev->no_tx_req_used++;
-		spin_unlock_irqrestore(&dev->req_lock, flags);
-
-		spin_lock_irqsave(&dev->lock, flags);
-		dev->tx_skb_hold_count = 0;
-		spin_unlock_irqrestore(&dev->lock, flags);
-	} else {
-		length = skb->len;
-		req->buf = skb->data;
-		req->context = skb;
-	}
-
-	req->complete = tx_complete;
+		req->complete = tx_complete;
 
-	/* NCM requires no zlp if transfer is dwNtbInMaxSize */
-	if (dev->port_usb->is_fixed &&
-	    length == dev->port_usb->fixed_in_len &&
-	    (length % in->maxpacket) == 0)
-		req->zero = 0;
-	else
-		req->zero = 1;
+		/* NCM requires no zlp if transfer is dwNtbInMaxSize */
+		if (dev->port_usb->is_fixed &&
+		length == dev->port_usb->fixed_in_len &&
+		(length % in->maxpacket) == 0)
+			req->zero = 0;
+		else
+			req->zero = 1;
+
+		/* use zlp framing on tx for strict CDC-Ether conformance,
+		* though any robust network rx path ignores extra padding.
+		* and some hardware doesn't like to write zlps.
+		*/
+		if (req->zero && !dev->zlp && (length % in->maxpacket) == 0) {
+			req->zero = 0;
+			length++;
+		}
 
-	/* use zlp framing on tx for strict CDC-Ether conformance,
-	 * though any robust network rx path ignores extra padding.
-	 * and some hardware doesn't like to write zlps.
-	 */
-	if (req->zero && !dev->zlp && (length % in->maxpacket) == 0) {
-		req->zero = 0;
-		length++;
-	}
+		req->length = length;
+		req->no_interrupt = 0;
 
-	req->length = length;
 
-	/* throttle highspeed IRQ rate back slightly */
-	if (gadget_is_dualspeed(dev->gadget) &&
-			 (dev->gadget->speed == USB_SPEED_HIGH)) {
-		dev->tx_qlen++;
-		if (dev->tx_qlen == (dev->qmult/2)) {
-			req->no_interrupt = 0;
-			dev->tx_qlen = 0;
-		} else {
-			req->no_interrupt = 1;
+		retval = usb_ep_queue(in, req, GFP_ATOMIC);
+		switch (retval) {
+		default:
+			DBG(dev, "tx queue err %d\n", retval);
+			break;
+		case 0:
+			net->trans_start = jiffies;
 		}
-	} else {
-		req->no_interrupt = 0;
-	}
 
-	retval = usb_ep_queue(in, req, GFP_ATOMIC);
-	switch (retval) {
-	default:
-		DBG(dev, "tx queue err %d\n", retval);
-		break;
-	case 0:
-		net->trans_start = jiffies;
-	}
-
-	if (retval) {
-		if (!dev->port_usb->multi_pkt_xfer)
-			dev_kfree_skb_any(skb);
+		if (retval) {
+			if (!dev->port_usb->multi_pkt_xfer)
+				dev_kfree_skb_any(skb);
 drop:
-		dev->net->stats.tx_dropped++;
-		spin_lock_irqsave(&dev->req_lock, flags);
-		if (list_empty(&dev->tx_reqs))
-			netif_start_queue(net);
-		list_add(&req->list, &dev->tx_reqs);
-		spin_unlock_irqrestore(&dev->req_lock, flags);
-	}
+			dev->net->stats.tx_dropped++;
+			spin_lock_irqsave(&dev->req_lock, flags);
+			if (list_empty(&dev->tx_reqs))
+				netif_start_queue(net);
+			list_add(&req->list, &dev->tx_reqs);
+			spin_unlock_irqrestore(&dev->req_lock, flags);
+		}
+        }
 success:
 	return NETDEV_TX_OK;
 }
-- 
1.9.1

