From 9e1fcd8febb51d65cf0504355a2c7e904cfe458a Mon Sep 17 00:00:00 2001
Message-Id: <9e1fcd8febb51d65cf0504355a2c7e904cfe458a.1413836944.git.chang-joon.lee@intel.com>
In-Reply-To: <5d8aa9994fff43965a6fab8c9a528b8b47a9d624.1413836944.git.chang-joon.lee@intel.com>
References: <5d8aa9994fff43965a6fab8c9a528b8b47a9d624.1413836944.git.chang-joon.lee@intel.com>
From: ian-lister <ian.lister@intel.com>
Date: Mon, 23 Dec 2013 10:26:31 +0000
Subject: [PATCH 56/71] REVERTME [VPG]: drm/i915: Timeout Detection Recovery
 for Engine hangs

Original Author: Ian Lister <ian.lister@intel.com>

v1: Timeout-Detection-Recovery (TDR) provides per-engine hang detection
and recovery. If an engine hangs then the TDR will attempt to reset
the engine and advance the command streamer to the next instruction
in the ring. If it was in the middle of processing a batch buffer
then control returns to the intstruction following the batch buffer
start command. This provides a less intrusive recovery mechanism
than a global reset because it only impacts the process which
caused the hang. uevents are sent so that user mode can detect that
something has gone wrong and take action if required.

uevents are sent so that user mode can detect that something has gone
wrong and take action if required.

Issues:
1) Full GPU resets can leave the system in a state where the display updates
   intermittently. This problem already existed with the current driver which
   could only do full GPU resets in response to a hang on any of the rings.
   The problem has not been seen with per-engine TDR unless the driver
   falls back to full GPU reset, either because an individual ring reset
   fails or because the rings are hanging too quickly.
   In the failing state the vblank reference counting seems to go wrong,
   but the exact cause is not yet understood.

TDR updated to handle setting of reset stats when TDR occurs. The
request list is scanned to detect which request caused the hang
and the context stats are updated.

Port to 3.10.xx Kernel.

v2: Port to 3.14.xx Kernel and address review comments (Akash)
Remove spinlock around dev_priv->hangcheck[ringid].active as it is
not required because it is updated atomically.
Create ring hangcheck initialization and cleanup functions.
Add mmio flip cleanup fn to handle mmio flip failure and restore
crtc->fb to the old framebuffer after a failure.
Remove intel_display_handle_reset() call from i915_reset() as it is
called after reset in i915_error_work_func().

v3: Integrate with execlists v2 on 3.14 kernel
  - On Gen7 execlists are disabled so TDR should work as expected
  - On Gen8 TDR doesn't work without integrating it with GPU scheduler
    but this patch doesn't break Gen8.

v4: Rebase as per execlists v4.

Note: Gen8 has an additional ring VCS2 and in this patch TDR support
is not added for this ring because it doesn't exist on Gen7 and it
cannot be tested at the moment on Gen8 also. This should be added
after integrating TDR with scheduler.

This is REVERTME because this will be replaced with a GPU Scheduler
based solution going forward.

For: GMIN-677
Tracked-On: https://jira01.devtools.intel.com/browse/GMIN-678
Signed-off-by: Joao Santos <joao.santos@intel.com>
Signed-off-by: Arun Siluvery <arun.siluvery@linux.intel.com>
(cherry picked from commit cbff18d3d8012eddc331e510a5836b00c2ccaea0)
Change-Id: Ifc9063bbf9bbac7e9111a2516589064e78226cbd
---
 drivers/gpu/drm/i915/i915_debugfs.c     |  121 +++++-
 drivers/gpu/drm/i915/i915_dma.c         |   37 +-
 drivers/gpu/drm/i915/i915_drv.c         |  214 +++++++++-
 drivers/gpu/drm/i915/i915_drv.h         |   47 ++-
 drivers/gpu/drm/i915/i915_gem.c         |  123 ++++--
 drivers/gpu/drm/i915/i915_gpu_error.c   |   68 ++--
 drivers/gpu/drm/i915/i915_irq.c         |  647 ++++++++++++++++---------------
 drivers/gpu/drm/i915/i915_params.c      |   51 +++
 drivers/gpu/drm/i915/i915_reg.h         |   10 +
 drivers/gpu/drm/i915/intel_display.c    |   40 +-
 drivers/gpu/drm/i915/intel_lrc.c        |   14 +-
 drivers/gpu/drm/i915/intel_ringbuffer.c |  542 +++++++++++++++++++++++++-
 drivers/gpu/drm/i915/intel_ringbuffer.h |   90 ++++-
 drivers/gpu/drm/i915/intel_uncore.c     |  126 +++++-
 include/drm/drmP.h                      |    7 +
 15 files changed, 1695 insertions(+), 442 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index 99f65d2..147feb62 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -4344,8 +4344,38 @@ i915_wedged_set(void *data, u64 val)
 
 	intel_runtime_pm_get(dev_priv);
 
-	i915_handle_error(dev, val,
-			  "Manually setting wedged to %llu", val);
+	if (!i915_reset_in_progress(&dev_priv->gpu_error)) {
+		switch (val) {
+		case RCS:
+			DRM_INFO("Manual RCS reset\n");
+			i915_handle_error(dev,
+					  &dev_priv->ring[RCS].hangcheck,
+					  "Manual RCS reset");
+			break;
+		case VCS:
+			DRM_INFO("Manual VCS reset\n");
+			i915_handle_error(dev,
+					  &dev_priv->ring[VCS].hangcheck,
+					  "Manual VCS reset");
+			break;
+		case BCS:
+			DRM_INFO("Manual BCS reset\n");
+			i915_handle_error(dev,
+					  &dev_priv->ring[BCS].hangcheck,
+					  "Manual BCS reset");
+			break;
+		case VECS:
+			DRM_INFO("Manual VECS reset\n");
+			i915_handle_error(dev,
+					  &dev_priv->ring[VECS].hangcheck,
+					  "Manual VECS reset");
+			break;
+		default:
+			DRM_INFO("Manual global reset\n");
+			i915_handle_error(dev, NULL, "Manual global reset");
+			break;
+		}
+	}
 
 	intel_runtime_pm_put(dev_priv);
 
@@ -4356,6 +4386,92 @@ DEFINE_SIMPLE_ATTRIBUTE(i915_wedged_fops,
 			i915_wedged_get, i915_wedged_set,
 			"%llu\n");
 
+static const char *ringid_to_str(enum intel_ring_id ring_id)
+{
+	switch (ring_id) {
+	case RCS:
+		return "RCS";
+	case VCS:
+		return "VCS";
+	case BCS:
+		return "BCS";
+	case VECS:
+		return "VECS";
+	case VCS2:
+		return "VCS2";
+	}
+
+	return "unknown";
+}
+
+static ssize_t
+i915_ring_hangcheck_read(struct file *filp, char __user *ubuf,
+			 size_t max, loff_t *ppos)
+{
+	int i;
+	int len;
+	char buf[300];
+	struct drm_device *dev = filp->private_data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/*
+	 * Returns the total number of times the rings
+	 * have hung and been reset since boot
+	 */
+	len = scnprintf(buf, sizeof(buf), "GPU=0x%08lX,",
+			dev_priv->gpu_error.total_resets);
+
+	for (i = 0; i < I915_NUM_RINGS; ++i)
+		len += scnprintf(buf + len, sizeof(buf) - len,
+				 "%s=0x%08lX,",
+				 ringid_to_str(i),
+				 (long unsigned)
+				 dev_priv->ring[i].hangcheck.total);
+
+	for (i = 0; i < I915_NUM_RINGS; ++i)
+		len += scnprintf(buf + len, sizeof(buf) - len,
+				 "%s_T=0x%08lX,",
+				 ringid_to_str(i),
+				 (long unsigned)
+				 dev_priv->ring[i].hangcheck.tdr_count);
+	len += scnprintf(buf + len - 1, sizeof(buf) - len, "\n");
+
+	return simple_read_from_buffer(ubuf, max, ppos, buf, len);
+}
+
+static ssize_t
+i915_ring_hangcheck_write(struct file *filp,
+			const char __user *ubuf,
+			size_t cnt, loff_t *ppos)
+{
+	int ret;
+	int i;
+	struct drm_device *dev = filp->private_data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		/* Reset the hangcheck counters */
+		dev_priv->ring[i].hangcheck.total = 0;
+		dev_priv->ring[i].hangcheck.tdr_count = 0;
+	}
+	dev_priv->gpu_error.total_resets = 0;
+	mutex_unlock(&dev->struct_mutex);
+
+	return cnt;
+}
+
+static const struct file_operations i915_ring_hangcheck_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = i915_ring_hangcheck_read,
+	.write = i915_ring_hangcheck_write,
+	.llseek = default_llseek,
+};
+
 static int
 i915_ring_stop_get(void *data, u64 *val)
 {
@@ -5169,6 +5285,7 @@ static const struct i915_debugfs_files {
 	{"i915_ring_missed_irq", &i915_ring_missed_irq_fops},
 	{"i915_ring_test_irq", &i915_ring_test_irq_fops},
 	{"i915_gem_drop_caches", &i915_drop_caches_fops},
+	{"i915_ring_hangcheck", &i915_ring_hangcheck_fops},
 	{"i915_error_state", &i915_error_state_fops},
 	{"i915_next_seqno", &i915_next_seqno_fops},
 	{"i915_display_crc_ctl", &i915_display_crc_ctl_fops},
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index a99212d..a105caf 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1573,6 +1573,38 @@ static void intel_device_info_runtime_init(struct drm_device *dev)
 	}
 }
 
+static void
+i915_hangcheck_init(struct drm_device *dev)
+{
+	int i;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		dev_priv->ring[i].hangcheck.count = 0;
+		dev_priv->ring[i].hangcheck.tdr_count = 0;
+		dev_priv->ring[i].hangcheck.total = 0;
+		dev_priv->ring[i].hangcheck.last_acthd = 0;
+		dev_priv->ring[i].hangcheck.ringid = i;
+		dev_priv->ring[i].hangcheck.dev = dev;
+		atomic_set(&dev_priv->ring[i].hangcheck.active, 0);
+
+		setup_timer(&dev_priv->ring[i].hangcheck.timer,
+			i915_hangcheck_sample,
+			(unsigned long) &dev_priv->ring[i].hangcheck);
+	}
+}
+
+static void
+i915_hangcheck_cleanup(struct drm_i915_private *dev_priv)
+{
+	int i;
+
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		del_timer_sync(&dev_priv->ring[i].hangcheck.timer);
+		atomic_set(&dev_priv->ring[i].hangcheck.active, 0);
+	}
+}
+
 /**
  * i915_driver_load - setup chip and create an initial config
  * @dev: DRM device
@@ -1746,6 +1778,8 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 
 	i915_gem_load(dev);
 
+	i915_hangcheck_init(dev);
+
 	/* On the 945G/GM, the chipset reports the MSI capability on the
 	 * integrated graphics even though the support isn't actually there
 	 * according to the published specs.  It doesn't appear to function
@@ -1887,10 +1921,11 @@ int i915_driver_unload(struct drm_device *dev)
 	}
 
 	/* Free error state after interrupts are fully disabled. */
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
 	cancel_work_sync(&dev_priv->gpu_error.work);
 	i915_destroy_error_state(dev);
 
+	i915_hangcheck_cleanup(dev_priv);
+
 	if (dev->pdev->msi_enabled)
 		pci_disable_msi(dev->pdev);
 
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index 439f865..bb9dfb1 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -507,6 +507,7 @@ static int i915_drm_freeze(struct drm_device *dev)
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_crtc *crtc;
 	int ret;
+	u32 i;
 
 	/* ignore lid events during suspend */
 	mutex_lock(&dev_priv->modeset_restore_lock);
@@ -544,6 +545,14 @@ static int i915_drm_freeze(struct drm_device *dev)
 			spin_unlock_irq(&dev_priv->irq_lock);
 		}
 
+		/* Clear any pending reset requests. They should be picked up
+		* after resume when new work is submitted */
+		for (i = 0; i < I915_NUM_RINGS; i++)
+			atomic_set(&dev_priv->ring[i].hangcheck.flags, 0);
+
+		atomic_clear_mask(I915_RESET_IN_PROGRESS_FLAG,
+			&dev_priv->gpu_error.reset_counter);
+
 		drm_irq_uninstall(dev);
 
 		intel_suspend_gt_powersave(dev);
@@ -779,6 +788,179 @@ static int i915_resume_legacy(struct drm_device *dev)
 }
 
 /**
+ * i915_handle_hung_ring - reset ring after a hang
+ * @ringid: ring id to be reset
+ *
+ * TDR - Reset Individual ring: Useful if a hang is detected on a ring.
+ * Returns zero on successful reset or otherwise an error code.
+ *
+ * Procedure is fairly simple:
+ *   - ring is first disabled
+ *   - ring specific registers are saved
+ *   - reset the ring using chipset reg
+ *   - restore saved ring specific registers
+ *   - enable the ring after reset
+ *
+ * If page flips are submitted using rings and if the ring is hung
+ * on a page flip it will be released after reset.
+ *
+ * WARNING: Hold dev->struct_mutex before entering this function
+ */
+int i915_handle_hung_ring(struct drm_device *dev, uint32_t ringid)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *ring = &dev_priv->ring[ringid];
+	int ret = 0;
+	int pipe = 0;
+	uint32_t head;
+	uint32_t acthd;
+	uint32_t ring_flags = 0;
+	uint32_t completed_seqno;
+	struct drm_crtc *crtc;
+	struct intel_crtc *intel_crtc;
+	struct drm_i915_gem_request *request;
+	struct intel_unpin_work *unpin_work;
+
+	acthd = intel_ring_get_active_head(ring);
+	completed_seqno = ring->get_seqno(ring, false);
+
+	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+
+	/* Take wake lock to prevent power saving mode */
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+
+	/* Search the request list to see which batch buffer caused
+	* the hang. Only checks requests that haven't yet completed.*/
+	list_for_each_entry(request, &ring->request_list, list) {
+		if (request && (request->seqno > completed_seqno))
+			i915_set_reset_status(dev_priv, request->ctx, false);
+	}
+
+	/*
+	 * Check if the ring has hung on a MI_DISPLAY_FLIP command.
+	 * The pipe value will be stored in the HWS page if it has.
+	 * At the moment this should only happen for the blitter but
+	 * each ring has its own status page so this should work for
+	 * all rings
+	 */
+	pipe = intel_read_status_page(ring, I915_GEM_PGFLIP_INDEX);
+	if (pipe) {
+		/* Clear it to avoid responding to it twice */
+		intel_write_status_page(ring, I915_GEM_PGFLIP_INDEX, 0);
+	}
+
+	/* Clear any simulated hang flags */
+	if (dev_priv->gpu_error.stop_rings) {
+		DRM_DEBUG_TDR("Simulated gpu hang, rst stop_rings bits %08x\n",
+			(0x1 << ringid));
+		dev_priv->gpu_error.stop_rings &= ~(0x1 << ringid);
+	}
+
+	ret = intel_ring_disable(ring);
+	if (ret != 0) {
+		DRM_ERROR("Failed to disable ring %d\n", ringid);
+			goto handle_hung_ring_error;
+	}
+
+	/* Sample the current ring head position */
+	head = I915_READ(RING_HEAD(ring->mmio_base)) & HEAD_ADDR;
+	DRM_DEBUG_TDR("head 0x%08X, last_head 0x%08X\n",
+		head, dev_priv->ring[ringid].hangcheck.last_head);
+	if (head == dev_priv->ring[ringid].hangcheck.last_head) {
+		/*
+		 * The ring has not advanced since the last
+		 * time it hung so force it to advance to the
+		 * next QWORD. In most cases the ring head
+		 * pointer will automatically advance to the
+		 * next instruction as soon as it has read the
+		 * current instruction, without waiting for it
+		 * to complete. This seems to be the default
+		 * behaviour, however an MBOX wait inserted
+		 * directly to the VCS/BCS rings does not behave
+		 * in the same way, instead the head pointer
+		 * will still be pointing at the MBOX instruction
+		 * until it completes.
+		 */
+		ring_flags = FORCE_ADVANCE;
+		DRM_DEBUG_TDR("Force ring head to advance\n");
+	}
+	dev_priv->ring[ringid].hangcheck.last_head = head;
+
+	ret = intel_ring_save(ring, ring_flags);
+	if (ret != 0) {
+		DRM_ERROR("Failed to save ring state\n");
+		goto handle_hung_ring_error;
+	}
+
+	ret = intel_gpu_engine_reset(dev, ringid);
+	if (ret != 0) {
+		DRM_ERROR("Failed to reset ring\n");
+		goto handle_hung_ring_error;
+	}
+	DRM_DEBUG_TDR("%s reset (GPU Hang)\n", ring->name);
+
+	ret = intel_ring_invalidate_tlb(ring);
+	if (ret != 0) {
+		DRM_ERROR("Failed to invalidate tlb for %s\n", ring->name);
+		goto handle_hung_ring_error;
+	}
+
+	/* Clear last_acthd in hangcheck timer for this ring */
+	dev_priv->ring[ringid].hangcheck.last_acthd = 0;
+
+	/* Clear reset flags to allow future hangchecks */
+	atomic_set(&dev_priv->ring[ringid].hangcheck.flags, 0);
+
+	ret = intel_ring_restore(ring);
+	if (ret != 0) {
+		DRM_ERROR("Failed to restore ring state\n");
+		goto handle_hung_ring_error;
+	}
+
+	/* Correct driver state */
+	intel_ring_resample(ring);
+
+	ret = intel_ring_enable(ring);
+	if (ret != 0) {
+		DRM_ERROR("Failed to enable ring\n");
+		goto handle_hung_ring_error;
+	}
+
+	/* Wake up anything waiting on this rings queue */
+	wake_up_all(&ring->irq_queue);
+
+	/*
+	 * Note: This will not happen when MMIO based page flipping is used.
+	 * Page flipping should continue unhindered as it will
+	 * not be relying on a ring.
+	 */
+	if (pipe &&
+		((pipe - 1) < ARRAY_SIZE(dev_priv->pipe_to_crtc_mapping))) {
+		/* The pipe value in the status page if offset by 1 */
+		pipe -= 1;
+
+		/* The ring hung on a page flip command so we
+		* must manually release the pending flip queue */
+		crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+		intel_crtc = to_intel_crtc(crtc);
+		unpin_work = intel_crtc->unpin_work;
+
+		if (unpin_work && unpin_work->pending_flip_obj) {
+			intel_prepare_page_flip(dev, intel_crtc->pipe);
+			intel_finish_page_flip(dev, intel_crtc->pipe);
+			DRM_DEBUG_TDR("Released stuck page flip for pipe %d\n",
+				pipe);
+		}
+	}
+
+handle_hung_ring_error:
+	/* Release power lock */
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+
+	return ret;
+}
+
+/**
  * i915_reset - reset chip after a hang
  * @dev: drm device to reset
  *
@@ -804,21 +986,30 @@ int i915_reset(struct drm_device *dev)
 
 	mutex_lock(&dev->struct_mutex);
 
+	DRM_ERROR("Reset GPU (GPU Hang)\n");
+
 	i915_gem_reset(dev);
 
 	simulated = dev_priv->gpu_error.stop_rings != 0;
 
-	ret = intel_gpu_reset(dev);
-
-	/* Also reset the gpu hangman. */
-	if (simulated) {
-		DRM_INFO("Simulated gpu hang, resetting stop_rings\n");
-		dev_priv->gpu_error.stop_rings = 0;
-		if (ret == -ENODEV) {
-			DRM_INFO("Reset not implemented, but ignoring "
-				 "error for simulated gpu hangs\n");
-			ret = 0;
-		}
+	if (!simulated && (get_seconds() - dev_priv->gpu_error.last_reset)
+		< i915.gpu_reset_min_alive_period) {
+		DRM_ERROR("GPU hanging too fast, declaring wedged!\n");
+		ret = -ENODEV;
+	} else {
+		ret = intel_gpu_reset(dev);
+
+		/* Also reset the gpu hangman. */
+		if (simulated) {
+			DRM_INFO("Simulated gpu hang, resetting stop_rings\n");
+			dev_priv->gpu_error.stop_rings = 0;
+			if (ret == -ENODEV) {
+				DRM_INFO("Reset not implemented, but ignoring "
+					  "error for simulated gpu hangs\n");
+				ret = 0;
+			}
+		} else
+			dev_priv->gpu_error.last_reset = get_seconds();
 	}
 
 	if (ret) {
@@ -1468,7 +1659,6 @@ static int intel_runtime_suspend(struct device *device)
 		return ret;
 	}
 
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
 	dev_priv->pm.suspended = true;
 
 	/*
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 4b6760c..7b3316d 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -345,7 +345,6 @@ struct drm_i915_error_state {
 		bool valid;
 		/* Software tracked state */
 		bool waiting;
-		int hangcheck_score;
 		enum intel_ring_hangcheck_action hangcheck_action;
 		int num_requests;
 
@@ -362,7 +361,7 @@ struct drm_i915_error_state {
 		u32 hws;
 		u32 ipeir;
 		u32 ipehr;
-		u32 instdone;
+		u32 instdone[I915_NUM_INSTDONE_REG];
 		u32 bbstate;
 		u32 instpm;
 		u32 instps;
@@ -1236,12 +1235,11 @@ struct i915_error_state_file_priv {
 
 struct i915_gpu_error {
 	/* For hangcheck timer */
-#define DRM_I915_HANGCHECK_PERIOD 1500 /* in ms */
-#define DRM_I915_HANGCHECK_JIFFIES msecs_to_jiffies(DRM_I915_HANGCHECK_PERIOD)
+#define DRM_I915_MIN_HANGCHECK_PERIOD 100 /* 100ms */
+#define DRM_I915_MAX_HANGCHECK_PERIOD 30000 /* 30s */
+#define DRM_I915_HANGCHECK_JIFFIES msecs_to_jiffies(i915.hangcheck_period)
 	/* Hang gpu twice in this window and your context gets banned */
-#define DRM_I915_CTX_BAN_PERIOD DIV_ROUND_UP(8*DRM_I915_HANGCHECK_PERIOD, 1000)
-
-	struct timer_list hangcheck_timer;
+#define DRM_I915_CTX_BAN_PERIOD DIV_ROUND_UP(8*i915.hangcheck_period, 1000)
 
 	/* For reset and error_state handling. */
 	spinlock_t lock;
@@ -1249,12 +1247,17 @@ struct i915_gpu_error {
 	struct drm_i915_error_state *first_error;
 	struct work_struct work;
 
-
+	unsigned long last_reset;
 	unsigned long missed_irq_rings;
 
 	/**
 	 * State variable controlling the reset flow and count
 	 *
+	 * NOTE: This is for global reset only. TDR resets are handled
+	 * separately and do not modify reset_counter. This is OK
+	 * because TDR seqno's will still be signalled where as
+	 * with global reset the seqno completion will be lost.
+	 *
 	 * This is a counter which gets incremented when reset is triggered,
 	 * and again when reset has been handled. So odd values (lowest bit set)
 	 * means that reset is in progress and even values that
@@ -1291,6 +1294,8 @@ struct i915_gpu_error {
 #define I915_STOP_RING_ALLOW_BAN       (1 << 31)
 #define I915_STOP_RING_ALLOW_WARN      (1 << 30)
 
+	unsigned long total_resets;
+
 	/* For missed irq/seqno simulation. */
 	unsigned int test_irq_rings;
 };
@@ -2305,6 +2310,9 @@ struct i915_params {
 	int enable_rps_boost;
 	int invert_brightness;
 	int enable_cmd_parser;
+	unsigned int hangcheck_period;
+	unsigned int ring_reset_min_alive_period;
+	unsigned int gpu_reset_min_alive_period;
 	/* leave bools at the end to not create holes */
 	bool enable_hangcheck;
 	bool fastboot;
@@ -2338,6 +2346,9 @@ extern int i915_emit_box(struct drm_device *dev,
 			 struct drm_clip_rect *box,
 			 int DR1, int DR4);
 extern int intel_gpu_reset(struct drm_device *dev);
+extern int intel_gpu_engine_reset(struct drm_device *dev,
+				  enum intel_ring_id engine);
+extern int i915_handle_hung_ring(struct drm_device *dev, uint32_t ringid);
 extern int i915_reset(struct drm_device *dev);
 extern unsigned long i915_chipset_val(struct drm_i915_private *dev_priv);
 extern unsigned long i915_mch_val(struct drm_i915_private *dev_priv);
@@ -2348,10 +2359,11 @@ int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool on);
 extern void intel_console_resume(struct work_struct *work);
 
 /* i915_irq.c */
-void i915_queue_hangcheck(struct drm_device *dev);
+void i915_queue_hangcheck(struct drm_device *dev, u32 ringid);
 __printf(3, 4)
-void i915_handle_error(struct drm_device *dev, bool wedged,
+void i915_handle_error(struct drm_device *dev, struct intel_ring_hangcheck *hc,
 		       const char *fmt, ...);
+void i915_hangcheck_sample(unsigned long data);
 
 void gen6_set_pm_mask(struct drm_i915_private *dev_priv, u32 pm_iir,
 							int new_delay);
@@ -2501,6 +2513,10 @@ int i915_gem_dumb_create(struct drm_file *file_priv,
 			 struct drm_mode_create_dumb *args);
 int i915_gem_mmap_gtt(struct drm_file *file_priv, struct drm_device *dev,
 		      uint32_t handle, uint64_t *offset);
+void i915_set_reset_status(struct drm_i915_private *dev_priv,
+				  struct intel_context *ctx,
+				  const bool guilty);
+
 /**
  * Returns true if seq1 is later than seq2.
  */
@@ -2524,9 +2540,10 @@ i915_gem_find_active_request(struct intel_engine_cs *ring);
 bool i915_gem_retire_requests(struct drm_device *dev);
 void i915_gem_retire_requests_ring(struct intel_engine_cs *ring);
 int __must_check i915_gem_check_wedge(struct i915_gpu_error *error,
-				      bool interruptible);
+				      bool interruptible,
+				      struct intel_engine_cs *ring);
 int __must_check i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno);
-
+int i915_gem_wedged(struct drm_device *dev, bool interruptible);
 static inline bool i915_reset_in_progress(struct i915_gpu_error *error)
 {
 	return unlikely(atomic_read(&error->reset_counter)
@@ -2784,14 +2801,14 @@ static inline void i915_error_state_buf_release(
 {
 	kfree(eb->buf);
 }
-void i915_capture_error_state(struct drm_device *dev, bool wedge,
-			      const char *error_msg);
+void i915_capture_error_state(struct drm_device *dev, const char *error_msg);
 void i915_error_state_get(struct drm_device *dev,
 			  struct i915_error_state_file_priv *error_priv);
 void i915_error_state_put(struct i915_error_state_file_priv *error_priv);
 void i915_destroy_error_state(struct drm_device *dev);
 
-void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone);
+void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone,
+			     struct intel_engine_cs *ring);
 const char *i915_cache_level_str(int type);
 
 /* i915_cmd_parser.c */
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index f5300cc..cf7895c 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -110,11 +110,11 @@ static void i915_gem_info_remove_obj(struct drm_i915_private *dev_priv,
 }
 
 static int
-i915_gem_wait_for_error(struct i915_gpu_error *error)
+i915_gem_wait_for_error(struct drm_device *dev, struct i915_gpu_error *error)
 {
 	int ret;
 
-#define EXIT_COND (!i915_reset_in_progress(error) || \
+#define EXIT_COND (!i915_gem_wedged(dev, true) || \
 		   i915_terminally_wedged(error))
 	if (EXIT_COND)
 		return 0;
@@ -127,7 +127,9 @@ i915_gem_wait_for_error(struct i915_gpu_error *error)
 	ret = wait_event_interruptible_timeout(error->reset_queue,
 					       EXIT_COND,
 					       10*HZ);
-	if (ret == 0) {
+	if (i915_terminally_wedged(error)) {
+		return -EIO;
+	} else if (ret == 0) {
 		DRM_ERROR("Timed out waiting for the gpu reset to complete\n");
 		return -EIO;
 	} else if (ret < 0) {
@@ -138,15 +140,46 @@ i915_gem_wait_for_error(struct i915_gpu_error *error)
 	return 0;
 }
 
-int i915_mutex_lock_interruptible(struct drm_device *dev)
+int i915_gem_wedged(struct drm_device *dev, bool interruptible)
 {
+	/*
+	 * Warning: This function can only give an indication
+	 * if the GPU is wedged at a particular instance of time.
+	 * The hangcheck process is asynchronous so a hang
+	 * may be detected just after the flags have been sampled
+	 */
+	unsigned i;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
+	int err = !interruptible ? -EIO : -EAGAIN;
 
-	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
-	if (ret)
-		return ret;
+	/* Full reset requested */
+	if (i915_reset_in_progress(&dev_priv->gpu_error))
+		return err;
+
+	/* Check for an individual ring which has hung */
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		if (atomic_read(&dev_priv->ring[i].hangcheck.flags)
+		& DRM_I915_HANGCHECK_HUNG) {
+			printk_ratelimited(KERN_ERR "%s() (%d) intr %d\n",
+				__func__, i, interruptible);
+
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+int i915_mutex_lock_interruptible(struct drm_device *dev)
+{
+	int ret;
 
+	/*
+	 * There should be no need to call i915_gem_wait_for_error
+	 * as the error recovery handler takes dev->struct_mutex
+	 * so if it is active we will wait on the mutex_lock_interruptible
+	 * call instead
+	 */
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
 	if (ret)
 		return ret;
@@ -1074,9 +1107,16 @@ unlock:
 
 int
 i915_gem_check_wedge(struct i915_gpu_error *error,
-		     bool interruptible)
+		     bool interruptible,
+		     struct intel_engine_cs *ring)
 {
-	if (i915_reset_in_progress(error)) {
+	struct drm_i915_private *dev_priv;
+	dev_priv = container_of(error, struct drm_i915_private, gpu_error);
+
+	if ((ring && (atomic_read(&dev_priv->ring[ring->id].hangcheck.flags)
+		      & DRM_I915_HANGCHECK_HUNG))
+	    || i915_reset_in_progress(error)) {
+
 		/* Non-interruptible callers can't handle -EAGAIN, hence return
 		 * -EIO unconditionally for these. */
 		if (!interruptible)
@@ -1171,7 +1211,8 @@ static int __wait_seqno(struct intel_engine_cs *ring, u32 seqno,
 	struct timespec before, now;
 	DEFINE_WAIT(wait);
 	unsigned long timeout_expire;
-	int ret;
+	int ret = 0;
+	int gem_wedged;
 
 	WARN(dev_priv->pm.irqs_disabled, "IRQs disabled\n");
 
@@ -1201,13 +1242,19 @@ static int __wait_seqno(struct intel_engine_cs *ring, u32 seqno,
 		prepare_to_wait(&ring->irq_queue, &wait,
 				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
 
-		/* We need to check whether any gpu reset happened in between
-		 * the caller grabbing the seqno and now ... */
-		if (reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter)) {
+		/*
+		 * We need to check whether any gpu reset happened in between
+		 * the caller grabbing the seqno and now ...
+		 */
+		gem_wedged = i915_gem_wedged(ring->dev, interruptible);
+		if (reset_counter !=
+		    atomic_read(&dev_priv->gpu_error.reset_counter) ||
+		    gem_wedged != 0) {
 			/* ... but upgrade the -EAGAIN to an -EIO if the gpu
 			 * is truely gone. */
-			ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
-			if (ret == 0)
+			if (gem_wedged != 0)
+				ret = gem_wedged;
+			else
 				ret = -EAGAIN;
 			break;
 		}
@@ -1276,7 +1323,7 @@ i915_wait_seqno(struct intel_engine_cs *ring, uint32_t seqno)
 	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
 	BUG_ON(seqno == 0);
 
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
+	ret = i915_gem_wedged(dev, interruptible);
 	if (ret)
 		return ret;
 
@@ -1353,7 +1400,7 @@ i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
 	if (seqno == 0)
 		return 0;
 
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, true);
+	ret = i915_gem_check_wedge(&dev_priv->gpu_error, true, ring);
 	if (ret)
 		return ret;
 
@@ -2437,8 +2484,6 @@ int __i915_add_request(struct intel_engine_cs *ring,
 	ring->preallocated_lazy_request = NULL;
 
 	if (!dev_priv->ums.mm_suspended) {
-		i915_queue_hangcheck(ring->dev);
-
 		cancel_delayed_work_sync(&dev_priv->mm.idle_work);
 		queue_delayed_work(dev_priv->wq,
 				   &dev_priv->mm.retire_work,
@@ -2489,7 +2534,7 @@ static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
 	return false;
 }
 
-static void i915_set_reset_status(struct drm_i915_private *dev_priv,
+void i915_set_reset_status(struct drm_i915_private *dev_priv,
 				  struct intel_context *ctx,
 				  const bool guilty)
 {
@@ -2549,7 +2594,8 @@ static void i915_gem_reset_ring_status(struct drm_i915_private *dev_priv,
 	if (request == NULL)
 		return;
 
-	ring_hung = ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG;
+	ring_hung = (atomic_read(&dev_priv->ring[ring->id].hangcheck.flags)
+		     & DRM_I915_HANGCHECK_HUNG);
 
 	i915_set_reset_status(dev_priv, request->ctx, ring_hung);
 
@@ -4055,11 +4101,7 @@ i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
 	u32 seqno = 0;
 	int ret;
 
-	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
-	if (ret)
-		return ret;
-
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, false);
+	ret = i915_gem_wait_for_error(dev, &dev_priv->gpu_error);
 	if (ret)
 		return ret;
 
@@ -4077,9 +4119,16 @@ i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
 	if (seqno == 0)
 		return 0;
 
-	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, NULL);
-	if (ret == 0)
-		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work, 0);
+	if (ring) {
+		if (i915_gem_wedged(dev, 1) != 0)
+			return -EIO;
+
+		ret = __wait_seqno(ring, seqno, reset_counter, true,
+								NULL, NULL);
+		if (ret == 0)
+			queue_delayed_work(dev_priv->wq,
+				&dev_priv->mm.retire_work, 0);
+	}
 
 	return ret;
 }
@@ -4597,6 +4646,7 @@ i915_gem_suspend(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int ret = 0;
+	int i;
 
 	mutex_lock(&dev->struct_mutex);
 	if (dev_priv->ums.mm_suspended)
@@ -4623,7 +4673,10 @@ i915_gem_suspend(struct drm_device *dev)
 							     DRIVER_MODESET);
 	mutex_unlock(&dev->struct_mutex);
 
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		del_timer_sync(&dev_priv->ring[i].hangcheck.timer);
+		atomic_set(&dev_priv->ring[i].hangcheck.active, 0);
+	}
 	cancel_delayed_work_sync(&dev_priv->mm.retire_work);
 	cancel_delayed_work_sync(&dev_priv->mm.idle_work);
 
@@ -4886,12 +4939,18 @@ i915_gem_entervt_ioctl(struct drm_device *dev, void *data,
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int ret;
+	int i;
 
 	if (drm_core_check_feature(dev, DRIVER_MODESET))
 		return 0;
 
-	if (i915_reset_in_progress(&dev_priv->gpu_error)) {
+	if (i915_gem_wedged(dev, false)) {
 		DRM_ERROR("Reenabling wedged hardware, good luck\n");
+
+		for (i = 0; i < I915_NUM_RINGS; i++) {
+			/* Clear the reset flag */
+			atomic_set(&dev_priv->ring[i].hangcheck.flags, 0);
+		}
 		atomic_set(&dev_priv->gpu_error.reset_counter, 0);
 	}
 
diff --git a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
index 6ab5926..40c3233 100644
--- a/drivers/gpu/drm/i915/i915_gpu_error.c
+++ b/drivers/gpu/drm/i915/i915_gpu_error.c
@@ -242,6 +242,8 @@ static void i915_ring_error_state(struct drm_i915_error_state_buf *m,
 				  struct drm_device *dev,
 				  struct drm_i915_error_ring *ring)
 {
+	int i;
+
 	if (!ring->valid)
 		return;
 
@@ -252,7 +254,9 @@ static void i915_ring_error_state(struct drm_i915_error_state_buf *m,
 	err_printf(m, "  ACTHD: 0x%08x %08x\n", (u32)(ring->acthd>>32), (u32)ring->acthd);
 	err_printf(m, "  IPEIR: 0x%08x\n", ring->ipeir);
 	err_printf(m, "  IPEHR: 0x%08x\n", ring->ipehr);
-	err_printf(m, "  INSTDONE: 0x%08x\n", ring->instdone);
+	for (i = 0; i < ARRAY_SIZE(ring->instdone); i++)
+		err_printf(m, "  INSTDONE_%d: 0x%08x\n", i, ring->instdone[i]);
+
 	if (INTEL_INFO(dev)->gen >= 4) {
 		err_printf(m, "  BBADDR: 0x%08x %08x\n", (u32)(ring->bbaddr>>32), (u32)ring->bbaddr);
 		err_printf(m, "  BB_STATE: 0x%08x\n", ring->bbstate);
@@ -293,9 +297,8 @@ static void i915_ring_error_state(struct drm_i915_error_state_buf *m,
 	err_printf(m, "  waiting: %s\n", yesno(ring->waiting));
 	err_printf(m, "  ring->head: 0x%08x\n", ring->cpu_ring_head);
 	err_printf(m, "  ring->tail: 0x%08x\n", ring->cpu_ring_tail);
-	err_printf(m, "  hangcheck: %s [%d]\n",
-		   hangcheck_action_to_str(ring->hangcheck_action),
-		   ring->hangcheck_score);
+	err_printf(m, "  hangcheck: %s\n",
+		   hangcheck_action_to_str(ring->hangcheck_action));
 }
 
 void i915_error_printf(struct drm_i915_error_state_buf *e, const char *f, ...)
@@ -328,7 +331,6 @@ int i915_error_state_to_str(struct drm_i915_error_state_buf *m,
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_error_state *error = error_priv->error;
 	int i, j, offset, elt;
-	int max_hangcheck_score;
 
 	if (!error) {
 		err_printf(m, "no error state collected\n");
@@ -339,20 +341,6 @@ int i915_error_state_to_str(struct drm_i915_error_state_buf *m,
 	err_printf(m, "Time: %ld s %ld us\n", error->time.tv_sec,
 		   error->time.tv_usec);
 	err_printf(m, "Kernel: " UTS_RELEASE "\n");
-	max_hangcheck_score = 0;
-	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
-		if (error->ring[i].hangcheck_score > max_hangcheck_score)
-			max_hangcheck_score = error->ring[i].hangcheck_score;
-	}
-	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
-		if (error->ring[i].hangcheck_score == max_hangcheck_score &&
-		    error->ring[i].pid != -1) {
-			err_printf(m, "Active process (on ring %s): %s [%d]\n",
-				   ring_str(i),
-				   error->ring[i].comm,
-				   error->ring[i].pid);
-		}
-	}
 	err_printf(m, "Reset count: %u\n", error->reset_count);
 	err_printf(m, "Suspend count: %u\n", error->suspend_count);
 	err_printf(m, "PCI ID: 0x%04x\n", dev->pdev->device);
@@ -706,7 +694,8 @@ static uint32_t i915_error_generate_code(struct drm_i915_private *dev_priv,
 			if (ring_id)
 				*ring_id = i;
 
-			return error->ring[i].ipehr ^ error->ring[i].instdone;
+			return error->ring[i].ipehr ^
+				error->ring[i].instdone[0];
 		}
 	}
 
@@ -773,7 +762,6 @@ static void i915_record_ring_state(struct drm_device *dev,
 		ering->faddr = I915_READ(RING_DMA_FADD(ring->mmio_base));
 		ering->ipeir = I915_READ(RING_IPEIR(ring->mmio_base));
 		ering->ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
-		ering->instdone = I915_READ(RING_INSTDONE(ring->mmio_base));
 		ering->instps = I915_READ(RING_INSTPS(ring->mmio_base));
 		ering->bbaddr = I915_READ(RING_BBADDR(ring->mmio_base));
 		if (INTEL_INFO(dev)->gen >= 8) {
@@ -785,9 +773,11 @@ static void i915_record_ring_state(struct drm_device *dev,
 		ering->faddr = I915_READ(DMA_FADD_I8XX);
 		ering->ipeir = I915_READ(IPEIR);
 		ering->ipehr = I915_READ(IPEHR);
-		ering->instdone = I915_READ(INSTDONE);
 	}
 
+	i915_get_extra_instdone(dev, ering->instdone,
+				&dev_priv->ring[ring->id]);
+
 	ering->waiting = waitqueue_active(&ring->irq_queue);
 	ering->instpm = I915_READ(RING_INSTPM(ring->mmio_base));
 	ering->seqno = ring->get_seqno(ring, false);
@@ -825,7 +815,6 @@ static void i915_record_ring_state(struct drm_device *dev,
 		ering->hws = I915_READ(mmio);
 	}
 
-	ering->hangcheck_score = ring->hangcheck.score;
 	ering->hangcheck_action = ring->hangcheck.action;
 
 	if (USES_PPGTT(dev)) {
@@ -1047,6 +1036,7 @@ static void i915_gem_capture_buffers(struct drm_i915_private *dev_priv,
 static void i915_capture_reg_state(struct drm_i915_private *dev_priv,
 				   struct drm_i915_error_state *error)
 {
+	int i;
 	struct drm_device *dev = dev_priv->dev;
 
 	/* General organization
@@ -1105,12 +1095,13 @@ static void i915_capture_reg_state(struct drm_i915_private *dev_priv,
 	error->eir = I915_READ(EIR);
 	error->pgtbl_er = I915_READ(PGTBL_ER);
 
-	i915_get_extra_instdone(dev, error->extra_instdone);
+	for (i = 0; i < I915_NUM_RINGS; ++i)
+		i915_get_extra_instdone(dev, error->ring[i].instdone,
+					&dev_priv->ring[i]);
 }
 
 static void i915_error_capture_msg(struct drm_device *dev,
 				   struct drm_i915_error_state *error,
-				   bool wedged,
 				   const char *error_msg)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1125,14 +1116,10 @@ static void i915_error_capture_msg(struct drm_device *dev,
 	if (ring_id != -1 && error->ring[ring_id].pid != -1)
 		len += scnprintf(error->error_msg + len,
 				 sizeof(error->error_msg) - len,
-				 ", in %s [%d]",
+				 ", in %s [%d], reason %s",
 				 error->ring[ring_id].comm,
-				 error->ring[ring_id].pid);
-
-	scnprintf(error->error_msg + len, sizeof(error->error_msg) - len,
-		  ", reason: %s, action: %s",
-		  error_msg,
-		  wedged ? "reset" : "continue");
+				 error->ring[ring_id].pid,
+				 error_msg);
 }
 
 static void i915_capture_gen_state(struct drm_i915_private *dev_priv,
@@ -1151,7 +1138,7 @@ static void i915_capture_gen_state(struct drm_i915_private *dev_priv,
  * out a structure which becomes available in debugfs for user level tools
  * to pick up.
  */
-void i915_capture_error_state(struct drm_device *dev, bool wedged,
+void i915_capture_error_state(struct drm_device *dev,
 			      const char *error_msg)
 {
 	static bool warned;
@@ -1179,7 +1166,7 @@ void i915_capture_error_state(struct drm_device *dev, bool wedged,
 	error->overlay = intel_overlay_capture_error_state(dev);
 	error->display = intel_display_capture_error_state(dev);
 
-	i915_error_capture_msg(dev, error, wedged, error_msg);
+	i915_error_capture_msg(dev, error, error_msg);
 	DRM_INFO("%s\n", error->error_msg);
 
 	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
@@ -1251,7 +1238,8 @@ const char *i915_cache_level_str(int type)
 }
 
 /* NB: please notice the memset */
-void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone)
+void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone,
+			     struct intel_engine_cs *ring)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	memset(instdone, 0, sizeof(*instdone) * I915_NUM_INSTDONE_REG);
@@ -1271,10 +1259,12 @@ void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone)
 		WARN_ONCE(1, "Unsupported platform\n");
 	case 7:
 	case 8:
-		instdone[0] = I915_READ(GEN7_INSTDONE_1);
-		instdone[1] = I915_READ(GEN7_SC_INSTDONE);
-		instdone[2] = I915_READ(GEN7_SAMPLER_INSTDONE);
-		instdone[3] = I915_READ(GEN7_ROW_INSTDONE);
+		instdone[0] = I915_READ(RING_INSTDONE(ring->mmio_base));
+		if (ring->id == RCS) {
+			instdone[1] = I915_READ(GEN7_SC_INSTDONE);
+			instdone[2] = I915_READ(GEN7_SAMPLER_INSTDONE);
+			instdone[3] = I915_READ(GEN7_ROW_INSTDONE);
+		}
 		break;
 	}
 }
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index bdb1213..27c4ea1 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -1243,13 +1243,13 @@ static void notify_ring(struct drm_device *dev,
 	if (!intel_ring_initialized(ring))
 		return;
 
+	ring->last_irq_seqno = ring->get_seqno(ring, false);
 	trace_i915_gem_request_complete(ring);
 
 	if (drm_core_check_feature(dev, DRIVER_MODESET))
 		intel_notify_mmio_flip(ring);
 
 	wake_up_all(&ring->irq_queue);
-	i915_queue_hangcheck(dev);
 
 	i915_sync_timeline_advance(ring);
 }
@@ -1628,12 +1628,17 @@ static void snb_gt_irq_handler(struct drm_device *dev,
 	if (gt_iir & GT_BLT_USER_INTERRUPT)
 		notify_ring(dev, &dev_priv->ring[BCS]);
 
-	if (gt_iir & (GT_BLT_CS_ERROR_INTERRUPT |
-		      GT_BSD_CS_ERROR_INTERRUPT |
-		      GT_RENDER_CS_MASTER_ERROR_INTERRUPT)) {
-		i915_handle_error(dev, false, "GT error interrupt 0x%08x",
-				  gt_iir);
-	}
+	if (gt_iir & GT_RENDER_CS_MASTER_ERROR_INTERRUPT)
+		i915_handle_error(dev, &dev_priv->ring[RCS].hangcheck,
+				  "GT RCS error interrupt 0x%08x", gt_iir);
+
+	if (gt_iir & GT_BSD_CS_ERROR_INTERRUPT)
+		i915_handle_error(dev, &dev_priv->ring[VCS].hangcheck,
+				  "GT VCS error interrupt 0x%08x", gt_iir);
+
+	if (gt_iir & GT_BLT_CS_ERROR_INTERRUPT)
+		i915_handle_error(dev, &dev_priv->ring[BCS].hangcheck,
+				  "GT BCS error interrupt 0x%08x", gt_iir);
 
 	if (gt_iir & GT_PARITY_ERROR(dev))
 		ivybridge_parity_error_irq_handler(dev, gt_iir);
@@ -1962,7 +1967,9 @@ static void gen6_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir)
 			notify_ring(dev_priv->dev, &dev_priv->ring[VECS]);
 
 		if (pm_iir & PM_VEBOX_CS_ERROR_INTERRUPT) {
-			i915_handle_error(dev_priv->dev, false,
+			DRM_ERROR("VEBOX CS error interrupt 0x%08x\n", pm_iir);
+			i915_handle_error(dev_priv->dev,
+					  &dev_priv->ring[VECS].hangcheck,
 					  "VEBOX CS error interrupt 0x%08x",
 					  pm_iir);
 		}
@@ -2663,9 +2670,37 @@ static void i915_error_work_func(struct work_struct *work)
 	char *reset_event[] = { I915_RESET_UEVENT "=1", NULL };
 	char *reset_done_event[] = { I915_ERROR_UEVENT "=0", NULL };
 	int ret;
+	int i;
+
+	mutex_lock(&dev->struct_mutex);
 
 	kobject_uevent_env(&dev->primary->kdev->kobj, KOBJ_CHANGE, error_event);
 
+	/* Check each ring for a pending reset condition */
+	for (i = 0; i < I915_NUM_RINGS; i++) {
+		/* Skip individual ring reset requests if full_reset requested*/
+		if (i915_reset_in_progress(error))
+			break;
+
+		if (atomic_read(&dev_priv->ring[i].hangcheck.flags)
+		    & DRM_I915_HANGCHECK_RESET) {
+
+			if (i915_handle_hung_ring(dev, i) != 0) {
+				DRM_ERROR("ring %d reset failed", i);
+
+				/* Force global reset instead */
+				atomic_set_mask(
+				I915_RESET_IN_PROGRESS_FLAG,
+				&dev_priv->gpu_error.reset_counter);
+				break;
+			}
+		}
+	}
+
+	/* Release struct->mutex for the full GPU reset. It will take
+	* it itself when it needs it */
+	mutex_unlock(&dev->struct_mutex);
+
 	/*
 	 * Note that there's only one work item which does gpu resets, so we
 	 * need not worry about concurrent gpu resets potentially incrementing
@@ -2678,9 +2713,6 @@ static void i915_error_work_func(struct work_struct *work)
 	 */
 	if (i915_reset_in_progress(error) && !i915_terminally_wedged(error)) {
 		DRM_DEBUG_DRIVER("resetting chip\n");
-		kobject_uevent_env(&dev->primary->kdev->kobj, KOBJ_CHANGE,
-				   reset_event);
-
 		/*
 		 * In most cases it's guaranteed that we get here with an RPM
 		 * reference held, for example because there is a pending GPU
@@ -2697,11 +2729,17 @@ static void i915_error_work_func(struct work_struct *work)
 		 */
 		ret = i915_reset(dev);
 
+		kobject_uevent_env(&dev->primary->kdev->kobj, KOBJ_CHANGE,
+			reset_event);
 		intel_display_handle_reset(dev);
 
 		intel_runtime_pm_put(dev_priv);
 
 		if (ret == 0) {
+			for (i = 0; i < I915_NUM_RINGS; i++)
+				atomic_set(&dev_priv->ring[i].hangcheck.flags,
+					   0);
+
 			/*
 			 * After all the gem state is reset, increment the reset
 			 * counter and wake up everyone waiting for the reset to
@@ -2709,39 +2747,46 @@ static void i915_error_work_func(struct work_struct *work)
 			 *
 			 * Since unlock operations are a one-sided barrier only,
 			 * we need to insert a barrier here to order any seqno
-			 * updates before
-			 * the counter increment.
+			 * updates before the counter increment.
+			 * The increment clears the RESET_IN_PROGRESS flag.
 			 */
 			smp_mb__before_atomic();
 			atomic_inc(&dev_priv->gpu_error.reset_counter);
-
-			kobject_uevent_env(&dev->primary->kdev->kobj,
-					   KOBJ_CHANGE, reset_done_event);
 		} else {
+			/* Terminal wedge condition */
 			atomic_set_mask(I915_WEDGED, &error->reset_counter);
 		}
-
-		/*
-		 * Note: The wake_up also serves as a memory barrier so that
-		 * waiters see the update value of the reset counter atomic_t.
-		 */
-		i915_error_wake_up(dev_priv, true);
 	}
+
+	/*
+	 * Note: The wake_up also serves as a memory barrier so that
+	 * waiters see the update value of the reset counter atomic_t.
+	 */
+	i915_error_wake_up(dev_priv, true);
+
+	kobject_uevent_env(&dev->primary->kdev->kobj,
+		KOBJ_CHANGE, reset_done_event);
+
+	DRM_DEBUG_TDR("End recovery work\n");
 }
 
 static void i915_report_and_clear_eir(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t instdone[I915_NUM_INSTDONE_REG];
-	u32 eir = I915_READ(EIR);
 	int pipe, i;
+	u32 eir;
+	uint32_t instdone[I915_NUM_INSTDONE_REG];
+	unsigned long flags;
 
+	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
+
+	eir = I915_READ(EIR);
 	if (!eir)
-		return;
+		goto i915_report_and_clear_eir_exit;
 
 	pr_err("render error detected, EIR: 0x%08x\n", eir);
 
-	i915_get_extra_instdone(dev, instdone);
+	i915_get_extra_instdone(dev, instdone, &dev_priv->ring[RCS]);
 
 	if (IS_G4X(dev)) {
 		if (eir & (GM45_ERROR_MEM_PRIV | GM45_ERROR_CP_PRIV)) {
@@ -2819,6 +2864,8 @@ static void i915_report_and_clear_eir(struct drm_device *dev)
 		I915_WRITE(EMR, I915_READ(EMR) | eir);
 		I915_WRITE(IIR, I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT);
 	}
+i915_report_and_clear_eir_exit:
+	spin_unlock_irqrestore(&dev_priv->gpu_error.lock, flags);
 }
 
 /**
@@ -2831,9 +2878,12 @@ static void i915_report_and_clear_eir(struct drm_device *dev)
  * so userspace knows something bad happened (should trigger collection
  * of a ring dump etc.).
  */
-void i915_handle_error(struct drm_device *dev, bool wedged,
+void i915_handle_error(struct drm_device *dev, struct intel_ring_hangcheck *hc,
 		       const char *fmt, ...)
 {
+	int full_reset = 0;
+	unsigned long cur_time;
+	unsigned long last_reset;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	va_list args;
 	char error_msg[80];
@@ -2842,35 +2892,91 @@ void i915_handle_error(struct drm_device *dev, bool wedged,
 	vscnprintf(error_msg, sizeof(error_msg), fmt, args);
 	va_end(args);
 
-	i915_capture_error_state(dev, wedged, error_msg);
+	i915_capture_error_state(dev, error_msg);
 	i915_report_and_clear_eir(dev);
 
-	if (wedged) {
-		atomic_set_mask(I915_RESET_IN_PROGRESS_FLAG,
-				&dev_priv->gpu_error.reset_counter);
+	/*
+	 * Currently we only support individual ring reset for GEN7 onwards,
+	 * older chips will revert to a full reset.
+	 * Error interrupts trigger a full reset (hc == NULL)
+	 */
+	if ((INTEL_INFO(dev)->gen >= 7) && hc) {
+		/*
+		 * Flag that the ring has hung. It is this flag that is used
+		 * later to update the stats to determine which batch caused
+		 * the hang
+		 */
+		atomic_set_mask(DRM_I915_HANGCHECK_HUNG, &hc->flags);
+
+		/* Now determine what type of reset to use to clear the hang */
+		cur_time = get_seconds();
+		last_reset = hc->last_reset;
+		hc->last_reset = cur_time;
+
+		if ((cur_time - last_reset) <
+			i915.ring_reset_min_alive_period) {
+			/* This ring is hanging too frequently.
+			* Opt for full-reset instead */
+			DRM_DEBUG_TDR("Ring %d hanging too quickly...\r\n",
+				hc->ringid);
+			full_reset = 1;
+		} else {
+			/* Flag that we want to try and reset this ring.
+			* This can still be overridden by a global
+			* reset */
+			atomic_set_mask(DRM_I915_HANGCHECK_RESET, &hc->flags);
+			DRM_DEBUG_TDR("Reset Ring %d\n", hc->ringid);
+		}
+	} else
+		full_reset = 1;
 
+	/*
+	 * Note: We could mark a ring as 'hung' before we are able
+	 * to set the reset request flag so it may be recovered by
+	 * an existing reset request. That is OK - it just means that
+	 * there may be an extra reset
+	 */
+
+	if (!hc || full_reset) {
 		/*
-		 * Wakeup waiting processes so that the reset work function
-		 * i915_error_work_func doesn't deadlock trying to grab various
-		 * locks. By bumping the reset counter first, the woken
-		 * processes will see a reset in progress and back off,
-		 * releasing their locks and then wait for the reset completion.
-		 * We must do this for _all_ gpu waiters that might hold locks
-		 * that the reset work needs to acquire.
-		 *
-		 * Note: The wake_up serves as the required memory barrier to
-		 * ensure that the waiters see the updated value of the reset
-		 * counter atomic_t.
+		 * Flag that we want a global reset. If there are any
+		 * individual ring resets pending then they may
+		 * still be processed before the global reset request
+		 * is noticed by the error work function.
 		 */
-		i915_error_wake_up(dev_priv, false);
+		atomic_set_mask(I915_RESET_IN_PROGRESS_FLAG,
+			&dev_priv->gpu_error.reset_counter);
 	}
 
 	/*
+	 * Wakeup waiting processes so that the reset work function
+	 * i915_error_work_func doesn't deadlock trying to grab various
+	 * locks. By bumping the reset counter first, the woken
+	 * processes will see a reset in progress and back off,
+	 * releasing their locks and then wait for the reset completion.
+	 * We must do this for _all_ gpu waiters that might hold locks
+	 * that the reset work needs to acquire.
+	 *
+	 * Note: The wake_up serves as the required memory barrier to
+	 * ensure that the waiters see the updated value of the reset
+	 * counter atomic_t.
+	 */
+	i915_error_wake_up(dev_priv, false);
+
+	/*
 	 * Our reset work can grab modeset locks (since it needs to reset the
-	 * state of outstanding pagelips). Hence it must not be run on our own
+	 * state of outstanding pageflips). Hence it must not be run on our own
 	 * dev-priv->wq work queue for otherwise the flush_work in the pageflip
 	 * code will deadlock.
+	 * If error_work is already in the work queue then it will not be added
+	 * again. It hasn't yet executed so it will see the reset flags when
+	 * it is scheduled. If it isn't in the queue or it is currently
+	 * executing then this call will add it to the queue again so that
+	 * even if it misses the reset flags during the current call it is
+	 * guaranteed to see them on the next call.
 	 */
+	DRM_DEBUG_TDR("Queue error work...\n");
+
 	schedule_work(&dev_priv->gpu_error.work);
 }
 
@@ -3089,317 +3195,219 @@ ring_last_seqno(struct intel_engine_cs *ring)
 			  struct drm_i915_gem_request, list)->seqno;
 }
 
-static bool
-ring_idle(struct intel_engine_cs *ring, u32 seqno)
+static bool kick_ring(struct intel_engine_cs *ring)
 {
-	return (list_empty(&ring->request_list) ||
-		i915_seqno_passed(seqno, ring_last_seqno(ring)));
-}
-
-static bool
-ipehr_is_semaphore_wait(struct drm_device *dev, u32 ipehr)
-{
-	if (INTEL_INFO(dev)->gen >= 8) {
-		/*
-		 * FIXME: gen8 semaphore support - currently we don't emit
-		 * semaphores on bdw anyway, but this needs to be addressed when
-		 * we merge that code.
-		 */
-		return false;
-	} else {
-		ipehr &= ~MI_SEMAPHORE_SYNC_MASK;
-		return ipehr == (MI_SEMAPHORE_MBOX | MI_SEMAPHORE_COMPARE |
-				 MI_SEMAPHORE_REGISTER);
-	}
-}
-
-static struct intel_engine_cs *
-semaphore_wait_to_signaller_ring(struct intel_engine_cs *ring, u32 ipehr)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	struct intel_engine_cs *signaller;
-	int i;
-
-	if (INTEL_INFO(dev_priv->dev)->gen >= 8) {
-		/*
-		 * FIXME: gen8 semaphore support - currently we don't emit
-		 * semaphores on bdw anyway, but this needs to be addressed when
-		 * we merge that code.
-		 */
-		return NULL;
-	} else {
-		u32 sync_bits = ipehr & MI_SEMAPHORE_SYNC_MASK;
-
-		for_each_ring(signaller, dev_priv, i) {
-			if(ring == signaller)
-				continue;
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 tmp = I915_READ_CTL(ring);
 
-			if (sync_bits == signaller->semaphore.mbox.wait[ring->id])
-				return signaller;
-		}
+	if (tmp & RING_WAIT) {
+		DRM_ERROR("Kicking stuck wait on %s\n",
+			ring->name);
+		I915_WRITE_CTL(ring, tmp);
+		ring->hangcheck.action = HANGCHECK_KICK;
+		return true;
 	}
-
-	DRM_ERROR("No signaller ring found for ring %i, ipehr 0x%08x\n",
-		  ring->id, ipehr);
-
-	return NULL;
+	return false;
 }
 
-static struct intel_engine_cs *
-semaphore_waits_for(struct intel_engine_cs *ring, u32 *seqno)
+/**
+ * This function is called when the TDR algorithm detects that the
+ * hardware has not advanced during the last sampling period
+ */
+static bool i915_hangcheck_hung(struct intel_ring_hangcheck *hc)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	u32 cmd, ipehr, head;
-	int i;
+	struct drm_device *dev = hc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t mbox_wait;
+	uint32_t threshold;
+	struct intel_engine_cs *ring;
 
-	ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
-	if (!ipehr_is_semaphore_wait(ring->dev, ipehr))
-		return NULL;
+	DRM_DEBUG_TDR("Ring [%d] hc->count = %d\n", hc->ringid, hc->count);
+	ring = &dev_priv->ring[hc->ringid];
 
 	/*
-	 * HEAD is likely pointing to the dword after the actual command,
-	 * so scan backwards until we find the MBOX. But limit it to just 3
-	 * dwords. Note that we don't care about ACTHD here since that might
-	 * point at at batch, and semaphores are always emitted into the
-	 * ringbuffer itself.
+	 * Is this ring waiting on a semaphore mbox?
+	 * If so, give it a bit longer as it may be waiting on another
+	 * ring which has actually hung. Give the other ring chance to
+	 * reset and clear the hang.
 	 */
-	head = I915_READ_HEAD(ring) & HEAD_ADDR;
-
-	for (i = 4; i; --i) {
-		/*
-		 * Be paranoid and presume the hw has gone off into the wild -
-		 * our ring is smaller than what the hardware (and hence
-		 * HEAD_ADDR) allows. Also handles wrap-around.
-		 */
-		head &= ring->buffer->size - 1;
-
-		/* This here seems to blow up */
-		cmd = ioread32(ring->buffer->virtual_start + head);
-		if (cmd == ipehr)
-			break;
+	mbox_wait = ((I915_READ(RING_CTL(ring->mmio_base)) >> 10) & 0x1);
+	threshold = mbox_wait ? DRM_I915_MBOX_HANGCHECK_THRESHOLD :
+				DRM_I915_HANGCHECK_THRESHOLD;
+	DRM_DEBUG_TDR("mbox_wait = %u threshold = %u", mbox_wait, threshold);
+
+	if (hc->count++ > threshold) {
+		bool hung = true;
+
+		/* Reset the counter */
+		hc->count = 0;
+
+		i915_sync_hung_ring(ring);
+
+		if (!IS_GEN2(dev)) {
+			/* If the ring is hanging on a WAIT_FOR_EVENT
+			* then simply poke the RB_WAIT bit
+			* and break the hang. This should work on
+			* all but the second generation chipsets.
+			*/
+			ring = &dev_priv->ring[hc->ringid];
+			hung &= !kick_ring(ring);
+			DRM_DEBUG_TDR("hung=%d after kick ring\n", hung);
+		}
 
-		head -= 4;
+		if (hung) {
+			hc->tdr_count++;
+			ring->hangcheck.action = HANGCHECK_HUNG;
+			i915_handle_error(dev, hc, "%s hung", ring->name);
+		}
+		return hung;
 	}
 
-	if (!i)
-		return NULL;
-
-	*seqno = ioread32(ring->buffer->virtual_start + head + 4) + 1;
-	return semaphore_wait_to_signaller_ring(ring, ipehr);
+	return false;
 }
 
-static int semaphore_passed(struct intel_engine_cs *ring)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	struct intel_engine_cs *signaller;
-	u32 seqno, ctl;
-
-	ring->hangcheck.deadlock++;
-
-	signaller = semaphore_waits_for(ring, &seqno);
-	if (signaller == NULL)
-		return -1;
-
-	/* Prevent pathological recursion due to driver bugs */
-	if (signaller->hangcheck.deadlock >= I915_NUM_RINGS)
-		return -1;
-
-	/* cursory check for an unkickable deadlock */
-	ctl = I915_READ_CTL(signaller);
-	if (ctl & RING_WAIT_SEMAPHORE && semaphore_passed(signaller) < 0)
-		return -1;
-
-	if (i915_seqno_passed(signaller->get_seqno(signaller, false), seqno))
-		return 1;
-
-	if (signaller->hangcheck.deadlock)
-		return -1;
-
-	return 0;
-}
-
-static void semaphore_clear_deadlocks(struct drm_i915_private *dev_priv)
-{
+/*
+ * This is called from the hangcheck timer for each ring.
+ * It samples the current state of the hardware to make
+ * sure that it is progressing.
+ */
+void i915_hangcheck_sample(unsigned long data)
+{
+	bool idle;
+	int empty;
+	int instdone_cmp;
+	int pending_work = 1;
+	int resched_timer = 1;
+	uint32_t cur_seqno = 0;
+	uint32_t last_seqno = 0;
+	uint32_t head, tail, acthd, instdone[I915_NUM_INSTDONE_REG];
+	struct drm_device *dev;
+	struct drm_i915_private *dev_priv;
 	struct intel_engine_cs *ring;
-	int i;
+	struct intel_ring_hangcheck *hc = (struct intel_ring_hangcheck *)data;
 
-	for_each_ring(ring, dev_priv, i)
-		ring->hangcheck.deadlock = 0;
-}
+	if (!i915.enable_hangcheck || !hc)
+		return;
 
-static enum intel_ring_hangcheck_action
-ring_stuck(struct intel_engine_cs *ring, u64 acthd)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 tmp;
+	dev = hc->dev;
+	dev_priv = dev->dev_private;
 
-	if (ring->hangcheck.acthd != acthd)
-		return HANGCHECK_ACTIVE;
+	/* Clear the active flag *before* assessing the ring state
+	* in case new work is added just after we sample the rings.
+	* This will allow new work to re-trigger the timer even
+	* though we may see the rings as idle on this occasion.*/
+	atomic_set(&hc->active, 0);
 
-	if (IS_GEN2(dev))
-		return HANGCHECK_HUNG;
+	ring = &dev_priv->ring[hc->ringid];
 
-	/* Is the chip hanging on a WAIT_FOR_EVENT?
-	 * If so we can simply poke the RB_WAIT bit
-	 * and break the hang. This should work on
-	 * all but the second generation chipsets.
-	 */
-	tmp = I915_READ_CTL(ring);
-	if (tmp & RING_WAIT) {
-		i915_handle_error(dev, false,
-				  "Kicking stuck wait on %s",
-				  ring->name);
-		I915_WRITE_CTL(ring, tmp);
-		return HANGCHECK_KICK;
+	/* Sample the current state */
+	head = I915_READ_HEAD(ring) & HEAD_ADDR;
+	tail = I915_READ_TAIL(ring) & TAIL_ADDR;
+	acthd = intel_ring_get_active_head(ring);
+	empty = list_empty(&ring->request_list);
+
+	i915_get_extra_instdone(dev, instdone, ring);
+	instdone_cmp = (memcmp(hc->prev_instdone,
+		instdone, sizeof(instdone)) == 0) ? 1 : 0;
+
+	if (!empty) {
+		/* Examine the seqno's to see where the HW has got to
+		* (Only call ring_last_seqno when the list is non-empty)*/
+		cur_seqno = ring->get_seqno(ring, false);
+		last_seqno = ring_last_seqno(ring);
 	}
 
-	if (INTEL_INFO(dev)->gen >= 6 && tmp & RING_WAIT_SEMAPHORE) {
-		switch (semaphore_passed(ring)) {
-		default:
-			return HANGCHECK_HUNG;
-		case 1:
-			i915_handle_error(dev, false,
-					  "Kicking stuck semaphore on %s",
-					  ring->name);
-			I915_WRITE_CTL(ring, tmp);
-			return HANGCHECK_KICK;
-		case 0:
-			return HANGCHECK_WAIT;
-		}
+	if (empty || i915_seqno_passed(cur_seqno, last_seqno)) {
+		/* If the request list is empty or the HW has passed the
+		* last seqno of the last item in the request list then the
+		* HW is considered idle.
+		* The driver may not have cleaned up the request list yet */
+		pending_work = 0;
 	}
 
-	return HANGCHECK_HUNG;
-}
-
-/**
- * This is called when the chip hasn't reported back with completed
- * batchbuffers in a long time. We keep track per ring seqno progress and
- * if there are no progress, hangcheck score for that ring is increased.
- * Further, acthd is inspected to see if the ring is stuck. On stuck case
- * we kick the ring. If we see no progress on three subsequent calls
- * we assume chip is wedged and try to fix it by resetting the chip.
- */
-static void i915_hangcheck_elapsed(unsigned long data)
-{
-	struct drm_device *dev = (struct drm_device *)data;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int i;
-	int busy_count = 0, rings_hung = 0;
-	bool stuck[I915_NUM_RINGS] = { 0 };
-#define BUSY 1
-#define KICK 5
-#define HUNG 20
-
-	if (!i915.enable_hangcheck)
-		return;
-
-	for_each_ring(ring, dev_priv, i) {
-		u64 acthd;
-		u32 seqno;
-		bool busy = true;
-
-		semaphore_clear_deadlocks(dev_priv);
-
-		seqno = ring->get_seqno(ring, false);
-		acthd = intel_ring_get_active_head(ring);
-
-		if (ring->hangcheck.seqno == seqno) {
-			if (ring_idle(ring, seqno)) {
-				ring->hangcheck.action = HANGCHECK_IDLE;
-
-				if (waitqueue_active(&ring->irq_queue)) {
-					/* Issue a wake-up to catch stuck h/w. */
-					if (!test_and_set_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings)) {
-						if (!(dev_priv->gpu_error.test_irq_rings & intel_ring_flag(ring)))
-							DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
-								  ring->name);
-						else
-							DRM_INFO("Fake missed irq on %s\n",
-								 ring->name);
-						wake_up_all(&ring->irq_queue);
-					}
-					/* Safeguard against driver failure */
-					ring->hangcheck.score += BUSY;
-				} else
-					busy = false;
+	idle = ((head == tail) && (pending_work == 0));
+
+	DRM_DEBUG_TDR("[%d] HD: 0x%08x 0x%08x, ACTHD: 0x%08x 0x%08x IC: %d\n",
+		ring->id, head, hc->last_hd, acthd, hc->last_acthd,
+		instdone_cmp);
+	DRM_DEBUG_TDR("E:%d PW:%d TL:0x%08x Csq:0x%08x Lsq:0x%08x Idle: %d\n",
+		empty, pending_work, tail, cur_seqno, last_seqno, idle);
+
+	/* Check both head and active head.
+	* Neither is enough on its own - acthd can be pointing within the
+	* batch buffer so is more likely to be moving, but the same
+	* underlying buffer object could be submitted more than once.
+	* If it happens to pause at exactly the same place in the batch
+	* buffer and we sample it at that moment then we could see it as
+	* hung over 3 sample periods that do not belong to the same
+	* batch submission - this would result in a false positive.
+	* We know that the head pointer will have advanced for each
+	* batch buffer as the ring has to contain a new MI_BATCH_BUFFER_START
+	* for every do_exec call, so by combining head and active head we can
+	* ensure that the hang detection distinguishes between batch buffers*/
+	if ((hc->last_acthd == acthd)
+	    && (hc->last_hd == head)
+	    && instdone_cmp) {
+		/* Ring hasn't advanced in this sampling period */
+		if (idle) {
+			ring->hangcheck.action = HANGCHECK_IDLE;
+
+			/* The hardware is idle */
+			if (waitqueue_active(&ring->irq_queue)) {
+				/* We expect the wait queue to drain
+				* if the hardware has remained idle
+				* for 3 consecutive samples. Wake up
+				* the queue on each sample to try and
+				* release it, but if it persists then
+				* trigger a reset */
+				DRM_DEBUG_TDR("Possible stuck wait (0x%08x)\n",
+					ring->last_irq_seqno);
+				wake_up_all(&ring->irq_queue);
+				i915_hangcheck_hung(hc);
 			} else {
-				/* We always increment the hangcheck score
-				 * if the ring is busy and still processing
-				 * the same request, so that no single request
-				 * can run indefinitely (such as a chain of
-				 * batches). The only time we do not increment
-				 * the hangcheck score on this ring, if this
-				 * ring is in a legitimate wait for another
-				 * ring. In that case the waiting ring is a
-				 * victim and we want to be sure we catch the
-				 * right culprit. Then every time we do kick
-				 * the ring, add a small increment to the
-				 * score so that we can catch a batch that is
-				 * being repeatedly kicked and so responsible
-				 * for stalling the machine.
-				 */
-				ring->hangcheck.action = ring_stuck(ring,
-								    acthd);
-
-				switch (ring->hangcheck.action) {
-				case HANGCHECK_IDLE:
-				case HANGCHECK_WAIT:
-					break;
-				case HANGCHECK_ACTIVE:
-					ring->hangcheck.score += BUSY;
-					break;
-				case HANGCHECK_KICK:
-					ring->hangcheck.score += KICK;
-					break;
-				case HANGCHECK_HUNG:
-					ring->hangcheck.score += HUNG;
-					stuck[i] = true;
-					break;
-				}
+				/* Hardware and driver both idle */
+				hc->count = 0;
+				resched_timer = 0;
 			}
 		} else {
-			ring->hangcheck.action = HANGCHECK_ACTIVE;
-
-			/* Gradually reduce the count so that we catch DoS
-			 * attempts across multiple batches.
+			/*
+			 * The hardware is busy but has not advanced
+			 * since the last sample - possible hang
 			 */
-			if (ring->hangcheck.score > 0)
-				ring->hangcheck.score--;
-		}
-
-		ring->hangcheck.seqno = seqno;
-		ring->hangcheck.acthd = acthd;
-		busy_count += busy;
-	}
-
-	for_each_ring(ring, dev_priv, i) {
-		if (ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG) {
-			DRM_INFO("%s on %s\n",
-				 stuck[i] ? "stuck" : "no progress",
-				 ring->name);
-			rings_hung++;
+			i915_hangcheck_hung(hc);
 		}
+	} else {
+		/* The state has changed so the hardware is active */
+		hc->count = 0;
+		ring->hangcheck.action = HANGCHECK_ACTIVE;
 	}
 
-	if (rings_hung)
-		return i915_handle_error(dev, true, "Ring hung");
+	/* Always update last sampled state */
+	hc->last_hd = head;
+	hc->last_acthd = acthd;
+	memcpy(hc->prev_instdone, instdone, sizeof(instdone));
 
-	if (busy_count)
-		/* Reset timer case chip hangs without another request
-		 * being added */
-		i915_queue_hangcheck(dev);
+	if (resched_timer)
+		i915_queue_hangcheck(dev, hc->ringid);
 }
 
-void i915_queue_hangcheck(struct drm_device *dev)
+void i915_queue_hangcheck(struct drm_device *dev, u32 ringid)
 {
+	int resched = 0;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+
 	if (!i915.enable_hangcheck)
 		return;
 
-	mod_timer(&dev_priv->gpu_error.hangcheck_timer,
-		  round_jiffies_up(jiffies + DRM_I915_HANGCHECK_JIFFIES));
+	/* Only re-schedule the timer if it is not currently active */
+	if (atomic_add_unless(&dev_priv->ring[ringid].hangcheck.active,
+			      1, 1) != 0)
+		resched = 1;
+
+	if (resched)
+		mod_timer(&dev_priv->ring[ringid].hangcheck.timer,
+			  jiffies + DRM_I915_HANGCHECK_JIFFIES);
 }
 
 static void ibx_irq_reset(struct drm_device *dev)
@@ -4165,7 +4173,7 @@ static irqreturn_t i8xx_irq_handler(int irq, void *arg)
 		 */
 		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, NULL,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
@@ -4349,7 +4357,7 @@ static irqreturn_t i915_irq_handler(int irq, void *arg)
 		 */
 		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, NULL,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
@@ -4581,7 +4589,7 @@ static irqreturn_t i965_irq_handler(int irq, void *arg)
 		 */
 		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, NULL,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
@@ -4733,9 +4741,6 @@ void intel_irq_init(struct drm_device *dev)
 	/* Let's track the enabled rps events */
 	dev_priv->pm_rps_events = GEN6_PM_RPS_EVENTS;
 
-	setup_timer(&dev_priv->gpu_error.hangcheck_timer,
-		    i915_hangcheck_elapsed,
-		    (unsigned long) dev);
 	setup_timer(&dev_priv->hotplug_reenable_timer, intel_hpd_irq_reenable,
 		    (unsigned long) dev_priv);
 
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 65f53d8..c6dfcb6 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -53,6 +53,9 @@ struct i915_params i915 __read_mostly = {
 	.disable_vtd_wa = 0,
 	.drrs_interval = 2000,
 	.use_mmio_flip = 0,
+	.hangcheck_period = 1000,
+	.ring_reset_min_alive_period = 0,
+	.gpu_reset_min_alive_period = 0,
 };
 module_param_named(limitbw, i915.limitbw, int, 0400);
 MODULE_PARM_DESC(limitbw,
@@ -185,3 +188,51 @@ MODULE_PARM_DESC(drrs_interval,
 module_param_named(use_mmio_flip, i915.use_mmio_flip, int, 0600);
 MODULE_PARM_DESC(use_mmio_flip,
 		 "use MMIO flips (-1=never, 0=driver discretion [default], 1=always)");
+
+int hangcheck_period_set(const char *val, const struct kernel_param *kp)
+{
+	/* Custom set function so we can validate the range */
+	unsigned long num;
+	int ret;
+
+	ret = kstrtoul(val, 0, &num);
+	if (ret)
+		return ret;
+
+	/* Enforce minimum delay in ms */
+	if ((num >= DRM_I915_MIN_HANGCHECK_PERIOD)
+	    && (num <= DRM_I915_MAX_HANGCHECK_PERIOD)) {
+		i915.hangcheck_period = num;
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static const struct kernel_param_ops hangcheck_ops = {
+	.set = hangcheck_period_set,
+	.get = param_get_uint,
+};
+
+module_param_cb(hangcheck_period, &hangcheck_ops,
+		&i915.hangcheck_period, 0644);
+MODULE_PARM_DESC(hangcheck_period,
+		"The hangcheck timer period in milliseconds. "
+		"The actual time to detect a hang may be 3 - 4 times "
+		"this value (default = 1000ms)");
+
+module_param_named(ring_reset_min_alive_period,
+		i915.ring_reset_min_alive_period, int, 0644);
+MODULE_PARM_DESC(ring_reset_min_alive_period,
+		"Catch excessive ring resets. Each ring maintains a timestamp of "
+		"the last time it was reset. If it hangs again within this period "
+		"then switch to full GPU reset to try and clear the hang."
+		"default=0 seconds (disabled)");
+
+module_param_named(gpu_reset_min_alive_period,
+		i915.gpu_reset_min_alive_period, int, 0644);
+MODULE_PARM_DESC(gpu_reset_min_alive_period,
+		"Catch excessive GPU resets. If the GPU hangs again within this period "
+		"following the previous GPU reset then declare it wedged and "
+		"prevent further resets. "
+		"default=0 seconds (disabled)");
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 06820d6..9ea5500 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -34,6 +34,7 @@
 
 #define _MASKED_BIT_ENABLE(a) (((a) << 16) | (a))
 #define _MASKED_BIT_DISABLE(a) ((a) << 16)
+#define _MASKED_BIT_ENABLE_ALL(b) (0xFFFF0000 | (b))
 
 /* PCI config space */
 
@@ -115,6 +116,7 @@
 #define  GEN6_GRDOM_RENDER		(1 << 1)
 #define  GEN6_GRDOM_MEDIA		(1 << 2)
 #define  GEN6_GRDOM_BLT			(1 << 3)
+#define  GEN6_GRDOM_VECS		(1 << 4)
 
 #define GEN8_SRID_0_2_0_PCI		0xf8
 
@@ -1088,6 +1090,10 @@ enum punit_power_well {
 #define RING_HEAD(base)		((base)+0x34)
 #define RING_START(base)	((base)+0x38)
 #define RING_CTL(base)		((base)+0x3c)
+#define RING_MI_MODE(base)	((base)+0x9c)
+#define RING_MODE_STOP		(1 << 8)
+#define RING_MODE_IDLE		(1 << 9)
+#define RING_UHPTR(base) ((base)+0x134)
 #define RING_SYNC_0(base)	((base)+0x40)
 #define RING_SYNC_1(base)	((base)+0x44)
 #define RING_SYNC_2(base)	((base)+0x48)
@@ -1157,6 +1163,9 @@ enum punit_power_well {
 #define   RING_WAIT_I8XX	(1<<0) /* gen2, PRBx_HEAD */
 #define   RING_WAIT		(1<<11) /* gen3+, PRBx_CTL */
 #define   RING_WAIT_SEMAPHORE	(1<<10) /* gen6+ */
+#define RS_CHICKEN(base)	((base)+0xdc)
+#define WAIT_FOR_RC6_EXIT(base) ((base)+0xe4)
+#define FF_SLICE_CS_CHICKEN2(base) ((base)+0xcc)
 
 #define GEN7_TLB_RD_ADDR	0x4700
 
@@ -1277,6 +1286,7 @@ enum punit_power_well {
 #define GFX_MODE	0x02520
 #define GFX_MODE_GEN7	0x0229c
 #define RING_MODE_GEN7(ring)	((ring)->mmio_base+0x29c)
+#define RING_EXCC_GEN7(ring)    ((ring)->mmio_base+0x028)
 #define   GFX_RUN_LIST_ENABLE		(1<<15)
 #define   GFX_TLB_INVALIDATE_EXPLICIT	(1<<13)
 #define   GFX_SURFACE_FAULT_ENABLE	(1<<12)
diff --git a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
index af29ae4..6a31bf2 100644
--- a/drivers/gpu/drm/i915/intel_display.c
+++ b/drivers/gpu/drm/i915/intel_display.c
@@ -2986,6 +2986,14 @@ void intel_display_handle_reset(struct drm_device *dev)
 	 * Need to make two loops over the crtcs so that we
 	 * don't try to grab a crtc mutex before the
 	 * pending_flip_queue really got woken up.
+	 *
+	 * For MMIO based page flips it is also possible that
+	 * the GPU could be reset between requesting the page
+	 * flip and before it reaches the next vblank when it
+	 * would normally send the page flip interrupt.
+	 * In that case we would be left with unpin work that
+	 * will not get cleaned up so we must cleanup any
+	 * pending page flips after a global reset
 	 */
 
 	for_each_crtc(dev, crtc) {
@@ -9840,7 +9848,7 @@ static int intel_gen7_queue_flip(struct drm_device *dev,
 		return -ENODEV;
 	}
 
-	len = 4;
+	len = 12;
 	if (ring->id == RCS) {
 		len += 6;
 		/*
@@ -9899,11 +9907,34 @@ static int intel_gen7_queue_flip(struct drm_device *dev,
 		}
 	}
 
+	/* Set a flag to indicate that a page flip interrupt is expected.
+	* The flag is used by the TDR logic to detect whether the blitter hung
+	* on a page flip command, in which case it will need to manually
+	* complete the page flip.
+	* The 'flag' is actually the pipe value associated with this page
+	* flip + 1 so that the TDR code knows which pipe failed to flip.
+	* A value of 0 indicates that a flip is not currently in progress on
+	* the HW.*/
+	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
+	intel_ring_emit(ring, (I915_GEM_PGFLIP_INDEX <<
+			       MI_STORE_DWORD_INDEX_SHIFT));
+	intel_ring_emit(ring, intel_crtc->pipe + 1);
+	intel_ring_emit(ring, MI_NOOP);
+
 	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 | plane_bit);
 	intel_ring_emit(ring, (fb->pitches[0] | obj->tiling_mode));
 	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
 	intel_ring_emit(ring, (MI_NOOP));
 
+	/* Clear the flag as soon as we pass over the page flip command.
+	* If we passed over the command without hanging then an interrupt should
+	* be received to complete the page flip.*/
+	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
+	intel_ring_emit(ring, (I915_GEM_PGFLIP_INDEX <<
+			       MI_STORE_DWORD_INDEX_SHIFT));
+	intel_ring_emit(ring, 0);
+	intel_ring_emit(ring, MI_NOOP);
+
 	intel_mark_page_flip_active(intel_crtc);
 	__intel_ring_advance(ring);
 
@@ -9966,6 +9997,7 @@ static void intel_do_mmio_flip(struct intel_crtc *intel_crtc)
 
 static int intel_postpone_flip(struct drm_i915_gem_object *obj)
 {
+	struct drm_i915_private *dev_priv;
 	struct intel_engine_cs *ring;
 	int ret;
 
@@ -9976,6 +10008,12 @@ static int intel_postpone_flip(struct drm_i915_gem_object *obj)
 
 	ring = obj->ring;
 
+	dev_priv = ring->dev;
+	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
+				   dev_priv->mm.interruptible, obj->ring);
+	if (ret)
+		return ret;
+
 	if (i915_seqno_passed(ring->get_seqno(ring, true),
 			      obj->last_write_seqno))
 		return 0;
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 201b00a..362e15f 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -876,9 +876,17 @@ void intel_logical_ring_advance_and_submit(struct intel_ringbuffer *ringbuf)
 {
 	struct intel_engine_cs *ring = ringbuf->ring;
 	struct intel_context *ctx = ringbuf->FIXME_lrc_ctx;
+	struct drm_i915_private *dev_priv = ring->dev->dev_private;
 
 	intel_logical_ring_advance(ringbuf);
 
+	/* Re-schedule the hangcheck timer each time the ring is given new work
+	* so that we can detect hangs caused by commands inserted directly
+	* to the ring as well as bad batch buffers */
+	if (!dev_priv->ums.mm_suspended &&
+	    dev_priv->ring[RCS].default_context->rcs_initialized)
+		i915_queue_hangcheck(ring->dev, ring->id);
+
 	if (intel_ring_stopped(ring))
 		return;
 
@@ -964,7 +972,7 @@ static int logical_ring_wait_for_space(struct intel_ringbuffer *ringbuf,
 		}
 
 		ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-					   dev_priv->mm.interruptible);
+					   dev_priv->mm.interruptible, ring);
 		if (ret)
 			break;
 
@@ -1039,7 +1047,7 @@ int intel_logical_ring_begin(struct intel_ringbuffer *ringbuf, int num_dwords)
 	int ret;
 
 	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-				   dev_priv->mm.interruptible);
+				   dev_priv->mm.interruptible, ring);
 	if (ret)
 		return ret;
 
@@ -1070,8 +1078,6 @@ static int gen8_init_common_ring(struct intel_engine_cs *ring)
 	POSTING_READ(RING_MODE_GEN7(ring));
 	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", ring->name);
 
-	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
-
 	return 0;
 }
 
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 688a771..eabb5f6 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -73,8 +73,18 @@ bool intel_ring_stopped(struct intel_engine_cs *ring)
 
 void __intel_ring_advance(struct intel_engine_cs *ring)
 {
+	struct drm_i915_private *dev_priv = ring->dev->dev_private;
 	struct intel_ringbuffer *ringbuf = ring->buffer;
+
 	ringbuf->tail &= ringbuf->size - 1;
+
+	/* Re-schedule the hangcheck timer each time the ring is given new work
+	* so that we can detect hangs caused by commands inserted directly
+	* to the ring as well as bad batch buffers */
+	if (!dev_priv->ums.mm_suspended &&
+	    dev_priv->ring[RCS].default_context->legacy_hw_ctx.initialized)
+		i915_queue_hangcheck(ring->dev, ring->id);
+
 	if (intel_ring_stopped(ring))
 		return;
 	ring->write_tail(ring, ringbuf->tail);
@@ -439,6 +449,420 @@ static void ring_write_tail(struct intel_engine_cs *ring,
 	I915_WRITE_TAIL(ring, value);
 }
 
+int intel_ring_disable(struct intel_engine_cs *ring)
+{
+	if (ring && ring->disable)
+		return ring->disable(ring);
+	else {
+		DRM_ERROR("ring disable not supported\n");
+		return -EINVAL;
+	}
+}
+
+static int
+gen6_ring_disable(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t ring_ctl;
+	uint32_t mi_mode;
+	uint32_t retries = 10000;
+
+	/* Request the ring to go idle */
+	I915_WRITE_MODE(ring, _MASKED_BIT_ENABLE(RING_MODE_STOP));
+
+	/* Wait for idle */
+	do {
+		mi_mode = I915_READ_MODE(ring);
+	} while (retries-- && !(mi_mode & RING_MODE_IDLE));
+
+	if (retries == 0) {
+		DRM_ERROR("timed out trying to disable ring %d\n", ring->id);
+		return -ETIMEDOUT;
+	}
+
+	/* Disable the ring */
+	ring_ctl = I915_READ_CTL(ring);
+	ring_ctl &= (RING_NR_PAGES | RING_REPORT_MASK);
+	I915_WRITE_CTL(ring, ring_ctl);
+	ring_ctl = I915_READ_CTL(ring); /* Barrier read */
+
+	return ((ring_ctl & RING_VALID) == 0) ? 0 : -EIO;
+}
+
+int intel_ring_enable(struct intel_engine_cs *ring)
+{
+	if (ring && ring->enable)
+		return ring->enable(ring);
+	else {
+		DRM_ERROR("ring enable not supported\n");
+		return -EINVAL;
+	}
+}
+
+static int
+gen6_ring_enable(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t ring_ctl;
+	uint32_t mode;
+
+	/* Clear the MI_MODE stop bit */
+	I915_WRITE_MODE(ring, _MASKED_BIT_DISABLE(RING_MODE_STOP));
+	mode = I915_READ_MODE(ring); /* Barrier read */
+
+	/* Enable the ring */
+	ring_ctl = I915_READ_CTL(ring);
+	ring_ctl &= (RING_NR_PAGES | RING_REPORT_MASK);
+	I915_WRITE_CTL(ring, ring_ctl | RING_VALID);
+	ring_ctl = I915_READ_CTL(ring); /* Barrier read */
+
+	return ((ring_ctl & RING_VALID) == 0) ? -EIO : 0;
+}
+
+int intel_ring_save(struct intel_engine_cs *ring, u32 flags)
+{
+	if (ring && ring->save)
+		return ring->save(ring, ring->saved_state,
+			I915_RING_CONTEXT_SIZE, flags);
+	else {
+		DRM_ERROR("ring save not supported\n");
+		return -EINVAL;
+	}
+}
+
+static int
+gen6_ring_save(struct intel_engine_cs *ring, uint32_t *data, uint32_t max,
+		u32 flags)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_ringbuffer *ringbuf = ring->buffer;
+	uint32_t idx = 0;
+	uint32_t gen = INTEL_INFO(dev)->gen;
+	uint32_t head;
+	uint32_t tail;
+	uint32_t head_addr;
+	uint32_t tail_addr;
+	int clamp_to_tail = 0;
+
+	/* Ring save only added for gen >= 7 */
+	WARN_ON(gen < 7);
+
+	/* Save common registers */
+	if (max < COMMON_RING_CTX_SIZE)
+		return -EINVAL;
+
+	head = I915_READ_HEAD(ring);
+	tail = I915_READ_TAIL(ring);
+
+	head_addr = head & HEAD_ADDR;
+	tail_addr = tail & TAIL_ADDR;
+
+	if (flags & FORCE_ADVANCE) {
+		/* The head must always chase the tail.
+		* If the tail is beyond the head then do not allow
+		* the head to overtake it. If the tail is less than
+		* the head then the tail has already wrapped and
+		* there is no problem in advancing the head or even
+		* wrapping the head back to 0 as worst case it will
+		* become equal to tail */
+		if (head_addr <= tail_addr)
+			clamp_to_tail = 1;
+
+		/* Force head to next QWORD boundary */
+		head_addr &= ~0x7;
+		head_addr += 8;
+
+		if (clamp_to_tail && (head_addr > tail_addr)) {
+			head_addr = tail_addr;
+		} else if (head_addr >= ringbuf->size) {
+			/* Wrap head back to start if it exceeds ring size*/
+			head_addr = 0;
+		}
+
+		/* Update the register */
+		head &= ~HEAD_ADDR;
+		head |= (head_addr & HEAD_ADDR);
+
+		DRM_DEBUG_TDR("Forced head to 0x%08x\n", head);
+	} else if (head & 0x7) {
+		/* Ensure head pointer is pointing to a QWORD boundary */
+		DRM_DEBUG_TDR("Rounding up head 0x%08x\n", head);
+		head += 0x7;
+		head &= ~0x7;
+	}
+
+	/* Saved with enable = 0 */
+	data[idx++] = I915_READ_CTL(ring) & (RING_NR_PAGES | RING_REPORT_MASK);
+
+	data[idx++] = (flags & RESET_HEAD_TAIL) ? 0 : tail;
+
+	if (flags & RESET_HEAD_TAIL) {
+		/* Save head as 0 so head is reset on restore */
+		data[idx++] = 0;
+	} else {
+		/* Head will already have advanced to next instruction location
+		* even if the current instruction caused a hang, so we just
+		* save the current value as the value to restart at */
+		data[idx++] = head;
+	}
+
+	data[idx++] = I915_READ_START(ring);
+
+	/* Workaround for reading DCLV registers for gen < 8 */
+	data[idx++] = (gen < 8) ?
+		I915_READ(RING_PP_DIR_DCLV(&dev_priv->ring[VCS]))
+		: I915_READ(RING_PP_DIR_DCLV(ring));
+
+	data[idx++] = (gen < 8) ?
+		I915_READ(RING_PP_DIR_BASE(&dev_priv->ring[VCS]))
+		: I915_READ(RING_PP_DIR_BASE(ring));
+
+	switch (ring->id) {
+	case RCS:
+		if (max < (COMMON_RING_CTX_SIZE + RCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		data[idx++] = I915_READ(RENDER_HWS_PGA_GEN7);
+		data[idx++] = I915_READ(RING_UHPTR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_INSTPM(ring->mmio_base));
+		data[idx++] = I915_READ(RING_IMR(ring->mmio_base));
+		data[idx++] = I915_READ(CACHE_MODE_1);
+		data[idx++] = I915_READ(RING_MI_MODE(ring->mmio_base));
+		data[idx++] = I915_READ(_3D_CHICKEN3);
+		data[idx++] = I915_READ(GAM_ECOCHK);
+		data[idx++] = I915_READ(GFX_MODE_GEN7);
+		data[idx++] = I915_READ(GEN6_RBSYNC);
+		data[idx++] = I915_READ(GEN7_FF_THREAD_MODE);
+		data[idx++] = I915_READ(RS_CHICKEN(ring->mmio_base));
+		data[idx++] = I915_READ(WAIT_FOR_RC6_EXIT(ring->mmio_base));
+		data[idx++] = I915_READ(FF_SLICE_CS_CHICKEN2(ring->mmio_base));
+		break;
+
+	case VCS:
+		if (max < (COMMON_RING_CTX_SIZE + VCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		data[idx++] = I915_READ(BSD_HWS_PGA_GEN7);
+		data[idx++] = I915_READ(RING_MI_MODE(ring->mmio_base));
+		data[idx++] = I915_READ(RING_IMR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_UHPTR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_INSTPM(ring->mmio_base));
+		data[idx++] = I915_READ(RING_EXCC_GEN7(ring));
+		data[idx++] = I915_READ(GAC_ECO_BITS);
+		data[idx++] = I915_READ(RING_MODE_GEN7(ring));
+		data[idx++] = I915_READ(GEN6_VRSYNC);
+		data[idx++] = I915_READ(RING_MAX_IDLE(ring->mmio_base));
+		break;
+
+	case BCS:
+		if (max < (COMMON_RING_CTX_SIZE + BCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		data[idx++] = I915_READ(BLT_HWS_PGA_GEN7);
+		data[idx++] = I915_READ(RING_MI_MODE(ring->mmio_base));
+		data[idx++] = I915_READ(RING_IMR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_UHPTR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_INSTPM(ring->mmio_base));
+		data[idx++] = I915_READ(RING_EXCC_GEN7(ring));
+		data[idx++] = I915_READ(GAB_CTL);
+		data[idx++] = I915_READ(RING_MODE_GEN7(ring));
+		data[idx++] = I915_READ(GEN6_BRSYNC);
+		data[idx++] = I915_READ(GEN6_BVSYNC);
+		data[idx++] = I915_READ(RING_MAX_IDLE(ring->mmio_base));
+		break;
+
+	case VECS:
+		if (max < (COMMON_RING_CTX_SIZE + VECS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		data[idx++] = I915_READ(VEBOX_HWS_PGA_GEN7);
+		data[idx++] = I915_READ(RING_MI_MODE(ring->mmio_base));
+		data[idx++] = I915_READ(RING_IMR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_UHPTR(ring->mmio_base));
+		data[idx++] = I915_READ(RING_INSTPM(ring->mmio_base));
+		data[idx++] = I915_READ(RING_EXCC_GEN7(ring));
+		data[idx++] = I915_READ(RING_MODE_GEN7(ring));
+		data[idx++] = I915_READ(GEN6_VEVSYNC);
+		break;
+
+	default:
+		DRM_ERROR("Invalid ring ID %d\n", ring->id);
+		break;
+	}
+
+	return 0;
+}
+
+int intel_ring_restore(struct intel_engine_cs *ring)
+{
+	if (ring && ring->restore)
+		return ring->restore(ring, ring->saved_state,
+			I915_RING_CONTEXT_SIZE);
+	else {
+		DRM_ERROR("ring restore not supported\n");
+		return -EINVAL;
+	}
+}
+
+static int
+gen6_ring_restore(struct intel_engine_cs *ring, uint32_t *data,
+			uint32_t max)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t idx = 0;
+	uint32_t x;
+
+	/* NOTE: Registers are restored in reverse order from when
+	* they were saved. */
+	switch (ring->id) {
+	case RCS:
+		if (max < (COMMON_RING_CTX_SIZE + RCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		idx = COMMON_RING_CTX_SIZE + RCS_RING_CTX_SIZE - 1;
+
+		I915_WRITE(FF_SLICE_CS_CHICKEN2(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(WAIT_FOR_RC6_EXIT(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RS_CHICKEN(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(GEN7_FF_THREAD_MODE, data[idx--]);
+		I915_WRITE(GEN6_RBSYNC, data[idx--]);
+		I915_WRITE(RING_MODE_GEN7(ring),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(GAM_ECOCHK, data[idx--]);
+		I915_WRITE(_3D_CHICKEN3,
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_MI_MODE(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(CACHE_MODE_1,
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_IMR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_INSTPM(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_UHPTR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RENDER_HWS_PGA_GEN7, data[idx--]);
+		break;
+
+	case VCS:
+		if (max < (COMMON_RING_CTX_SIZE + VCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		idx = COMMON_RING_CTX_SIZE + VCS_RING_CTX_SIZE - 1;
+		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), data[idx--]);
+		I915_WRITE(GEN6_VRSYNC, data[idx--]);
+		I915_WRITE(RING_MODE_GEN7(ring),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(GAC_ECO_BITS, data[idx--]);
+		I915_WRITE(RING_EXCC_GEN7(ring),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_INSTPM(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_UHPTR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_IMR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_MI_MODE(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(BSD_HWS_PGA_GEN7, data[idx--]);
+		break;
+
+	case BCS:
+		if (max < (COMMON_RING_CTX_SIZE + BCS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		idx = COMMON_RING_CTX_SIZE + BCS_RING_CTX_SIZE - 1;
+
+		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), data[idx--]);
+		I915_WRITE(GEN6_BVSYNC, data[idx--]);
+		I915_WRITE(GEN6_BRSYNC, data[idx--]);
+		I915_WRITE(RING_MODE_GEN7(ring),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(GAB_CTL, data[idx--]);
+		I915_WRITE(RING_EXCC_GEN7(ring),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_INSTPM(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_UHPTR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_IMR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_MI_MODE(ring->mmio_base),
+			_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(BLT_HWS_PGA_GEN7, data[idx--]);
+		break;
+
+	case VECS:
+		if (max < (COMMON_RING_CTX_SIZE + VECS_RING_CTX_SIZE))
+			return -EINVAL;
+
+		idx = COMMON_RING_CTX_SIZE + VECS_RING_CTX_SIZE - 1;
+
+		I915_WRITE(GEN6_VEVSYNC, data[idx--]);
+		I915_WRITE(RING_MODE_GEN7(ring),
+				_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_EXCC_GEN7(ring),
+				_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_INSTPM(ring->mmio_base),
+				_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(RING_UHPTR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_IMR(ring->mmio_base), data[idx--]);
+		I915_WRITE(RING_MI_MODE(ring->mmio_base),
+				_MASKED_BIT_ENABLE_ALL(data[idx--]));
+		I915_WRITE(VEBOX_HWS_PGA_GEN7, data[idx--]);
+		break;
+
+	default:
+		DRM_ERROR("Invalid ring ID %d\n", ring->id);
+		break;
+	}
+
+	/* Restore common registers */
+	if (max < COMMON_RING_CTX_SIZE)
+		return -EINVAL;
+
+	idx = COMMON_RING_CTX_SIZE - 1;
+
+	I915_WRITE(RING_PP_DIR_BASE(ring), data[idx--]);
+	I915_WRITE(RING_PP_DIR_DCLV(ring), data[idx--]);
+
+	/* Write ring base address before head/tail as it clears head to 0 */
+	I915_WRITE_START(ring, data[idx--]);
+	x = I915_READ_START(ring);
+	I915_WRITE_HEAD(ring, data[idx--]);
+	I915_WRITE_TAIL(ring, data[idx--]);
+	I915_WRITE_CTL(ring, data[idx--]);
+
+	return 0;
+}
+
+int intel_ring_invalidate_tlb(struct intel_engine_cs *ring)
+{
+	if (ring && ring->invalidate_tlb)
+		return ring->invalidate_tlb(ring);
+	else {
+		DRM_ERROR("ring invalidate tlb not supported\n");
+		return -EINVAL;
+	}
+}
+
+void intel_ring_resample(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_ringbuffer *ringbuf = ring->buffer;
+
+	if (!drm_core_check_feature(ring->dev, DRIVER_MODESET))
+		i915_kernel_lost_context(ring->dev);
+	else {
+		ringbuf->head = I915_READ_HEAD(ring);
+		ringbuf->tail = I915_READ_TAIL(ring) & TAIL_ADDR;
+		ringbuf->space = intel_ring_space(ringbuf);
+		ringbuf->last_retired_head = -1;
+	}
+}
+
 u64 intel_ring_get_active_head(struct intel_engine_cs *ring)
 {
 	struct drm_i915_private *dev_priv = ring->dev->dev_private;
@@ -560,7 +984,6 @@ static int init_ring_common(struct intel_engine_cs *ring)
 		ringbuf->last_retired_head = -1;
 	}
 
-	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
 
 out:
 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
@@ -1168,6 +1591,40 @@ i8xx_ring_put_irq(struct intel_engine_cs *ring)
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
+/* gen6_ring_invalidate_tlb
+ * GFX soft resets do not invalidate TLBs, it is up to
+ * GFX driver to explicitly invalidate TLBs post reset.
+ */
+static int gen6_ring_invalidate_tlb(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 reg;
+	int ret;
+
+	if ((INTEL_INFO(dev)->gen < 6) || (!ring->stop) || (!ring->start))
+		return -EINVAL;
+
+	/* stop the ring before sync_flush */
+	ret = ring->stop(ring);
+	if ((ret) && (ret != -EALREADY))
+		DRM_ERROR("%s: unable to stop the ring\n", ring->name);
+
+	/* Invalidate TLB */
+	reg = RING_INSTPM(ring->mmio_base);
+	I915_WRITE(reg, _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |
+				INSTPM_SYNC_FLUSH));
+	if (wait_for((I915_READ(reg) & INSTPM_SYNC_FLUSH) == 0, 1000))
+		DRM_ERROR("%s: wait for SyncFlush to complete timed out\n",
+				ring->name);
+
+	/* only start if stop was sucessfull */
+	if (!ret)
+		ring->start(ring);
+
+	return 0;
+}
+
 void intel_ring_setup_status_page(struct intel_engine_cs *ring)
 {
 	struct drm_device *dev = ring->dev;
@@ -1821,7 +2278,7 @@ static int ring_wait_for_space(struct intel_engine_cs *ring, int n)
 		}
 
 		ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-					   dev_priv->mm.interruptible);
+					   dev_priv->mm.interruptible, ring);
 		if (ret)
 			break;
 
@@ -1927,7 +2384,7 @@ int intel_ring_begin(struct intel_engine_cs *ring,
 	int ret;
 
 	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-				   dev_priv->mm.interruptible);
+				   dev_priv->mm.interruptible, ring);
 	if (ret)
 		return ret;
 
@@ -1981,7 +2438,6 @@ void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno)
 	}
 
 	ring->set_seqno(ring, seqno);
-	ring->hangcheck.seqno = seqno;
 }
 
 static void gen6_bsd_ring_write_tail(struct intel_engine_cs *ring,
@@ -2244,6 +2700,43 @@ gen6_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
 	return ret;
 }
 
+static int
+gen6_ring_stop(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/* check if ring is already stopped */
+	if (I915_READ_MODE(ring) & RING_MODE_STOP)
+		return -EALREADY;
+
+	/* Request the ring to go idle */
+	I915_WRITE_MODE(ring, _MASKED_BIT_ENABLE(RING_MODE_STOP));
+
+	/* Wait for idle */
+	if (wait_for_atomic((I915_READ_MODE(ring) & RING_MODE_IDLE) != 0,
+			    1000)) {
+		DRM_ERROR("%s :timed out trying to stop ring", ring->name);
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static int
+gen6_ring_start(struct intel_engine_cs *ring)
+{
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t mode;
+
+	/* Clear the MI_MODE stop bit */
+	I915_WRITE_MODE(ring, _MASKED_BIT_DISABLE(RING_MODE_STOP));
+	mode = I915_READ_MODE(ring);    /* Barrier read */
+
+	return 0;
+}
+
 /* Blitter support (SandyBridge+) */
 
 static int gen6_ring_flush(struct intel_engine_cs *ring,
@@ -2333,6 +2826,13 @@ int intel_init_render_ring_buffer(struct drm_device *dev)
 		ring->semaphore.mbox.signal[BCS] = GEN6_BRSYNC;
 		ring->semaphore.mbox.signal[VECS] = GEN6_VERSYNC;
 		ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+		ring->enable = gen6_ring_enable;
+		ring->disable = gen6_ring_disable;
+		ring->save = gen6_ring_save;
+		ring->restore = gen6_ring_restore;
+		ring->start = gen6_ring_start;
+		ring->stop = gen6_ring_stop;
+		ring->invalidate_tlb = gen6_ring_invalidate_tlb;
 	} else if (IS_GEN5(dev)) {
 		ring->add_request = pc_render_add_request;
 		ring->flush = gen4_render_ring_flush;
@@ -2535,6 +3035,13 @@ int intel_init_bsd_ring_buffer(struct drm_device *dev)
 		ring->semaphore.mbox.signal[BCS] = GEN6_BVSYNC;
 		ring->semaphore.mbox.signal[VECS] = GEN6_VEVSYNC;
 		ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+		ring->enable = gen6_ring_enable;
+		ring->disable = gen6_ring_disable;
+		ring->save = gen6_ring_save;
+		ring->restore = gen6_ring_restore;
+		ring->start = gen6_ring_start;
+		ring->stop = gen6_ring_stop;
+		ring->invalidate_tlb = gen6_ring_invalidate_tlb;
 	} else {
 		ring->mmio_base = BSD_RING_BASE;
 		ring->flush = bsd_ring_flush;
@@ -2604,6 +3111,13 @@ int intel_init_bsd2_ring_buffer(struct drm_device *dev)
 	ring->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
 	ring->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
 	ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+	ring->enable = gen6_ring_enable;
+	ring->disable = gen6_ring_disable;
+	ring->save = gen6_ring_save;
+	ring->restore = gen6_ring_restore;
+	ring->start = gen6_ring_start;
+	ring->stop = gen6_ring_stop;
+	ring->invalidate_tlb = gen6_ring_invalidate_tlb;
 
 	ring->init = init_ring_common;
 
@@ -2654,6 +3168,14 @@ int intel_init_blt_ring_buffer(struct drm_device *dev)
 	ring->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
 	ring->semaphore.mbox.signal[VECS] = GEN6_VEBSYNC;
 	ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+	ring->enable = gen6_ring_enable;
+	ring->disable = gen6_ring_disable;
+	ring->save = gen6_ring_save;
+	ring->restore = gen6_ring_restore;
+	ring->start = gen6_ring_start;
+	ring->stop = gen6_ring_stop;
+	ring->invalidate_tlb = gen6_ring_invalidate_tlb;
+
 	ring->init = init_ring_common;
 
 	return intel_init_ring_buffer(dev, ring);
@@ -2698,6 +3220,18 @@ int intel_init_vebox_ring_buffer(struct drm_device *dev)
 	ring->semaphore.mbox.signal[BCS] = GEN6_BVESYNC;
 	ring->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
 	ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+	ring->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;
+	ring->irq_get = hsw_vebox_get_irq;
+	ring->irq_put = hsw_vebox_put_irq;
+	ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
+	ring->enable = gen6_ring_enable;
+	ring->disable = gen6_ring_disable;
+	ring->start = gen6_ring_start;
+	ring->stop = gen6_ring_stop;
+	ring->save = gen6_ring_save;
+	ring->restore = gen6_ring_restore;
+	ring->invalidate_tlb = gen6_ring_invalidate_tlb;
+
 	ring->init = init_ring_common;
 
 	return intel_init_ring_buffer(dev, ring);
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.h b/drivers/gpu/drm/i915/intel_ringbuffer.h
index 8ff5986..440fe86 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.h
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.h
@@ -29,6 +29,23 @@ struct  intel_hw_status_page {
 	struct		drm_i915_gem_object *obj;
 };
 
+/*
+ * These values must match the requirements of the ring save/restore functions
+ * which may need to change for different versions of the chip
+ */
+#define COMMON_RING_CTX_SIZE 6
+#define RCS_RING_CTX_SIZE 14
+#define VCS_RING_CTX_SIZE 10
+#define BCS_RING_CTX_SIZE 11
+#define VECS_RING_CTX_SIZE 8
+#define MAX_CTX(a, b) (((a) > (b)) ? (a) : (b))
+
+/* Largest of individual rings + common */
+#define I915_RING_CONTEXT_SIZE (COMMON_RING_CTX_SIZE +		    \
+				MAX_CTX(MAX_CTX(RCS_RING_CTX_SIZE,  \
+						VCS_RING_CTX_SIZE), \
+						BCS_RING_CTX_SIZE))
+
 #define I915_READ_TAIL(ring) I915_READ(RING_TAIL((ring)->mmio_base))
 #define I915_WRITE_TAIL(ring, val) I915_WRITE(RING_TAIL((ring)->mmio_base), val)
 
@@ -55,14 +72,55 @@ enum intel_ring_hangcheck_action {
 	HANGCHECK_HUNG,
 };
 
-#define HANGCHECK_SCORE_RING_HUNG 31
+#define RESET_HEAD_TAIL 0x1
+#define FORCE_ADVANCE   0x2
 
 struct intel_ring_hangcheck {
-	u64 acthd;
-	u32 seqno;
-	int score;
 	enum intel_ring_hangcheck_action action;
-	int deadlock;
+
+	/* The ring being monitored */
+	u32 ringid;
+
+	/* Parent drm_device */
+	struct drm_device *dev;
+
+	/* Timer for this ring only */
+	struct timer_list timer;
+
+	/* Count of consecutive hang detections
+	 * (reset flag set once count exceeds threshold) */
+#define DRM_I915_HANGCHECK_THRESHOLD 1
+#define DRM_I915_MBOX_HANGCHECK_THRESHOLD 4
+	u32 count;
+
+	/* Last sampled head and active head */
+	u32 last_acthd;
+	u32 last_hd;
+
+	/* Last recorded ring head index.
+	* This is only ever a ring index where as active
+	* head may be a graphics address in a ring buffer */
+	u32 last_head;
+
+	/* Last recorded instdone */
+	u32 prev_instdone[I915_NUM_INSTDONE_REG];
+
+	/* Flag to indicate if ring reset required */
+#define DRM_I915_HANGCHECK_HUNG 0x01 /* Indicates this ring has hung */
+#define DRM_I915_HANGCHECK_RESET 0x02 /* Indicates request to reset this ring */
+	atomic_t flags;
+
+	/* Keep a record of the last time the ring was reset */
+	unsigned long last_reset;
+
+	/* Number of times this ring has been
+	* reset since boot*/
+	u32 total;
+
+	/* Number of TDR hang detections of r */
+	u32 tdr_count;
+
+	atomic_t active;
 };
 
 struct intel_ringbuffer {
@@ -147,6 +205,15 @@ struct intel_engine_cs {
 #define I915_DISPATCH_SECURE 0x1
 #define I915_DISPATCH_PINNED 0x2
 	void		(*cleanup)(struct intel_engine_cs *ring);
+	int (*enable)(struct intel_engine_cs *ring);
+	int (*disable)(struct intel_engine_cs *ring);
+	int (*start)(struct intel_engine_cs *ring);
+	int (*stop)(struct intel_engine_cs *ring);
+	int (*save)(struct intel_engine_cs *ring,
+		    uint32_t *data, uint32_t max, u32 flags);
+	int (*restore)(struct intel_engine_cs *ring,
+		       uint32_t *data, uint32_t max);
+	int (*invalidate_tlb)(struct intel_engine_cs *ring);
 
 	struct {
 		u32	sync_seqno[I915_NUM_RINGS-1];
@@ -211,6 +278,12 @@ struct intel_engine_cs {
 	struct intel_context *last_context;
 
 	struct intel_ring_hangcheck hangcheck;
+	/*
+	 * Area large enough to store all the register
+	 * data associated with this ring
+	 */
+	u32 saved_state[I915_RING_CONTEXT_SIZE];
+	uint32_t last_irq_seqno;
 
 	struct {
 		struct drm_i915_gem_object *obj;
@@ -343,10 +416,17 @@ static inline void intel_ring_advance(struct intel_engine_cs *ring)
 	struct intel_ringbuffer *ringbuf = ring->buffer;
 	ringbuf->tail &= ringbuf->size - 1;
 }
+
 int __intel_ring_space(int head, int tail, int size);
 int intel_ring_space(struct intel_ringbuffer *ringbuf);
 bool intel_ring_stopped(struct intel_engine_cs *ring);
 void __intel_ring_advance(struct intel_engine_cs *ring);
+void intel_ring_resample(struct intel_engine_cs *ring);
+int intel_ring_disable(struct intel_engine_cs *ring);
+int intel_ring_enable(struct intel_engine_cs *ring);
+int intel_ring_save(struct intel_engine_cs *ring, u32 flags);
+int intel_ring_restore(struct intel_engine_cs *ring);
+int intel_ring_invalidate_tlb(struct intel_engine_cs *ring);
 
 int __must_check intel_ring_idle(struct intel_engine_cs *ring);
 void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno);
diff --git a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
index 503188a..dccee2b 100644
--- a/drivers/gpu/drm/i915/intel_uncore.c
+++ b/drivers/gpu/drm/i915/intel_uncore.c
@@ -1135,7 +1135,8 @@ static int gen6_do_reset(struct drm_device *dev)
 	__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_FULL);
 
 	/* Spin waiting for the device to ack the reset request */
-	ret = wait_for((__raw_i915_read32(dev_priv, GEN6_GDRST) & GEN6_GRDOM_FULL) == 0, 500);
+	ret = wait_for((__raw_i915_read32(dev_priv, GEN6_GDRST) &
+					GEN6_GRDOM_FULL) == 0, 500);
 
 	intel_uncore_forcewake_reset(dev, true);
 
@@ -1144,18 +1145,131 @@ static int gen6_do_reset(struct drm_device *dev)
 
 int intel_gpu_reset(struct drm_device *dev)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret = -ENODEV;
+
 	switch (INTEL_INFO(dev)->gen) {
 	case 8:
 	case 7:
-	case 6: return gen6_do_reset(dev);
-	case 5: return ironlake_do_reset(dev);
+	case 6:
+		ret = gen6_do_reset(dev);
+		break;
+	case 5:
+		ret = ironlake_do_reset(dev);
+		break;
 	case 4:
 		if (IS_G4X(dev))
-			return g4x_do_reset(dev);
+			ret = g4x_do_reset(dev);
 		else
-			return i965_do_reset(dev);
-	default: return -ENODEV;
+			ret = i965_do_reset(dev);
+		break;
+	default:
+		return ret;
+	}
+
+	dev_priv->gpu_error.total_resets++;
+	DRM_DEBUG_TDR("total_resets %ld\n", dev_priv->gpu_error.total_resets);
+
+	return ret;
+}
+
+static int gen6_do_engine_reset(struct drm_device *dev,
+				enum intel_ring_id engine)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret = -ENODEV;
+	unsigned long irqflags;
+	char *reset_event[2];
+	reset_event[1] = NULL;
+
+	/* Hold uncore.lock across reset to prevent any register access
+	 * with forcewake not set correctly
+	 */
+	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+
+	/* Reset the engine.
+	 * GEN6_GDRST is not in the gt power well so no need to check
+	 * for fifo space for the write or forcewake the chip for
+	 * the read
+	 */
+	switch (engine) {
+	case RCS:
+		__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_RENDER);
+		dev_priv->ring[RCS].hangcheck.total++;
+
+		/* Spin waiting for the device to ack the reset request */
+		ret = wait_for_atomic_us((__raw_i915_read32(dev_priv,
+			GEN6_GDRST)
+			& GEN6_GRDOM_RENDER) == 0, 500);
+		break;
+
+	case BCS:
+		__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_BLT);
+		dev_priv->ring[BCS].hangcheck.total++;
+
+		/* Spin waiting for the device to ack the reset request */
+		ret = wait_for_atomic_us((__raw_i915_read32(dev_priv,
+			GEN6_GDRST)
+			& GEN6_GRDOM_BLT) == 0, 500);
+		break;
+
+	case VCS:
+		__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_MEDIA);
+		dev_priv->ring[VCS].hangcheck.total++;
+
+		/* Spin waiting for the device to ack the reset request */
+		ret = wait_for_atomic_us((__raw_i915_read32(dev_priv,
+			GEN6_GDRST)
+			& GEN6_GRDOM_MEDIA) == 0, 500);
+		break;
+
+	case VECS:
+		__raw_i915_write32(dev_priv, GEN6_GDRST, GEN6_GRDOM_VECS);
+		dev_priv->ring[VECS].hangcheck.total++;
+
+		/* Spin waiting for the device to ack the reset request */
+		ret = wait_for_atomic_us((__raw_i915_read32(dev_priv,
+			GEN6_GDRST)
+			& GEN6_GRDOM_VECS) == 0, 500);
+		break;
+
+	default:
+		DRM_ERROR("Unexpected Engine\n");
+		break;
+	}
+
+	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+
+	/* Do uevent outside of spinlock as uevent can sleep */
+	reset_event[0] = kasprintf(GFP_KERNEL, "RESET RING=%d", engine);
+	kobject_uevent_env(&dev->primary->kdev->kobj,
+		KOBJ_CHANGE, reset_event);
+	kfree(reset_event[0]);
+
+	return ret;
+}
+
+int intel_gpu_engine_reset(struct drm_device *dev, enum intel_ring_id engine)
+{
+	/* Reset an individual engine */
+	int ret = -ENODEV;
+
+	if (!dev)
+		return -EINVAL;
+
+	switch (INTEL_INFO(dev)->gen) {
+	case 7:
+	case 6:
+		ret = gen6_do_engine_reset(dev, engine);
+		break;
+	default:
+		DRM_ERROR("Per Engine Reset not supported on Gen%d\n",
+			  INTEL_INFO(dev)->gen);
+		ret = -ENODEV;
+		break;
 	}
+
+	return ret;
 }
 
 void intel_uncore_check_errors(struct drm_device *dev)
diff --git a/include/drm/drmP.h b/include/drm/drmP.h
index f7aa21f..ae52afd 100644
--- a/include/drm/drmP.h
+++ b/include/drm/drmP.h
@@ -121,6 +121,7 @@ struct videomode;
 #define DRM_UT_DRIVER		0x02
 #define DRM_UT_KMS		0x04
 #define DRM_UT_PRIME		0x08
+#define DRM_UT_TDR		0x10
 
 extern __printf(2, 3)
 void drm_ut_debug_printk(const char *function_name,
@@ -222,10 +223,16 @@ int drm_err(const char *func, const char *format, ...);
 		if (unlikely(drm_debug & DRM_UT_PRIME))			\
 			drm_ut_debug_printk(__func__, fmt, ##args);	\
 	} while (0)
+#define DRM_DEBUG_TDR(fmt, args...)					\
+	do {								\
+		if (unlikely(drm_debug & DRM_UT_TDR))			\
+			drm_ut_debug_printk(__func__, fmt, ##args);	\
+	} while (0)
 #else
 #define DRM_DEBUG_DRIVER(fmt, args...) do { } while (0)
 #define DRM_DEBUG_KMS(fmt, args...)	do { } while (0)
 #define DRM_DEBUG_PRIME(fmt, args...)	do { } while (0)
+#define DRM_DEBUG_TDR(fmt, args...)	do { } while (0)
 #define DRM_DEBUG(fmt, arg...)		 do { } while (0)
 #endif
 
-- 
1.7.9.5

