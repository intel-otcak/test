From 91efc0bbef1c87e1d29fdf7960042e3d74998687 Mon Sep 17 00:00:00 2001
Message-Id: <91efc0bbef1c87e1d29fdf7960042e3d74998687.1417780177.git.chang-joon.lee@intel.com>
In-Reply-To: <e293659a31a2970805251d9fed030ac809a9a6d0.1417780177.git.chang-joon.lee@intel.com>
References: <e293659a31a2970805251d9fed030ac809a9a6d0.1417780177.git.chang-joon.lee@intel.com>
From: Adam Rutkowski <adam.j.rutkowski@intel.com>
Date: Wed, 27 Aug 2014 13:52:22 -0400
Subject: [PATCH 29/30] REVERTME [VPG]: drm/i915: Perfmon OA/NOA configuration

Writing OA / Perfmon configuration to ring buffer.
Some registers for OA configuration are part of context and
thus need to be written via LRIs inserted to the ring buffer.
It is convinient to send all OA configuration registers this way
since it enables us to implement multiconfiguration, meaning
multiple users each using different OA config. To have this
working programming of per-context workaround batch buffers
is required.

Patch is marked with REVERTME as Perfmon IOCTL will be removed
once OA couters are exposed via 'perf event' framework.

Issue: GMINL-3120
Change-Id: Ifc87f3542a8852943f13cf2bf724547df439b8d8
Signed-off-by: Adam Rutkowski <adam.j.rutkowski@intel.com>
Signed-off-by: Andrzej Datczuk <andrzej.datczuk@intel.com>
---
 drivers/gpu/drm/i915/i915_dma.c          |    1 +
 drivers/gpu/drm/i915/i915_drv.h          |    5 +
 drivers/gpu/drm/i915/i915_gem_context.c  |    2 +
 drivers/gpu/drm/i915/i915_perfmon.c      |  340 +++++++++++++++++++++++++++++-
 drivers/gpu/drm/i915/i915_perfmon_defs.h |   25 +++
 drivers/gpu/drm/i915/i915_reg.h          |    2 +
 drivers/gpu/drm/i915/intel_lrc.c         |  155 ++++++++++++++
 7 files changed, 526 insertions(+), 4 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index e37dec2..1488da0 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1678,6 +1678,7 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 	mutex_init(&dev_priv->new_dpio_lock);
 	mutex_init(&dev_priv->modeset_restore_lock);
 	mutex_init(&dev_priv->dpst.ioctl_lock);
+	mutex_init(&dev_priv->perfmon.config.lock);
 
 	intel_pm_setup(dev);
 
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index d38ec0a..911f9be 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -673,6 +673,9 @@ struct intel_context {
 	} engine[I915_NUM_RINGS];
 
 	struct list_head link;
+
+	/* perfmon configuration */
+	struct drm_i915_perfmon_context perfmon;
 };
 
 struct i915_fbc {
@@ -3025,6 +3028,8 @@ int i915_perfmon_ioctl(struct drm_device *dev, void *data,
 	struct drm_file *file);
 void i915_perfmon_setup(struct drm_i915_private *dev_priv);
 void i915_perfmon_cleanup(struct drm_i915_private *dev_priv);
+void i915_perfmon_ctx_setup(struct intel_context *ctx);
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx);
 
 extern void i915_write_bits32(struct drm_i915_private *dev_priv,
 	u32 reg, u32 val, u32 mask, bool trace);
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index 635e74f..5bba965 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -146,6 +146,7 @@ void i915_gem_context_free(struct kref *ctx_ref)
 		drm_gem_object_unreference(&ctx->legacy_hw_ctx.rcs_state->base);
 
 	put_pid(ctx->pid);
+	i915_perfmon_ctx_cleanup(ctx);
 
 	list_del(&ctx->link);
 	kfree(ctx);
@@ -278,6 +279,7 @@ i915_gem_create_context(struct drm_device *dev,
 	}
 
 	ctx->pid = get_pid(task_tgid(current));
+	i915_perfmon_ctx_setup(ctx);
 
 	return ctx;
 
diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
index 6dc7baf..7886bed 100644
--- a/drivers/gpu/drm/i915/i915_perfmon.c
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -290,6 +290,313 @@ exit:
 }
 
 /**
+ * copy_entries
+ *
+ * Helper function to copy OA configuration entries to new destination.
+ *
+ * Source configuration is first validated. In case of success pointer to newly
+ * allocated memory containing copy of source configuration is returned in *out.
+ *
+ */
+static int copy_entries(
+	struct drm_i915_perfmon_config *source,
+	bool user,
+	void **out)
+{
+	size_t size = 0;
+
+	*out = NULL;
+
+	/* basic validation of input */
+	if (source->id == 0 || source->size == 0 || source->entries == NULL)
+		return 0;
+
+	if (source->size > I915_PERFMON_CONFIG_SIZE)
+		return -EINVAL;
+
+	size = source->size  * sizeof(struct drm_i915_perfmon_config_entry);
+
+	*out = kzalloc(
+		   size,
+		   GFP_KERNEL);
+	if (*out == NULL) {
+		DRM_ERROR("failed to allocate configuration buffer\n");
+		return -ENOMEM;
+	}
+
+	if (user) {
+		int ret = copy_from_user(*out, source->entries, size);
+		if (ret) {
+			DRM_ERROR("failed to copy user provided config: %x\n",
+					ret);
+			kfree(*out);
+			*out = NULL;
+			return -EFAULT;
+		}
+	} else
+		memcpy(*out, source->entries, size);
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_copy_config
+ *
+ * Utility function to copy OA and GP configuration to its destination.
+ *
+ * This is first used when global configuration is set by the user by calling
+ * I915_PERFMON_SET_CONFIG and then for the second time (optionally) when user
+ * calls I915_PERFMON_LOAD_CONFIG to copy the configuration from global storage
+ * to his context.
+ *
+ * 'user' boolean value indicates whether pointer to source config is provided
+ * by usermode (I915_PERFMON_SET_CONFIG case).
+ *
+ * If both OA and GP config are provided (!= NULL) then either both are copied
+ * to their respective locations or none of them (which is indicated by return
+ * value != 0).
+ *
+ * target_oa and target_gp are assumed to be non-NULL.
+ *
+ */
+static int i915_perfmon_copy_config(
+	struct drm_i915_private *dev_priv,
+	struct drm_i915_perfmon_config *target_oa,
+	struct drm_i915_perfmon_config *target_gp,
+	struct drm_i915_perfmon_config source_oa,
+	struct drm_i915_perfmon_config source_gp,
+	bool user)
+{
+	void *temp_oa = NULL;
+	void *temp_gp = NULL;
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	/* copy configurations to temporary storage */
+	ret = copy_entries(&source_oa, user, &temp_oa);
+	if (ret)
+		return ret;
+	ret = copy_entries(&source_gp, user, &temp_gp);
+	if (ret) {
+		kfree(temp_oa);
+		return ret;
+	}
+
+	/*
+	 * Allocation and copy successful, free old config memory and swap
+	 * pointers
+	 */
+	if (temp_oa) {
+		kfree(target_oa->entries);
+		target_oa->entries = temp_oa;
+		target_oa->id = source_oa.id;
+		target_oa->size = source_oa.size;
+	}
+	if (temp_gp) {
+		kfree(target_gp->entries);
+		target_gp->entries = temp_gp;
+		target_gp->id = source_gp.id;
+		target_gp->size = source_gp.size;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_set_config
+ *
+ * Store OA/GP configuration for later use.
+ *
+ * Configuration content is not validated since it is provided by user who had
+ * previously called Perfmon Open with sysadmin privilege level.
+ *
+ */
+static int i915_perfmon_set_config(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_set_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	int ret = 0;
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+
+	if (!(IS_GEN8(dev)))
+		return -EINVAL;
+
+	/* validate target */
+	switch (args->target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+	case I915_PERFMON_CONFIG_TARGET_PID:
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		/* OK */
+		break;
+	default:
+		DRM_DEBUG("invalid target\n");
+		return -EINVAL;
+	}
+
+	/* setup input for i915_perfmon_copy_config */
+	user_config_oa.id = args->oa.id;
+	user_config_oa.size = args->oa.size;
+	user_config_oa.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->oa.entries;
+
+	user_config_gp.id = args->gp.id;
+	user_config_gp.size = args->gp.size;
+	user_config_gp.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->gp.entries;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_perfmon;
+	}
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&dev_priv->perfmon.config.oa,
+			&dev_priv->perfmon.config.gp,
+			user_config_oa, user_config_gp,
+			true);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	dev_priv->perfmon.config.target = args->target;
+	dev_priv->perfmon.config.pid = args->pid;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
+
+/**
+ * i915_perfmon_load_config
+ *
+ * Copy configuration from global storage to current context.
+ *
+ */
+static int i915_perfmon_load_config(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_load_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct intel_context *ctx;
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+	int ret;
+
+	if (!(IS_GEN8(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ctx = i915_gem_context_get(
+				file_priv,
+				args->ctx_id);
+
+	if (IS_ERR_OR_NULL(ctx) || IS_ERR_OR_NULL(ctx->engine[RCS].state)) {
+		DRM_DEBUG("invalid context\n");
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		goto unlock_dev;
+
+	user_config_oa = dev_priv->perfmon.config.oa;
+	user_config_gp = dev_priv->perfmon.config.gp;
+
+	/*
+	 * copy configuration to the context only if requested config ID matches
+	 * device configuration ID
+	 */
+	if (!(args->oa_id != 0 &&
+	      args->oa_id == dev_priv->perfmon.config.oa.id))
+		user_config_oa.entries = NULL;
+	if (!(args->gp_id != 0 &&
+	     args->gp_id == dev_priv->perfmon.config.gp.id))
+		user_config_gp.entries = NULL;
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&ctx->perfmon.config.oa.pending,
+			&ctx->perfmon.config.gp.pending,
+			dev_priv->perfmon.config.oa,
+			dev_priv->perfmon.config.gp,
+			false);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	/*
+	 * return info about what is actualy set for submission in
+	 * target context
+	 */
+	args->gp_id = ctx->perfmon.config.gp.pending.id;
+	args->oa_id = ctx->perfmon.config.oa.pending.id;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+unlock_dev:
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+/**
+* i915_perfmon_config_enable_disable
+*
+* Enable/disable OA/GP configuration transport.
+*/
+static int i915_perfmon_config_enable_disable(
+	struct drm_device *dev,
+	int enable)
+{
+	int ret;
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+
+	if (!(IS_GEN8(dev)))
+		return -EINVAL;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (enable) {
+		if (atomic_inc_return(&dev_priv->perfmon.config.enable) == 1) {
+			dev_priv->perfmon.config.target =
+				I915_PERFMON_CONFIG_TARGET_ALL;
+			dev_priv->perfmon.config.oa.id = 0;
+			dev_priv->perfmon.config.gp.id = 0;
+		}
+	} else if (atomic_read(&dev_priv->perfmon.config.enable))
+		atomic_dec(&dev_priv->perfmon.config.enable);
+
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+
+	return 0;
+}
+
+
+/**
  * i915_perfmon_open
  *
  * open perfmon for current file
@@ -367,19 +674,28 @@ int i915_perfmon_ioctl(struct drm_device *dev, void *data,
 		break;
 
 	case I915_PERFMON_ENABLE_CONFIG:
-		ret = -ENODEV;
+		ret = i915_perfmon_config_enable_disable(dev, 1);
 		break;
 
 	case I915_PERFMON_DISABLE_CONFIG:
-		ret = -ENODEV;
+		ret = i915_perfmon_config_enable_disable(dev, 0);
 		break;
 
 	case I915_PERFMON_SET_CONFIG:
-		ret = -ENODEV;
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_set_config(
+			dev,
+			&perfmon->data.set_config);
 		break;
 
 	case I915_PERFMON_LOAD_CONFIG:
-		ret = -ENODEV;
+		ret = i915_perfmon_load_config(
+			dev,
+			file,
+			&perfmon->data.load_config);
 		break;
 
 	case I915_PERFMON_GET_HW_CTX_ID:
@@ -413,9 +729,25 @@ void i915_perfmon_setup(struct drm_i915_private *dev_priv)
 {
 	atomic_set(&dev_priv->perfmon.buffer_interrupts, 0);
 	init_waitqueue_head(&dev_priv->perfmon.buffer_queue);
+	atomic_set(&dev_priv->perfmon.config.enable, 0);
+	dev_priv->perfmon.config.oa.entries = NULL;
+	dev_priv->perfmon.config.gp.entries = NULL;
 }
 
 void i915_perfmon_cleanup(struct drm_i915_private *dev_priv)
 {
+	kfree(dev_priv->perfmon.config.oa.entries);
+	kfree(dev_priv->perfmon.config.gp.entries);
 }
 
+void i915_perfmon_ctx_setup(struct intel_context *ctx)
+{
+	ctx->perfmon.config.oa.pending.entries = NULL;
+	ctx->perfmon.config.gp.pending.entries = NULL;
+}
+
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx)
+{
+	kfree(ctx->perfmon.config.oa.pending.entries);
+	kfree(ctx->perfmon.config.gp.pending.entries);
+}
diff --git a/drivers/gpu/drm/i915/i915_perfmon_defs.h b/drivers/gpu/drm/i915/i915_perfmon_defs.h
index 169d2e8..300f3e5 100644
--- a/drivers/gpu/drm/i915/i915_perfmon_defs.h
+++ b/drivers/gpu/drm/i915/i915_perfmon_defs.h
@@ -24,10 +24,35 @@
 #ifndef _I915_PERFMON_DEFS_H_
 #define _I915_PERFMON_DEFS_H_
 
+struct drm_i915_perfmon_config {
+	struct drm_i915_perfmon_config_entry *entries;
+	__u32 size;
+	__u32  id;
+};
+
+struct drm_i915_perfmon_context {
+	struct {
+		struct {
+			struct drm_i915_perfmon_config pending;
+			__u32 submitted_id;
+		} oa, gp;
+	} config;
+};
+
 struct drm_i915_perfmon_device {
 	/* perfmon interrupt support */
 	wait_queue_head_t	buffer_queue;
 	atomic_t		buffer_interrupts;
+
+	/* perfmon counters configuration */
+	struct {
+		struct drm_i915_perfmon_config oa;
+		struct drm_i915_perfmon_config gp;
+		enum DRM_I915_PERFMON_CONFIG_TARGET target;
+		pid_t pid;
+		atomic_t enable;
+		struct mutex lock;
+	} config;
 };
 
 struct drm_i915_perfmon_file {
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 03059fb..88b6b19 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -6780,4 +6780,6 @@ enum punit_power_well {
 #define _PALETTE_A (dev_priv->info.display_mmio_offset + 0xa000)
 #define _PALETTE_B (dev_priv->info.display_mmio_offset + 0xa800)
 
+#define GEN8_OA_CTX_CONTROL 0x2360
+
 #endif /* _I915_REG_H_ */
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 7891987..4ecf0b6 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -341,6 +341,158 @@ static void execlists_elsp_write(struct intel_engine_cs *ring,
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, flags);
 }
 
+static void perfmon_send_config(
+	struct intel_ringbuffer *ringbuf,
+	struct drm_i915_perfmon_config *config)
+{
+	int i;
+
+	for (i = 0; i < config->size; i++) {
+		DRM_DEBUG("perfmon config %x reg:%05x val:%08x\n",
+			config->id,
+			config->entries[i].offset,
+			config->entries[i].value);
+
+		intel_logical_ring_emit(ringbuf, MI_NOOP);
+		intel_logical_ring_emit(ringbuf, MI_LOAD_REGISTER_IMM(1));
+		intel_logical_ring_emit(ringbuf, config->entries[i].offset);
+		intel_logical_ring_emit(ringbuf, config->entries[i].value);
+	}
+}
+
+static inline struct drm_i915_perfmon_config *get_perfmon_config(
+	struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	struct drm_i915_perfmon_config *config_global,
+	struct drm_i915_perfmon_config *config_context,
+	__u32 ctx_submitted_config_id)
+
+{
+	struct drm_i915_perfmon_config *config  = NULL;
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	target = dev_priv->perfmon.config.target;
+	switch (target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+		config = config_context;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_PID:
+		if (pid_vnr(ctx->pid) == dev_priv->perfmon.config.pid)
+			config = config_global;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		config = config_global;
+		break;
+	default:
+		BUG_ON(1);
+		break;
+	}
+
+	if (config != NULL) {
+		if (config->size == 0 || config->id == 0) {
+			/* configuration is empty or targets other context */
+			DRM_DEBUG("perfmon configuration empty\n");
+			config = NULL;
+		} else if (config->id == ctx_submitted_config_id) {
+			/* configuration is already submitted in this context*/
+			DRM_DEBUG("perfmon configuration %x is submitted\n",
+					config->id);
+			config = NULL;
+		}
+	}
+
+	if (config != NULL)
+		DRM_DEBUG("perfmon configuration TARGET:%u SIZE:%x ID:%x",
+			target,
+			config->size,
+			config->id);
+
+	return config;
+}
+
+static inline int
+i915_program_perfmon(struct drm_device *dev,
+			struct intel_ringbuffer *ringbuf,
+			struct intel_context *ctx)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_perfmon_config *config_oa, *config_gp;
+	size_t size;
+	int ret = 0;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable) &&
+	    ctx->perfmon.config.oa.submitted_id == 0)
+		return 0;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		if (ctx->perfmon.config.oa.submitted_id != 0) {
+			/* write 0 to OA_CTX_CONTROL to stop counters */
+			ret = intel_logical_ring_begin(ringbuf, 4);
+			if (!ret) {
+				intel_logical_ring_emit(ringbuf, MI_NOOP);
+				intel_logical_ring_emit(ringbuf,
+					MI_LOAD_REGISTER_IMM(1));
+				intel_logical_ring_emit(ringbuf,
+					GEN8_OA_CTX_CONTROL);
+				intel_logical_ring_emit(ringbuf, 0);
+				intel_logical_ring_advance(ringbuf);
+			}
+			ctx->perfmon.config.oa.submitted_id = 0;
+		}
+		goto unlock;
+	}
+
+	/* check for pending OA config */
+	config_oa = get_perfmon_config(dev_priv, ctx,
+					&dev_priv->perfmon.config.oa,
+					&ctx->perfmon.config.oa.pending,
+					ctx->perfmon.config.oa.submitted_id);
+
+	/* check for pending PERFMON config */
+	config_gp = get_perfmon_config(dev_priv, ctx,
+					&dev_priv->perfmon.config.gp,
+					&ctx->perfmon.config.gp.pending,
+					ctx->perfmon.config.gp.submitted_id);
+
+	size = (config_oa ? config_oa->size : 0) +
+	       (config_gp ? config_gp->size : 0);
+
+	if (size == 0)
+		goto unlock;
+
+	ret = intel_logical_ring_begin(ringbuf, 4 * size);
+	if (ret)
+		goto unlock;
+
+	/* submit pending OA config */
+	if (config_oa) {
+		perfmon_send_config(
+			ringbuf,
+			config_oa);
+		ctx->perfmon.config.oa.submitted_id = config_oa->id;
+	}
+
+
+	/* submit pending general purpose perfmon counters config */
+	if (config_gp) {
+		perfmon_send_config(
+			ringbuf,
+			config_gp);
+		ctx->perfmon.config.gp.submitted_id = config_gp->id;
+	}
+
+	intel_logical_ring_advance(ringbuf);
+
+unlock:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
 static int execlists_ctx_write_tail(struct drm_i915_gem_object *ctx_obj, u32 tail)
 {
 	struct page *page;
@@ -926,6 +1078,9 @@ int intel_execlists_submission(struct drm_device *dev, struct drm_file *file,
 		dev_priv->relative_constants_mode = instp_mode;
 	}
 
+	if (IS_GEN8(dev) && ring == &dev_priv->ring[RCS])
+		i915_program_perfmon(dev, ringbuf, ctx);
+
 	/* Flag this seqno as being active on the ring so the watchdog
 	 * code knows where to look if things go wrong. */
 	ret = logical_ring_write_active_seqno(ringbuf, seqno);
-- 
1.7.9.5

